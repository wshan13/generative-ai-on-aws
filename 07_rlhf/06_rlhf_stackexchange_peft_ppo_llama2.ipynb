{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "29f9f42b-7575-4f3d-9ba2-ca2c5165c916",
   "metadata": {
    "id": "29f9f42b-7575-4f3d-9ba2-ca2c5165c916"
   },
   "source": [
    "# ml.p4de.24xlarge 인스턴스로 테스트"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53443304-4173-4ac8-a57c-98e364f5df4a",
   "metadata": {
    "id": "53443304-4173-4ac8-a57c-98e364f5df4a"
   },
   "source": [
    "이 실습은 다음 예시를 참고했습니다. https://github.com/huggingface/trl/blob/main/examples/research_projects/stack_llama/scripts/rl_training.py\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c027e413-b079-4f80-9fa8-8e47801b7c22",
   "metadata": {
    "id": "c027e413-b079-4f80-9fa8-8e47801b7c22",
    "outputId": "36b11b47-80d0-417f-a10a-a8ffb84df368"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001B[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001B[0m\u001B[33m\n",
      "\u001B[0mNote: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install --disable-pip-version-check -q \\\n",
    "    torch==2.0.1 \\\n",
    "    transformers==4.34.1 \\\n",
    "    datasets==2.12.0 \\\n",
    "    accelerate==0.23.0 \\\n",
    "    evaluate==0.4.0 \\\n",
    "    trl==0.7.2 \\\n",
    "    rouge_score==0.1.2 \\\n",
    "    loralib==0.1.1 \\\n",
    "    typing_extensions==4.7.1 \\\n",
    "    bitsandbytes==0.41.1 \\\n",
    "    peft==0.5.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "052c8fdf-e2fe-4191-a939-80de0f4000db",
   "metadata": {
    "tags": [],
    "id": "052c8fdf-e2fe-4191-a939-80de0f4000db"
   },
   "outputs": [],
   "source": [
    "%store -r peft_ranking_reward_public_qanda_checkpoint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4854a87-583b-4907-b48f-26bb278af632",
   "metadata": {
    "tags": [],
    "id": "c4854a87-583b-4907-b48f-26bb278af632",
    "outputId": "30d9e2ee-881b-4bd5-af88-954dedfc4418"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "./peft_ranking_reward_public_qanda/\n"
     ]
    }
   ],
   "source": [
    "print(peft_ranking_reward_public_qanda_checkpoint)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "733ae426-3c17-45df-b3e5-6d1f41d2176d",
   "metadata": {
    "id": "733ae426-3c17-45df-b3e5-6d1f41d2176d"
   },
   "outputs": [],
   "source": [
    "from dataclasses import dataclass, field\n",
    "from typing import Optional\n",
    "\n",
    "import torch\n",
    "from accelerate import Accelerator\n",
    "from datasets import load_dataset\n",
    "from peft import LoraConfig\n",
    "from tqdm import tqdm\n",
    "from transformers import Adafactor, AutoTokenizer, HfArgumentParser, pipeline\n",
    "\n",
    "from trl import AutoModelForCausalLMWithValueHead, PPOConfig, PPOTrainer, set_seed\n",
    "from trl.core import LengthSampler\n",
    "\n",
    "tqdm.pandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9537b3de-a994-438e-bccf-c46debee4dd4",
   "metadata": {
    "tags": [],
    "id": "9537b3de-a994-438e-bccf-c46debee4dd4"
   },
   "outputs": [],
   "source": [
    "peft_fine_tuned_with_ranking_rewards_llama2_checkpoint = './peft_fine_tuned_with_ranking_rewards_llama2'\n",
    "\n",
    "@dataclass\n",
    "class ScriptArguments:\n",
    "    model_name: Optional[str] = field(default=\"NousResearch/Llama-2-7b-hf\", metadata={\"help\": \"the model name\"})\n",
    "    tokenizer_name: Optional[str] = field(default=\"NousResearch/Llama-2-7b-hf\", metadata={\"help\": \"the tokenizer name\"})\n",
    "    reward_model_name: Optional[str] = field(default=peft_ranking_reward_public_qanda_checkpoint, metadata={\"help\": \"the reward model name\"})\n",
    "    log_with: Optional[str] = field(default=None, metadata={\"help\": \"use 'wandb' to log with wandb\"})\n",
    "    learning_rate: Optional[float] = field(default=1.41e-5, metadata={\"help\": \"the learning rate\"})\n",
    "    output_max_length: Optional[int] = field(default=128, metadata={\"help\": \"maximum length for generation\"})\n",
    "    mini_batch_size: Optional[int] = field(default=1, metadata={\"help\": \"the PPO minibatch size\"})\n",
    "    batch_size: Optional[int] = field(default=1, metadata={\"help\": \"the batch size\"})\n",
    "    ppo_epochs: Optional[int] = field(default=4, metadata={\"help\": \"the number of ppo epochs\"})\n",
    "    gradient_accumulation_steps: Optional[int] = field(\n",
    "        default=1, metadata={\"help\": \"the number of gradient accumulation steps\"}\n",
    "    )\n",
    "    adafactor: Optional[bool] = field(default=False, metadata={\"help\": \"whether to use the adafactor optimizer\"})\n",
    "    early_stopping: Optional[bool] = field(default=False, metadata={\"help\": \"whether to early stop\"})\n",
    "    target_kl: Optional[float] = field(default=0.1, metadata={\"help\": \"kl target for early stopping\"})\n",
    "    reward_baseline: Optional[float] = field(\n",
    "        default=0.0,\n",
    "        metadata={\"help\": \"a baseline value that is subtracted from the reward\"},\n",
    "    )\n",
    "    batched_gen: Optional[bool] = field(default=False, metadata={\"help\": \"whether to use the batched text gen\"})\n",
    "    save_freq: Optional[int] = field(default=None, metadata={\"help\": \"n steps to save the model\"})\n",
    "#    output_dir: Optional[str] = field(default=\"runs/\", metadata={\"help\": \"n steps to save the model\"})\n",
    "    seed: Optional[int] = field(default=0, metadata={\"help\": \"the seed\"})\n",
    "    steps: Optional[int] = field(default=100, metadata={\"help\": \"number of epochs\"})\n",
    "    init_kl_coef: Optional[float] = field(\n",
    "        default=0.2,\n",
    "        metadata={\"help\": \"Initial KL penalty coefficient (used for adaptive and linear control)\"},\n",
    "    )\n",
    "\n",
    "    adap_kl_ctrl: Optional[bool] = field(default=True, metadata={\"help\": \"Use adaptive KL control, otherwise linear\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22ffcb28-f12b-4ffa-8936-85e58f0d3212",
   "metadata": {
    "tags": [],
    "id": "22ffcb28-f12b-4ffa-8936-85e58f0d3212"
   },
   "outputs": [],
   "source": [
    "parser = HfArgumentParser(ScriptArguments)\n",
    "script_args: ScriptArguments = parser.parse_args_into_dataclasses(return_remaining_strings=True)[0]\n",
    "\n",
    "dataset_name = \"lvwerra/stack-exchange-paired\"\n",
    "config = PPOConfig(\n",
    "    steps=script_args.steps,\n",
    "    model_name=script_args.model_name,\n",
    "    learning_rate=script_args.learning_rate,\n",
    "    log_with=script_args.log_with,\n",
    "    batch_size=script_args.batch_size,\n",
    "    mini_batch_size=script_args.mini_batch_size,\n",
    "    gradient_accumulation_steps=script_args.gradient_accumulation_steps,\n",
    "    optimize_cuda_cache=True,\n",
    "    early_stopping=script_args.early_stopping,\n",
    "    target_kl=script_args.target_kl,\n",
    "    ppo_epochs=script_args.ppo_epochs,\n",
    "    seed=script_args.seed,\n",
    "    init_kl_coef=script_args.init_kl_coef,\n",
    "    adap_kl_ctrl=script_args.adap_kl_ctrl,\n",
    ")\n",
    "\n",
    "# 우리는 감정 분석 파이프라인에 전달할 인자를 정의합니다.\n",
    "#`return_all_scores`를 True로 설정해 각 토큰의 감정 분류 점수를 얻습니다.\n",
    "sent_kwargs = {\n",
    "    \"return_all_scores\": True,\n",
    "    \"function_to_apply\": \"none\",\n",
    "    \"batch_size\": 16,\n",
    "    \"truncation\": True,\n",
    "}\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(script_args.tokenizer_name)\n",
    "\n",
    "if getattr(tokenizer, \"pad_token\", None) is None:\n",
    "    tokenizer.pad_token = tokenizer.eos_token"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "7bdf44c7-7f48-436d-b37d-101211794e9d",
   "metadata": {
    "tags": [],
    "id": "7bdf44c7-7f48-436d-b37d-101211794e9d",
    "ExecuteTime": {
     "start_time": "2024-09-14T15:02:06.935216Z",
     "end_time": "2024-09-14T15:02:06.935216Z"
    }
   },
   "outputs": [],
   "source": [
    "# 아래는 데이터 세트를 구축하는 함수 예시입니다.\n",
    "# 여기서는 datasets 라이브러리의 IMDB 데이터 세트를 사용합니다.\n",
    "# 이 함수는 자체 데이터 세트에서 모델을 학습시키기 위해 사용자 정의되어야 합니다.\n",
    "\n",
    "def build_dataset(\n",
    "    tokenizer,\n",
    "    dataset_name=\"lvwerra/stack-exchange-paired\",\n",
    "    data_dir=\"data/rl\",\n",
    "    split=\"train\"\n",
    "):\n",
    "    \"\"\"\n",
    "    학습을 위한 데이터 세트를 구축합니다. `load_dataset`에서 데이터 세트를 구축하며, 모델을 자신의 데이터 세트로 학습하려면 이 함수를 맞춤화해야 합니다.\n",
    "\n",
    "    인자:\n",
    "        dataset_name (`str`):\n",
    "            적재할 데이터 세트 이름입니다.\n",
    "\n",
    "    반환값:\n",
    "        dataloader (`torch.utils.data.DataLoader`):\n",
    "            데이터 세트에 대한 데이터 로더입니다.\n",
    "    \"\"\"\n",
    "\n",
    "    ds = load_dataset(dataset_name, data_dir=data_dir, split=split)\n",
    "    original_columns = ds.column_names\n",
    "    num_proc = 24\n",
    "\n",
    "    def preprocess_function(examples):\n",
    "        new_examples = {\n",
    "            \"query\": [],\n",
    "            \"input_ids\": [],\n",
    "        }\n",
    "        for question in examples[\"question\"]:\n",
    "            query = \"Question: \" + question + \"\\n\\nAnswer: \"\n",
    "            tokenized_question = tokenizer(query, truncation=True)\n",
    "            new_examples[\"query\"].append(query)\n",
    "            new_examples[\"input_ids\"].append(tokenized_question[\"input_ids\"])\n",
    "\n",
    "        return new_examples\n",
    "\n",
    "    ds = ds.map(\n",
    "        preprocess_function,\n",
    "        batched=True,\n",
    "        num_proc=num_proc,\n",
    "        remove_columns=original_columns,\n",
    "    )\n",
    "    ds = ds.filter(lambda x: len(x[\"input_ids\"]) < 512, batched=False)\n",
    "\n",
    "    ds.set_format(type=\"torch\")\n",
    "    return ds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6cf63b9d-589b-4bd5-9f2d-4e385fd31c76",
   "metadata": {
    "tags": [],
    "id": "6cf63b9d-589b-4bd5-9f2d-4e385fd31c76"
   },
   "outputs": [],
   "source": [
    "def print_number_of_trainable_model_parameters(model):\n",
    "    trainable_model_params = 0\n",
    "    all_model_params = 0\n",
    "    for _, param in model.named_parameters():\n",
    "        all_model_params += param.numel()\n",
    "        if param.requires_grad:\n",
    "            trainable_model_params += param.numel()\n",
    "    return f\"\\ntrainable model parameters: {trainable_model_params}\\nall model parameters: {all_model_params}\\npercentage of trainable model parameters: {100 * trainable_model_params / all_model_params:.2f}%\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62cea29c-a5be-44e0-ad0f-bb1b56a67bf5",
   "metadata": {
    "colab": {
     "referenced_widgets": [
      ""
     ]
    },
    "id": "62cea29c-a5be-44e0-ad0f-bb1b56a67bf5",
    "outputId": "5e3fb966-1d47-44c6-e503-4aeaf52e923f"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Found cached dataset parquet (/root/.cache/huggingface/datasets/lvwerra___parquet/lvwerra--stack-exchange-paired-e5ccc5f74f1da5b7/0.0.0/2a3b91fbd88a2c90d1dbbb32b460cf621d31bd5b05b934492fdef7d8d6f236ec)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map (num_proc=24):   0%|          | 0/7435908 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Asking to truncate to max_length but no maximum length is provided and the model has no predefined maximum length. Default to no truncation.\n",
      "Asking to truncate to max_length but no maximum length is provided and the model has no predefined maximum length. Default to no truncation.\n",
      "Asking to truncate to max_length but no maximum length is provided and the model has no predefined maximum length. Default to no truncation.\n",
      "Asking to truncate to max_length but no maximum length is provided and the model has no predefined maximum length. Default to no truncation.\n",
      "Asking to truncate to max_length but no maximum length is provided and the model has no predefined maximum length. Default to no truncation.\n",
      "Asking to truncate to max_length but no maximum length is provided and the model has no predefined maximum length. Default to no truncation.\n",
      "Asking to truncate to max_length but no maximum length is provided and the model has no predefined maximum length. Default to no truncation.\n",
      "Asking to truncate to max_length but no maximum length is provided and the model has no predefined maximum length. Default to no truncation.\n",
      "Asking to truncate to max_length but no maximum length is provided and the model has no predefined maximum length. Default to no truncation.\n",
      "Asking to truncate to max_length but no maximum length is provided and the model has no predefined maximum length. Default to no truncation.\n",
      "Asking to truncate to max_length but no maximum length is provided and the model has no predefined maximum length. Default to no truncation.\n",
      "Asking to truncate to max_length but no maximum length is provided and the model has no predefined maximum length. Default to no truncation.\n",
      "Asking to truncate to max_length but no maximum length is provided and the model has no predefined maximum length. Default to no truncation.\n",
      "Asking to truncate to max_length but no maximum length is provided and the model has no predefined maximum length. Default to no truncation.\n",
      "Asking to truncate to max_length but no maximum length is provided and the model has no predefined maximum length. Default to no truncation.\n",
      "Asking to truncate to max_length but no maximum length is provided and the model has no predefined maximum length. Default to no truncation.\n",
      "Asking to truncate to max_length but no maximum length is provided and the model has no predefined maximum length. Default to no truncation.\n",
      "Asking to truncate to max_length but no maximum length is provided and the model has no predefined maximum length. Default to no truncation.\n",
      "Asking to truncate to max_length but no maximum length is provided and the model has no predefined maximum length. Default to no truncation.\n",
      "Asking to truncate to max_length but no maximum length is provided and the model has no predefined maximum length. Default to no truncation.\n",
      "Asking to truncate to max_length but no maximum length is provided and the model has no predefined maximum length. Default to no truncation.\n",
      "Asking to truncate to max_length but no maximum length is provided and the model has no predefined maximum length. Default to no truncation.\n",
      "Asking to truncate to max_length but no maximum length is provided and the model has no predefined maximum length. Default to no truncation.\n",
      "Asking to truncate to max_length but no maximum length is provided and the model has no predefined maximum length. Default to no truncation.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Filter:   0%|          | 0/7435908 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# build_dataset 함수를 호출해 데이터 로더를 가져옵니다.\n",
    "train_dataset = build_dataset(tokenizer, \"lvwerra/stack-exchange-paired\", data_dir=\"data/rl\", split=\"train\")\n",
    "\n",
    "def collator(data):\n",
    "    return dict((key, [d[key] for d in data]) for key in data[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a36d960-3d7f-4b84-8d53-1edf906d316e",
   "metadata": {
    "tags": [],
    "id": "5a36d960-3d7f-4b84-8d53-1edf906d316e"
   },
   "outputs": [],
   "source": [
    "# value 헤드를 초기화하기 전에 시드를 설정해 결정적인 평가를 보장합니다.\n",
    "set_seed(config.seed)\n",
    "\n",
    "# 이제 모델, 참조 모델 및 토크나이저를 구축합니다.\n",
    "current_device = Accelerator().local_process_index\n",
    "\n",
    "lora_config = LoraConfig(\n",
    "    r=16,\n",
    "    lora_alpha=32,\n",
    "    lora_dropout=0.05,\n",
    "    bias=\"none\",\n",
    "    task_type=\"CAUSAL_LM\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd0c12ff-05ae-40d1-9e1d-03d8348fe245",
   "metadata": {
    "tags": [],
    "colab": {
     "referenced_widgets": [
      "c107636acaac4e5483d8fd756091b1f3"
     ]
    },
    "id": "dd0c12ff-05ae-40d1-9e1d-03d8348fe245",
    "outputId": "90aafad1-47a9-4ba2-e83b-30add69508ad"
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c107636acaac4e5483d8fd756091b1f3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:362: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.9` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`. This was detected when initializing the generation config instance, which means the corresponding file may hold incorrect parameterization and should be fixed.\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:367: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.6` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`. This was detected when initializing the generation config instance, which means the corresponding file may hold incorrect parameterization and should be fixed.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reference model parameters to be updated:\n",
      "\n",
      "trainable model parameters: 0\n",
      "all model parameters: 6746808321\n",
      "percentage of trainable model parameters: 0.00%\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from trl import create_reference_model\n",
    "\n",
    "model = AutoModelForCausalLMWithValueHead.from_pretrained(\n",
    "    config.model_name,\n",
    "    load_in_8bit=True,\n",
    "    device_map={\"\": current_device},\n",
    "    peft_config=lora_config,\n",
    ")\n",
    "\n",
    "ref_model = create_reference_model(model)\n",
    "print(f'Reference model parameters to be updated:\\n{print_number_of_trainable_model_parameters(ref_model)}\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "530cb8dc-aadc-41cf-aa90-fbb6b7d0b188",
   "metadata": {
    "id": "530cb8dc-aadc-41cf-aa90-fbb6b7d0b188"
   },
   "outputs": [],
   "source": [
    "# 그런 다음 모델, 참조 모델 및 토크나이저를 전달해 PPOTrainer를 구축합니다.\n",
    "optimizer = None\n",
    "if script_args.adafactor:\n",
    "    optimizer = Adafactor(\n",
    "        filter(lambda p: p.requires_grad, model.parameters()),\n",
    "        scale_parameter=False,\n",
    "        relative_step=False,\n",
    "        warmup_init=False,\n",
    "        lr=config.learning_rate,\n",
    "    )\n",
    "\n",
    "ppo_trainer = PPOTrainer(\n",
    "    config,\n",
    "    model,\n",
    "    ref_model=ref_model,\n",
    "    tokenizer=tokenizer,\n",
    "    dataset=train_dataset,\n",
    "    data_collator=collator,\n",
    "    optimizer=optimizer,\n",
    ")\n",
    "\n",
    "\n",
    "# 보상 모델을 사용해 감정 분석 파이프라인을 구축합니다.\n",
    "# 모델 이름과 감정 분석 파이프라인 인자를 전달합니다.\n",
    "# 또한 PPOTrainer와 동일한 장치에 설정되도록 합니다.\n",
    "reward_model_tokenizer = AutoTokenizer.from_pretrained(script_args.reward_model_name)\n",
    "\n",
    "device = ppo_trainer.accelerator.device\n",
    "if ppo_trainer.accelerator.num_processes == 1:\n",
    "    device = 0 if torch.cuda.is_available() else \"cpu\"  # to avoid a ` pipeline` bug\n",
    "sentiment_pipe = pipeline(\n",
    "    \"sentiment-analysis\",\n",
    "    model=script_args.reward_model_name,\n",
    "    device_map={\"\": current_device},\n",
    "    model_kwargs={\"load_in_8bit\": True},\n",
    "    tokenizer=reward_model_tokenizer,\n",
    "    return_token_type_ids=False,\n",
    ")\n",
    "\n",
    "# `generate` 함수에 전달할 인자를 정의합니다.\n",
    "# 이 인자는 PPOTrainer의 `generate` 함수에 전달되며, 이는 학습된 모델의 `generate` 함수에 대한 래퍼입니다.\n",
    "generation_kwargs = {\n",
    "    # \"min_length\": -1,\n",
    "    \"top_k\": 0.0,\n",
    "    \"top_p\": 1.0,\n",
    "    \"do_sample\": True,\n",
    "    \"pad_token_id\": tokenizer.pad_token_id,\n",
    "    \"eos_token_id\": 100_000,\n",
    "}\n",
    "output_min_length = 32\n",
    "output_max_length = script_args.output_max_length\n",
    "output_length_sampler = LengthSampler(output_min_length, output_max_length)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb3a5820-79fd-4388-b848-c1f5e3a32bbf",
   "metadata": {
    "id": "eb3a5820-79fd-4388-b848-c1f5e3a32bbf",
    "outputId": "3106122a-21ae-4176-c28c-227329356f5f"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0it [00:00, ?it/s]You're using a LlamaTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "/opt/conda/lib/python3.10/site-packages/transformers/generation/utils.py:1421: UserWarning: You have modified the pretrained model configuration to control generation. This is a deprecated strategy to control generation and will be removed soon, in a future version. Please use and modify the model generation configuration (see https://huggingface.co/docs/transformers/generation_strategies#default-text-generation-configuration )\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.10/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization\n",
      "  warnings.warn(f\"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization\")\n",
      "/opt/conda/lib/python3.10/site-packages/transformers/pipelines/text_classification.py:105: UserWarning: `return_all_scores` is now deprecated,  if want a similar functionality use `top_k=None` instead of `return_all_scores=True` or `top_k=1` instead of `return_all_scores=False`.\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.10/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization\n",
      "  warnings.warn(f\"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization\")\n",
      "1it [00:24, 24.47s/it]`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`...\n",
      "/opt/conda/lib/python3.10/site-packages/torch/utils/checkpoint.py:31: UserWarning: None of the inputs have requires_grad=True. Gradients will be None\n",
      "  warnings.warn(\"None of the inputs have requires_grad=True. Gradients will be None\")\n",
      "/opt/conda/lib/python3.10/site-packages/transformers/pipelines/text_classification.py:105: UserWarning: `return_all_scores` is now deprecated,  if want a similar functionality use `top_k=None` instead of `return_all_scores=True` or `top_k=1` instead of `return_all_scores=False`.\n",
      "  warnings.warn(\n",
      "6it [02:50, 27.44s/it]/opt/conda/lib/python3.10/site-packages/trl/trainer/ppo_trainer.py:1240: UserWarning: KL divergence is starting to become negative: -2.13 - this might be a precursor for failed training. sometimes this happens because the generation kwargs are not correctly set. Please make sure that the generation kwargs are set correctly, or review your training hyperparameters.\n",
      "  warnings.warn(\n",
      "10it [04:49, 29.62s/it]/opt/conda/lib/python3.10/site-packages/transformers/pipelines/base.py:1101: UserWarning: You seem to be using the pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n",
      "  warnings.warn(\n",
      "16it [07:57, 30.24s/it]/opt/conda/lib/python3.10/site-packages/trl/trainer/ppo_trainer.py:1240: UserWarning: KL divergence is starting to become negative: -1.57 - this might be a precursor for failed training. sometimes this happens because the generation kwargs are not correctly set. Please make sure that the generation kwargs are set correctly, or review your training hyperparameters.\n",
      "  warnings.warn(\n",
      "19it [09:23, 29.34s/it]/opt/conda/lib/python3.10/site-packages/trl/trainer/ppo_trainer.py:1240: UserWarning: KL divergence is starting to become negative: -1.38 - this might be a precursor for failed training. sometimes this happens because the generation kwargs are not correctly set. Please make sure that the generation kwargs are set correctly, or review your training hyperparameters.\n",
      "  warnings.warn(\n",
      "20it [10:00, 31.64s/it]/opt/conda/lib/python3.10/site-packages/trl/trainer/ppo_trainer.py:1240: UserWarning: KL divergence is starting to become negative: -1.28 - this might be a precursor for failed training. sometimes this happens because the generation kwargs are not correctly set. Please make sure that the generation kwargs are set correctly, or review your training hyperparameters.\n",
      "  warnings.warn(\n",
      "32it [15:53, 31.17s/it]/opt/conda/lib/python3.10/site-packages/trl/trainer/ppo_trainer.py:1240: UserWarning: KL divergence is starting to become negative: -1.50 - this might be a precursor for failed training. sometimes this happens because the generation kwargs are not correctly set. Please make sure that the generation kwargs are set correctly, or review your training hyperparameters.\n",
      "  warnings.warn(\n",
      "50it [23:53, 27.66s/it]/opt/conda/lib/python3.10/site-packages/trl/trainer/ppo_trainer.py:1240: UserWarning: KL divergence is starting to become negative: -1.08 - this might be a precursor for failed training. sometimes this happens because the generation kwargs are not correctly set. Please make sure that the generation kwargs are set correctly, or review your training hyperparameters.\n",
      "  warnings.warn(\n",
      "60it [27:34, 21.62s/it]/opt/conda/lib/python3.10/site-packages/trl/trainer/ppo_trainer.py:1240: UserWarning: KL divergence is starting to become negative: -1.56 - this might be a precursor for failed training. sometimes this happens because the generation kwargs are not correctly set. Please make sure that the generation kwargs are set correctly, or review your training hyperparameters.\n",
      "  warnings.warn(\n",
      "68it [30:54, 24.92s/it]/opt/conda/lib/python3.10/site-packages/trl/trainer/ppo_trainer.py:1240: UserWarning: KL divergence is starting to become negative: -1.98 - this might be a precursor for failed training. sometimes this happens because the generation kwargs are not correctly set. Please make sure that the generation kwargs are set correctly, or review your training hyperparameters.\n",
      "  warnings.warn(\n",
      "71it [32:07, 23.59s/it]/opt/conda/lib/python3.10/site-packages/trl/trainer/ppo_trainer.py:1240: UserWarning: KL divergence is starting to become negative: -1.05 - this might be a precursor for failed training. sometimes this happens because the generation kwargs are not correctly set. Please make sure that the generation kwargs are set correctly, or review your training hyperparameters.\n",
      "  warnings.warn(\n",
      "88it [39:55, 28.02s/it]/opt/conda/lib/python3.10/site-packages/trl/trainer/ppo_trainer.py:1240: UserWarning: KL divergence is starting to become negative: -2.38 - this might be a precursor for failed training. sometimes this happens because the generation kwargs are not correctly set. Please make sure that the generation kwargs are set correctly, or review your training hyperparameters.\n",
      "  warnings.warn(\n",
      "97it [44:03, 28.99s/it]/opt/conda/lib/python3.10/site-packages/trl/trainer/ppo_trainer.py:1240: UserWarning: KL divergence is starting to become negative: -1.15 - this might be a precursor for failed training. sometimes this happens because the generation kwargs are not correctly set. Please make sure that the generation kwargs are set correctly, or review your training hyperparameters.\n",
      "  warnings.warn(\n",
      "100it [45:31, 27.31s/it]\n"
     ]
    }
   ],
   "source": [
    "for epoch, batch in tqdm(enumerate(ppo_trainer.dataloader)):\n",
    "    if epoch >= config.total_ppo_epochs:\n",
    "        break\n",
    "\n",
    "    question_tensors = batch[\"input_ids\"]\n",
    "\n",
    "    response_tensors = ppo_trainer.generate(\n",
    "        question_tensors,\n",
    "        return_prompt=False,\n",
    "        length_sampler=output_length_sampler,\n",
    "        **generation_kwargs,\n",
    "    )\n",
    "    batch[\"response\"] = tokenizer.batch_decode(response_tensors, skip_special_tokens=True)\n",
    "\n",
    "    # 감정 분석 파이프라인을 사용해 보상 점수를 계산합니다.\n",
    "    texts = [q + r for q, r in zip(batch[\"query\"], batch[\"response\"])]\n",
    "    pipe_outputs = sentiment_pipe(texts, **sent_kwargs)\n",
    "    rewards = [torch.tensor(output[0][\"score\"] - script_args.reward_baseline) for output in pipe_outputs]\n",
    "\n",
    "    # PPO 단계 수행\n",
    "    stats = ppo_trainer.step(question_tensors, response_tensors, rewards)\n",
    "    ppo_trainer.log_stats(stats, batch, rewards)\n",
    "\n",
    "    if script_args.save_freq and epoch and epoch % script_args.save_freq == 0:\n",
    "        #ppo_trainer.save_pretrained(script_args.output_dir + f\"step_{epoch}\")\n",
    "        ppo_trainer.tokenizer.save_pretrained(peft_fine_tuned_with_ranking_rewards_llama2_checkpoint)\n",
    "        ppo_trainer.accelerator.unwrap_model(ppo_trainer.model).save_pretrained(peft_fine_tuned_with_ranking_rewards_llama2_checkpoint) # merge\n",
    "        #ppo_trainer.model.save_pretrained(peft_fine_tuned_with_detoxification_rewards_checkpoint)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19af42bc-f1b4-4079-94ae-43e2369d4a33",
   "metadata": {
    "id": "19af42bc-f1b4-4079-94ae-43e2369d4a33"
   },
   "outputs": [],
   "source": [
    "ppo_trainer.tokenizer.save_pretrained(peft_fine_tuned_with_ranking_rewards_llama2_checkpoint)\n",
    "ppo_trainer.accelerator.unwrap_model(ppo_trainer.model).save_pretrained(peft_fine_tuned_with_ranking_rewards_llama2_checkpoint) # merge?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef44c530-a091-4eb9-ac05-1f58a74a8f63",
   "metadata": {
    "id": "ef44c530-a091-4eb9-ac05-1f58a74a8f63"
   },
   "source": [
    "# 평가"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44d9784f-2b92-466a-abcb-de36c3cd4aa2",
   "metadata": {
    "tags": [],
    "id": "44d9784f-2b92-466a-abcb-de36c3cd4aa2"
   },
   "outputs": [],
   "source": [
    "from transformers import pipeline, AutoTokenizer, AutoModelForSequenceClassification, AutoModelForCausalLM, GenerationConfig\n",
    "from datasets import load_dataset\n",
    "from peft import PeftModel, PeftConfig, LoraConfig, TaskType, AutoPeftModelForCausalLM\n",
    "\n",
    "import torch\n",
    "import evaluate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ed096ec-fa04-4a89-a729-d6765886f0bf",
   "metadata": {
    "colab": {
     "referenced_widgets": [
      "f34c79b14dba4fe8997c4df813afc9a6"
     ]
    },
    "id": "7ed096ec-fa04-4a89-a729-d6765886f0bf",
    "outputId": "d559fcea-cc6a-47e0-ecfa-40ac86ecec3f"
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f34c79b14dba4fe8997c4df813afc9a6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "ppo_model = AutoPeftModelForCausalLM.from_pretrained(\n",
    "    peft_fine_tuned_with_ranking_rewards_llama2_checkpoint,\n",
    "    torch_dtype=torch.float16,\n",
    "    device_map=\"auto\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46f46c0e-85fb-4860-9114-10fd906d913f",
   "metadata": {
    "tags": [],
    "colab": {
     "referenced_widgets": [
      "984df35b5218415e88d57be5300026df"
     ]
    },
    "id": "46f46c0e-85fb-4860-9114-10fd906d913f",
    "outputId": "d9e08592-4663-49a3-c4bc-03a99ab67b2e"
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "984df35b5218415e88d57be5300026df",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "ref_model = AutoModelForCausalLM.from_pretrained(\n",
    "    script_args.model_name,\n",
    "    torch_dtype=torch.float16,\n",
    "    device_map=\"auto\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23a69baa-2b18-465b-9de3-8aa601f3eec4",
   "metadata": {
    "tags": [],
    "id": "23a69baa-2b18-465b-9de3-8aa601f3eec4",
    "outputId": "6203cb34-85e5-4d09-935a-62825c54789e"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{0: 'nothate', 1: 'hate'}\n"
     ]
    }
   ],
   "source": [
    "toxicity_model_name = \"facebook/roberta-hate-speech-dynabench-r4-target\"\n",
    "toxicity_tokenizer = AutoTokenizer.from_pretrained(\n",
    "    toxicity_model_name,\n",
    "    device_map=\"auto\"\n",
    ")\n",
    "toxicity_model = AutoModelForSequenceClassification.from_pretrained(\n",
    "    toxicity_model_name,\n",
    "    device_map=\"auto\"\n",
    ")\n",
    "print(toxicity_model.config.id2label)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8fd8c0b-21be-47cb-b529-27f67fec9b5b",
   "metadata": {
    "tags": [],
    "id": "a8fd8c0b-21be-47cb-b529-27f67fec9b5b"
   },
   "source": [
    "유해하지 않는 텍스트를 가져와서 토큰화하고 모델에게 전달합니다. 출력 로짓값, 확률 및 미세 조정을 위해 사용할 보상 점수를 출력합니다.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b9619ff-6d77-41f5-8be4-57d938a0fd20",
   "metadata": {
    "tags": [],
    "id": "2b9619ff-6d77-41f5-8be4-57d938a0fd20",
    "outputId": "c03bf13a-e906-45bc-c5d7-588a0b364106"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "logits [not hate, hate]: [4.6532111167907715, -4.178227424621582]\n",
      "probabilities [not hate, hate]: [0.9998539686203003, 0.0001460467028664425]\n",
      "reward (value of \"not hate\" logit): [4.6532111167907715]\n"
     ]
    }
   ],
   "source": [
    "non_toxic_text = \"You are a great person and i like you.\"\n",
    "\n",
    "toxicity_input_ids = toxicity_tokenizer(non_toxic_text, return_tensors=\"pt\").input_ids\n",
    "\n",
    "logits = toxicity_model(input_ids=toxicity_input_ids).logits\n",
    "print(f'logits [not hate, hate]: {logits.tolist()[0]}')\n",
    "\n",
    "# [혐오 아님, 혐오]에 대한 확률 출력\n",
    "probabilities = logits.softmax(dim=-1).tolist()[0]\n",
    "print(f'probabilities [not hate, hate]: {probabilities}')\n",
    "\n",
    "# '혐오 아님'에 대한 로짓값을 가져옵니다 - 이것이 보상입니다!\n",
    "not_hate_index = 0\n",
    "nothate_reward = (logits[:, not_hate_index]).tolist()\n",
    "print(f'reward (value of \"not hate\" logit): {nothate_reward}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "663f515b-b314-4eaf-8d2a-31895e0d167a",
   "metadata": {
    "id": "663f515b-b314-4eaf-8d2a-31895e0d167a"
   },
   "source": [
    "유해한 댓글을 보여주겠습니다. 이는 더 유해하므로 보상이 낮을 것입니다.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5038a63b-8129-482b-acab-72ec34245afb",
   "metadata": {
    "tags": [],
    "id": "5038a63b-8129-482b-acab-72ec34245afb",
    "outputId": "d53aa2ab-ae87-4311-e12e-aa74d72a9246"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "logits [not hate, hate]: [-2.064443349838257, 1.6650441884994507]\n",
      "probabilities [not hate, hate]: [0.023442398756742477, 0.9765575528144836]\n",
      "reward (value of \"not hate\" logit): [-2.064443349838257]\n"
     ]
    }
   ],
   "source": [
    "toxic_text = \"You are a terrible person and i hate you.\"\n",
    "\n",
    "toxicity_input_ids = toxicity_tokenizer(toxic_text, return_tensors=\"pt\").input_ids\n",
    "\n",
    "logits = toxicity_model(toxicity_input_ids).logits\n",
    "print(f'logits [not hate, hate]: {logits.tolist()[0]}')\n",
    "\n",
    "# [혐오 아님, 혐오]에 대한 확률 출력\n",
    "probabilities = logits.softmax(dim=-1).tolist()[0]\n",
    "print(f'probabilities [not hate, hate]: {probabilities}')\n",
    "\n",
    "# '혐오 아님'에 대한 로짓값을 가져옵니다 - 이것이 보상입니다!\n",
    "nothate_reward = (logits[:, not_hate_index]).tolist()\n",
    "print(f'reward (value of \"not hate\" logit): {nothate_reward}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d229e83a-cddb-4e44-9953-a3f57e1a1221",
   "metadata": {
    "id": "d229e83a-cddb-4e44-9953-a3f57e1a1221"
   },
   "source": [
    "유해성 보상 모델을 위한 코드를 단순화하기 위해 허깅 페이스 추론 파이프라인을 설정하세요."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf46e7ad-011b-4fab-ad2b-be01c5831219",
   "metadata": {
    "tags": [],
    "id": "cf46e7ad-011b-4fab-ad2b-be01c5831219",
    "outputId": "cd0cb7fc-f03b-4ead-8638-c8b62d54529b"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reward model output for non-toxic text:\n",
      "[{'label': 'nothate', 'score': 4.6532111167907715}, {'label': 'hate', 'score': -4.178227424621582}]\n",
      "[{'label': 'nothate', 'score': 0.9998539686203003}, {'label': 'hate', 'score': 0.0001460467028664425}]\n",
      "\n",
      "Reward model output for toxic text:\n",
      "[{'label': 'hate', 'score': 1.6650441884994507}, {'label': 'nothate', 'score': -2.064443349838257}]\n",
      "[{'label': 'hate', 'score': 0.9765575528144836}, {'label': 'nothate', 'score': 0.023442398756742477}]\n"
     ]
    }
   ],
   "source": [
    "device = 0 if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "sentiment_pipe = pipeline(\"sentiment-analysis\",\n",
    "                          model=toxicity_model_name,\n",
    "                          device=device)\n",
    "reward_logits_kwargs = {\n",
    "    \"top_k\": None, # 모든 점수를 반환합니다.\n",
    "    \"function_to_apply\": \"none\", # \"none\"으로 설정해 원시 로짓값을 검색합니다.\n",
    "    \"batch_size\": 16\n",
    "}\n",
    "\n",
    "reward_probabilities_kwargs = {\n",
    "    \"top_k\": None, # 모든 점수를 반환합니다.\n",
    "    \"function_to_apply\": \"softmax\", # \"softmax\"로 설정해 소프트맥스를 적용하고 확률을 검색합니다.\n",
    "    \"batch_size\": 16\n",
    "}\n",
    "\n",
    "print(\"Reward model output for non-toxic text:\")\n",
    "print(sentiment_pipe(non_toxic_text, **reward_logits_kwargs))\n",
    "print(sentiment_pipe(non_toxic_text, **reward_probabilities_kwargs))\n",
    "print(\"\\nReward model output for toxic text:\")\n",
    "print(sentiment_pipe(toxic_text, **reward_logits_kwargs))\n",
    "print(sentiment_pipe(toxic_text, **reward_probabilities_kwargs))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b98528a1-3f1d-45d8-8511-20dff9e86ae8",
   "metadata": {
    "id": "b98528a1-3f1d-45d8-8511-20dff9e86ae8"
   },
   "source": [
    "출력은 `nothate`(긍정) 클래스와 `hate`(부정) 클래스 모두에 대한 로짓값입니다. 하지만 PPO는 LLM 출력의 유해성을 줄이는 데 도움이 되는 긍정적인 보상 신호로 `nothate` 클래스의 로짓값만 사용할 것입니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9fcfb315-775f-40f2-8a2d-3dfc2ce28f9d",
   "metadata": {
    "id": "9fcfb315-775f-40f2-8a2d-3dfc2ce28f9d",
    "outputId": "e8761c38-f2dc-46a3-a698-fa92835dd4bf"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'label': 'nothate', 'score': 4.6532111167907715}, {'label': 'hate', 'score': -4.178227424621582}]\n",
      "[{'label': 'nothate', 'score': 0.9998539686203003}, {'label': 'hate', 'score': 0.0001460467028664425}]\n"
     ]
    }
   ],
   "source": [
    "print(sentiment_pipe(non_toxic_text, **reward_logits_kwargs))\n",
    "print(sentiment_pipe(non_toxic_text, **reward_probabilities_kwargs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98eccf2b-9804-49a9-b5f2-d58f1c51d662",
   "metadata": {
    "id": "98eccf2b-9804-49a9-b5f2-d58f1c51d662",
    "outputId": "4a6808d6-58ef-40b1-d6da-3317a280121d"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'label': 'hate', 'score': 1.6650441884994507}, {'label': 'nothate', 'score': -2.064443349838257}]\n",
      "[{'label': 'hate', 'score': 0.9765575528144836}, {'label': 'nothate', 'score': 0.023442398756742477}]\n"
     ]
    }
   ],
   "source": [
    "print(sentiment_pipe(toxic_text, **reward_logits_kwargs))\n",
    "print(sentiment_pipe(toxic_text, **reward_probabilities_kwargs))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3cdfc6dc-edb4-448f-88ff-6bba3fe9180f",
   "metadata": {
    "tags": [],
    "id": "3cdfc6dc-edb4-448f-88ff-6bba3fe9180f"
   },
   "source": [
    "<a name='2.3'></a>\n",
    "### 2.3. - 유해성 평가\n",
    "\n",
    "미세 조정/유해성 제거 전후의 모델을 평가하려면 [유해성 평가 지표](https://huggingface.co/spaces/evaluate-measurement/toxicity)를 설정해야 합니다. **유해성 점수**는 0에서 1 사이의 소수점 값으로, 1이 가장 높은 유해성을 나타냅니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b75b272b-0758-4e4c-8a3b-213d4971b668",
   "metadata": {
    "tags": [],
    "id": "b75b272b-0758-4e4c-8a3b-213d4971b668"
   },
   "outputs": [],
   "source": [
    "toxicity_evaluator = evaluate.load(\"toxicity\",\n",
    "                                    toxicity_model_name,\n",
    "                                    module_type=\"measurement\",\n",
    "                                    toxic_label=\"hate\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b2b36bb-77d7-4103-a0d3-e0235c0feedb",
   "metadata": {
    "tags": [],
    "id": "9b2b36bb-77d7-4103-a0d3-e0235c0feedb"
   },
   "source": [
    "[2.2](#2.2) 섹션에서 사용한 문장들에 대한 유해성을 계산해 보십시오. 유해성 점수는 보상 모델에서 직접 반환된 `혐오` 클래스의 확률과 같습니다.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "967655d2-ac62-437d-a056-c9bd8a02bdd1",
   "metadata": {
    "tags": [],
    "id": "967655d2-ac62-437d-a056-c9bd8a02bdd1",
    "outputId": "86e8528d-567b-4be7-d98a-a386c9730bd0"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Toxicity score for non-toxic text:\n",
      "[0.00014604683383367956]\n",
      "\n",
      "Toxicity score for toxic text:\n",
      "[0.9765576720237732]\n"
     ]
    }
   ],
   "source": [
    "toxicity_score = toxicity_evaluator.compute(predictions=[\n",
    "    non_toxic_text\n",
    "])\n",
    "\n",
    "print(\"Toxicity score for non-toxic text:\")\n",
    "print(toxicity_score[\"toxicity\"])\n",
    "\n",
    "toxicity_score = toxicity_evaluator.compute(predictions=[\n",
    "    toxic_text\n",
    "])\n",
    "\n",
    "print(\"\\nToxicity score for toxic text:\")\n",
    "print(toxicity_score[\"toxicity\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68cf2bd0-9622-4db8-8e2e-9c37ae7ac4a8",
   "metadata": {
    "tags": [],
    "id": "68cf2bd0-9622-4db8-8e2e-9c37ae7ac4a8"
   },
   "source": [
    "이 평가기는 [2.1](#2.1) 섹션에서 준비한 대화의 유해성을 계산하는 데 사용될 수 있습니다. 테스트 데이터 세트(`dataset[\"test\"]`), 해당 섹션에서 사용한 것과 동일한 토크나이저, [2.2](#2.2) 섹션에서 준비한 고정된 PEFT 모델, 그리고 유해성 평가기를 전달해야 합니다. 필요한 단계를 `evaluate_toxicity` 함수로 감싸는 것이 편리합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3de48105-df1e-4bb7-8e04-8a37c67b3aff",
   "metadata": {
    "tags": [],
    "colab": {
     "referenced_widgets": [
      ""
     ]
    },
    "id": "3de48105-df1e-4bb7-8e04-8a37c67b3aff",
    "outputId": "ce61ed99-eac9-46cb-e59e-0eba02466351"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:datasets.builder:Found cached dataset parquet (/root/.cache/huggingface/datasets/lvwerra___parquet/lvwerra--stack-exchange-paired-6fbcbcc16115b7c8/0.0.0/2a3b91fbd88a2c90d1dbbb32b460cf621d31bd5b05b934492fdef7d8d6f236ec)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map (num_proc=24):   0%|          | 0/4483004 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Asking to truncate to max_length but no maximum length is provided and the model has no predefined maximum length. Default to no truncation.\n",
      "Asking to truncate to max_length but no maximum length is provided and the model has no predefined maximum length. Default to no truncation.\n",
      "Asking to truncate to max_length but no maximum length is provided and the model has no predefined maximum length. Default to no truncation.\n",
      "Asking to truncate to max_length but no maximum length is provided and the model has no predefined maximum length. Default to no truncation.\n",
      "Asking to truncate to max_length but no maximum length is provided and the model has no predefined maximum length. Default to no truncation.\n",
      "Asking to truncate to max_length but no maximum length is provided and the model has no predefined maximum length. Default to no truncation.\n",
      "Asking to truncate to max_length but no maximum length is provided and the model has no predefined maximum length. Default to no truncation.\n",
      "Asking to truncate to max_length but no maximum length is provided and the model has no predefined maximum length. Default to no truncation.\n",
      "Asking to truncate to max_length but no maximum length is provided and the model has no predefined maximum length. Default to no truncation.\n",
      "Asking to truncate to max_length but no maximum length is provided and the model has no predefined maximum length. Default to no truncation.\n",
      "Asking to truncate to max_length but no maximum length is provided and the model has no predefined maximum length. Default to no truncation.\n",
      "Asking to truncate to max_length but no maximum length is provided and the model has no predefined maximum length. Default to no truncation.\n",
      "Asking to truncate to max_length but no maximum length is provided and the model has no predefined maximum length. Default to no truncation.\n",
      "Asking to truncate to max_length but no maximum length is provided and the model has no predefined maximum length. Default to no truncation.\n",
      "Asking to truncate to max_length but no maximum length is provided and the model has no predefined maximum length. Default to no truncation.\n",
      "Asking to truncate to max_length but no maximum length is provided and the model has no predefined maximum length. Default to no truncation.\n",
      "Asking to truncate to max_length but no maximum length is provided and the model has no predefined maximum length. Default to no truncation.\n",
      "Asking to truncate to max_length but no maximum length is provided and the model has no predefined maximum length. Default to no truncation.\n",
      "Asking to truncate to max_length but no maximum length is provided and the model has no predefined maximum length. Default to no truncation.\n",
      "Asking to truncate to max_length but no maximum length is provided and the model has no predefined maximum length. Default to no truncation.\n",
      "Asking to truncate to max_length but no maximum length is provided and the model has no predefined maximum length. Default to no truncation.\n",
      "Asking to truncate to max_length but no maximum length is provided and the model has no predefined maximum length. Default to no truncation.\n",
      "Asking to truncate to max_length but no maximum length is provided and the model has no predefined maximum length. Default to no truncation.\n",
      "Asking to truncate to max_length but no maximum length is provided and the model has no predefined maximum length. Default to no truncation.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Filter:   0%|          | 0/4483004 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "test_dataset = build_dataset(tokenizer, \"lvwerra/stack-exchange-paired\", data_dir=\"data/evaluation\", split=\"train\")\n",
    "test_dataset = test_dataset.select(range(1000))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d90b06ca-af87-4a14-9e02-3f72f22446b7",
   "metadata": {
    "tags": [],
    "id": "d90b06ca-af87-4a14-9e02-3f72f22446b7"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def evaluate_toxicity(model,\n",
    "                      toxicity_evaluator,\n",
    "                      tokenizer,\n",
    "                      dataset,\n",
    "                      num_samples):\n",
    "\n",
    "    max_new_tokens=100\n",
    "\n",
    "    toxicities = []\n",
    "    input_texts = []\n",
    "    for i, sample in tqdm(enumerate(dataset)):\n",
    "        input_text = sample[\"query\"]\n",
    "\n",
    "        if i > num_samples:\n",
    "            break\n",
    "\n",
    "        input_ids = tokenizer(input_text, return_tensors=\"pt\", padding=True).input_ids.to(device)\n",
    "\n",
    "        generation_config = GenerationConfig(max_new_tokens=max_new_tokens,\n",
    "                                             tok_k=0.0,\n",
    "                                             top_p=1.0,\n",
    "                                             do_sample=True)\n",
    "\n",
    "        response_token_ids = model.generate(input_ids=input_ids,\n",
    "                                            generation_config=generation_config)\n",
    "\n",
    "        generated_text = tokenizer.decode(response_token_ids[0], skip_special_tokens=True)\n",
    "\n",
    "        toxicity_score = toxicity_evaluator.compute(predictions=[(input_text + \" \" + generated_text)])\n",
    "\n",
    "        toxicities.extend(toxicity_score[\"toxicity\"])\n",
    "\n",
    "    # 평균 및 표준 편차는 np를 사용해 계산.\n",
    "    mean = np.mean(toxicities)\n",
    "    std = np.std(toxicities)\n",
    "\n",
    "    return mean, std"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6110249-18ba-4c32-97f6-347b4ad3f9af",
   "metadata": {
    "tags": [],
    "id": "a6110249-18ba-4c32-97f6-347b4ad3f9af"
   },
   "source": [
    "이제 미세 조정/유해성 제거 전에 모델 유해성을 계산해 보겠습니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a90da8d8-e953-48ae-88cb-74b942d4a856",
   "metadata": {
    "tags": [],
    "id": "a90da8d8-e953-48ae-88cb-74b942d4a856",
    "outputId": "6a142086-8b85-4591-8a85-a8b54be2692e"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "101it [07:13,  4.29s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "toxicity [mean, std] before detox: [0.01928018110269441, 0.01788940742928178]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(script_args.model_name, device_map=\"auto\")\n",
    "\n",
    "mean_before_detoxification, std_before_detoxification = evaluate_toxicity(model=ref_model,\n",
    "                                                                          toxicity_evaluator=toxicity_evaluator,\n",
    "                                                                          tokenizer=tokenizer,\n",
    "                                                                          dataset=test_dataset,\n",
    "                                                                          num_samples=100)\n",
    "\n",
    "print(f'toxicity [mean, std] before detox: [{mean_before_detoxification}, {std_before_detoxification}]')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5efbec56-5339-4588-ad72-2a6b81bf24a0",
   "metadata": {
    "tags": [],
    "id": "5efbec56-5339-4588-ad72-2a6b81bf24a0",
    "outputId": "84816e95-cffe-453b-ce73-ea45852a2d11"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "101it [08:40,  5.16s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "toxicity [mean, std] after detox: [0.018666810890899437, 0.019985045280750742]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "mean_after_detoxification, std_after_detoxification = evaluate_toxicity(model=ppo_model,\n",
    "                                                                        toxicity_evaluator=toxicity_evaluator,\n",
    "                                                                        tokenizer=tokenizer,\n",
    "                                                                        dataset=test_dataset,\n",
    "                                                                        num_samples=100)\n",
    "print(f'toxicity [mean, std] after detox: [{mean_after_detoxification}, {std_after_detoxification}]')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "045f4b42-5a32-41bf-ac07-84b842c53f77",
   "metadata": {
    "tags": [],
    "id": "045f4b42-5a32-41bf-ac07-84b842c53f77"
   },
   "source": [
    "\n",
    "그리고 참조 모델(유해성 제거 전)과 미세 조정된 모델(유해성 제거 이후)의 유해성 점수를 비교합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a0239e8-9e37-4405-8083-a0dd1b2f05d2",
   "metadata": {
    "id": "6a0239e8-9e37-4405-8083-a0dd1b2f05d2",
    "outputId": "c1f0f39b-902f-41a7-8ece-6ecabc53989b"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Percentage improvement of toxicity score after detoxification:\n",
      "mean: 3.18%\n",
      "std: -11.71%\n"
     ]
    }
   ],
   "source": [
    "mean_improvement = (mean_before_detoxification - mean_after_detoxification) / mean_before_detoxification\n",
    "std_improvement = (std_before_detoxification - std_after_detoxification) / std_before_detoxification\n",
    "\n",
    "print(f'Percentage improvement of toxicity score after detoxification:')\n",
    "print(f'mean: {mean_improvement*100:.2f}%')\n",
    "print(f'std: {std_improvement*100:.2f}%')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d92d5fe-7bd5-44f7-a671-dbb17284f66f",
   "metadata": {
    "id": "3d92d5fe-7bd5-44f7-a671-dbb17284f66f"
   },
   "source": [
    "<a name='3.4'></a>\n",
    "### 3.4 - 모델의 정성적 평가\n",
    "\n",
    "테스트 데이터 세트에서 몇 가지 예를 살펴보겠습니다. 유해성 평가기를 사용해 원래의 `ref_model`과 미세 조정/유해성이 제거된 `ppo_model`을 비교할 수 있습니다."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d36c18b1-449e-4e85-9548-8628343b29a1",
   "metadata": {
    "id": "d36c18b1-449e-4e85-9548-8628343b29a1"
   },
   "source": [
    "<img src=\"data:image/svg+xml;base64,Cjxzdmcgd2lkdGg9IjgwMCIgaGVpZ2h0PSIxMjUiIHZpZXdCb3g9IjAgMCA4MDAgMTI1IiB4bWxucz0iaHR0cDovL3d3dy53My5vcmcvMjAwMC9zdmciPgogICAgPGRlZnM+CiAgICAgICAgPGxpbmVhckdyYWRpZW50IGlkPSJmYWRlR3JhZGllbnQiIHgxPSIwIiB4Mj0iMSI+CiAgICAgICAgICAgIDxzdG9wIG9mZnNldD0iMCUiIHN0b3AtY29sb3I9IiNGMEYwRjAiLz4KICAgICAgICAgICAgPHN0b3Agb2Zmc2V0PSIxMDAlIiBzdG9wLWNvbG9yPSIjRjBGMEYwIiBzdG9wLW9wYWNpdHk9IjAiLz4KICAgICAgICA8L2xpbmVhckdyYWRpZW50PgogICAgICAgIDxtYXNrIGlkPSJmYWRlTWFzayI+CiAgICAgICAgICAgIDxyZWN0IHg9IjAiIHk9IjAiIHdpZHRoPSI3NTAiIGhlaWdodD0iMTI1IiBmaWxsPSJ3aGl0ZSIvPgogICAgICAgICAgICA8cmVjdCB4PSI3NTAiIHk9IjAiIHdpZHRoPSI1MCIgaGVpZ2h0PSIxMjUiIGZpbGw9InVybCgjZmFkZUdyYWRpZW50KSIvPgogICAgICAgIDwvbWFzaz4KICAgIDwvZGVmcz4KICAgIDxwYXRoIGQ9Ik0zLDUwIEE1MCw1MCAwIDAgMSA1MywzIEw3OTcsMyBMNzk3LDk3IEw5Nyw5NyBMNTAsMTE1IEwzLDk3IFoiIGZpbGw9IiNGMEYwRjAiIHN0cm9rZT0iI0UwRTBFMCIgc3Ryb2tlLXdpZHRoPSIxIiBtYXNrPSJ1cmwoI2ZhZGVNYXNrKSIvPgogICAgPGNpcmNsZSBjeD0iNTAiIGN5PSI1MCIgcj0iMzAiIGZpbGw9IiM1N2M0ZjgiIHN0cm9rZT0iIzU3YzRmOCIgc3Ryb2tlLXdpZHRoPSIxIi8+CiAgICA8Y2lyY2xlIGN4PSI1MCIgY3k9IjUwIiByPSIyNSIgZmlsbD0iI0YwRjBGMCIvPgogICAgPGxpbmUgeDE9IjUwIiB5MT0iNTAiIHgyPSI1MCIgeTI9IjMwIiBzdHJva2U9IiM1N2M0ZjgiIHN0cm9rZS13aWR0aD0iMyIgc3Ryb2tlLWxpbmVjYXA9InJvdW5kIi8+CiAgICA8bGluZSB4MT0iNTAiIHkxPSI1MCIgeDI9IjY1IiB5Mj0iNTAiIHN0cm9rZT0iIzU3YzRmOCIgc3Ryb2tlLXdpZHRoPSIzIiBzdHJva2UtbGluZWNhcD0icm91bmQiLz4KICAgIDx0ZXh0IHg9IjEwMCIgeT0iMzQiIGZvbnQtZmFtaWx5PSJBcmlhbCwgc2Fucy1zZXJpZiIgZm9udC1zaXplPSIxNCIgZmlsbD0iIzMzMzMzMyI+VGhlIG5leHQgY2VsbCBtYXkgdGFrZSAyLTMgbWludXRlcyB0byBydW4uPC90ZXh0Pgo8L3N2Zz4K\" alt=\"Time alert open medium\"/>\n",
    "​"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f49d852-2d35-4c1f-8299-626e6dbcadcd",
   "metadata": {
    "id": "9f49d852-2d35-4c1f-8299-626e6dbcadcd",
    "outputId": "72f45ee0-b7d6-4edc-8706-df7198167f2f"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 100/100 [12:12<00:00,  7.32s/it]\n"
     ]
    }
   ],
   "source": [
    "batch_size = 100\n",
    "compare_results = {}\n",
    "\n",
    "df_batch = test_dataset[0:batch_size]\n",
    "\n",
    "compare_results[\"query\"] = df_batch[\"query\"]\n",
    "prompt_tensors = df_batch[\"input_ids\"]\n",
    "\n",
    "summary_tensors_ref = []\n",
    "summary_tensors = []\n",
    "\n",
    "# ppo 및 기본 모델에서 응답을 가져옵니다.\n",
    "for i in tqdm(range(batch_size)):\n",
    "    gen_len = output_length_sampler()\n",
    "    generation_kwargs[\"max_new_tokens\"] = gen_len\n",
    "\n",
    "    summary = ref_model.generate(\n",
    "        input_ids=torch.as_tensor(prompt_tensors[i]).unsqueeze(dim=0).to(device),\n",
    "        **generation_kwargs\n",
    "    ).squeeze()[-gen_len:]\n",
    "    summary_tensors_ref.append(summary)\n",
    "\n",
    "    summary = ppo_model.generate(\n",
    "        input_ids=torch.as_tensor(prompt_tensors[i]).unsqueeze(dim=0).to(device),\n",
    "        **generation_kwargs\n",
    "    ).squeeze()[-gen_len:]\n",
    "    summary_tensors.append(summary)\n",
    "\n",
    "# 응답을 디코딩.\n",
    "compare_results[\"response_before\"] = [tokenizer.decode(summary_tensors_ref[i]) for i in range(batch_size)]\n",
    "compare_results[\"response_after\"] = [tokenizer.decode(summary_tensors[i]) for i in range(batch_size)]\n",
    "\n",
    "# 쿼리/응답 쌍의 감정 분석을 유해성 제거 이전과 이후로 수행합니다.\n",
    "texts_before = [d + s for d, s in zip(compare_results[\"query\"], compare_results[\"response_before\"])]\n",
    "rewards_before = sentiment_pipe(texts_before, **sent_kwargs)\n",
    "compare_results[\"reward_before\"] = [reward[not_hate_index][\"score\"] for reward in rewards_before]\n",
    "\n",
    "texts_after = [d + s for d, s in zip(compare_results[\"query\"], compare_results[\"response_after\"])]\n",
    "rewards_after = sentiment_pipe(texts_after, **sent_kwargs)\n",
    "compare_results[\"reward_after\"] = [reward[not_hate_index][\"score\"] for reward in rewards_after]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b78cfaac-e7da-4179-86c5-5ac1c85744a9",
   "metadata": {
    "tags": [],
    "id": "b78cfaac-e7da-4179-86c5-5ac1c85744a9"
   },
   "source": [
    "<img src=\"data:image/svg+xml;base64,Cjxzdmcgd2lkdGg9IjgwMCIgaGVpZ2h0PSI1MCIgdmlld0JveD0iMCAwIDgwMCA1MCIgeG1sbnM9Imh0dHA6Ly93d3cudzMub3JnLzIwMDAvc3ZnIj4KICAgIDxkZWZzPgogICAgICAgIDxsaW5lYXJHcmFkaWVudCBpZD0iZmFkZUdyYWRpZW50IiB4MT0iMCIgeDI9IjEiPgogICAgICAgICAgICA8c3RvcCBvZmZzZXQ9IjAlIiBzdG9wLWNvbG9yPSIjRjBGMEYwIi8+CiAgICAgICAgICAgIDxzdG9wIG9mZnNldD0iMTAwJSIgc3RvcC1jb2xvcj0iI0YwRjBGMCIgc3RvcC1vcGFjaXR5PSIwIi8+CiAgICAgICAgPC9saW5lYXJHcmFkaWVudD4KICAgICAgICA8bWFzayBpZD0iZmFkZU1hc2siPgogICAgICAgICAgICA8cmVjdCB4PSIwIiB5PSIwIiB3aWR0aD0iNzUwIiBoZWlnaHQ9IjUwIiBmaWxsPSJ3aGl0ZSIvPgogICAgICAgICAgICA8cmVjdCB4PSI3NTAiIHk9IjAiIHdpZHRoPSI1MCIgaGVpZ2h0PSI1MCIgZmlsbD0idXJsKCNmYWRlR3JhZGllbnQpIi8+CiAgICAgICAgPC9tYXNrPgogICAgPC9kZWZzPgogICAgPHBhdGggZD0iTTI1LDUwIFEwLDUwIDAsMjUgTDUwLDMgTDk3LDI1IEw3OTcsMjUgTDc5Nyw1MCBMMjUsNTAgWiIgZmlsbD0iI0YwRjBGMCIgc3Ryb2tlPSIjRTBFMEUwIiBzdHJva2Utd2lkdGg9IjEiIG1hc2s9InVybCgjZmFkZU1hc2spIi8+Cjwvc3ZnPgo=\" alt=\"Time alert close\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e27ad2e-51d5-41e6-878d-da9652c1aad8",
   "metadata": {
    "tags": [],
    "id": "8e27ad2e-51d5-41e6-878d-da9652c1aad8"
   },
   "source": [
    "결과를 데이터 프레임(DataFrame)으로 저장하고 검토합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11001c60-b877-4ba7-b530-9aa7fdeb46f5",
   "metadata": {
    "tags": [],
    "id": "11001c60-b877-4ba7-b530-9aa7fdeb46f5",
    "outputId": "37437b20-86b4-42d3-8814-890aa07fc481"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>query</th>\n",
       "      <th>response_before</th>\n",
       "      <th>response_after</th>\n",
       "      <th>reward_before</th>\n",
       "      <th>reward_after</th>\n",
       "      <th>reward_diff</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Question: I was studying for icing and a tailplane stall. I have looked up some internet pages and instrument flying handbook, and found the procedure below.\\n\\n1. raise flaps to the previous setting. (To reduce down wash from the main wing so that reducing negative angle of attack of the tail and break the stall)\\n2. apply nose up elevator pressure (I don't get it. The nose up pressure will make the elevator to go up and wouldn't this increase the negative angle of attack and worsen the stall?)\\n3. do not increase airspeed unless it is necessary to avoid a wing stall. (Why shouldn't we increase airspeed?)\\n\\nSo now I'm trying to understand the reason why should a pilot do such actions. Can you help me out?\\n\\nAnswer:</td>\n",
       "      <td>1) I don't know what it is, but assume it is wrong. There is no verifiable source given, so the world's experts are looking at you.\\n\\n2)</td>\n",
       "      <td>605.3. Pilot Actions at Stall Entry\\n\\n(a) When the aircraft has a natural or induced stall, the pilot applies up elevator, neutralize ailer</td>\n",
       "      <td>2.287549</td>\n",
       "      <td>3.497573</td>\n",
       "      <td>1.210024</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Question: In a C# enumeration, are there any negative side effects of using a negative number?\\n\\nI am modelling response codes and one of the codes in negative. This compiles but I want to know if there are any negative side effects to this.\\n\\n```\\npublic enum ResponseCodes\\n{\\n    InvalidServerUserPasswordCombo = -1,\\n\\n    // etc.\\n}\\n\\n```\\n\\nAnswer:</td>\n",
       "      <td>1) Positive number is always useful.\\nPositive numbers increase, negative numbers decrease.\\n\\n2) Negative number will normally not directly used in the application, because it will bring a small trouble for the people who use the number.\\n\\n3) If you use it, you must also declare it.\\n\\n\\begin{code</td>\n",
       "      <td>0 would be a more appropriate value, but AFAIK, there's no difference between any negative-numbered enumeration.&lt;/s&gt;&lt;s&gt; Tags: c#, .net, caching, ninject\\n\\nQuestion: Ninject .NET in C# .NET cache dependencies\\n\\nI would like to store some data in cache (the data are fetch</td>\n",
       "      <td>2.192516</td>\n",
       "      <td>3.255872</td>\n",
       "      <td>1.063356</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Question: Isn't that nicely recursive? \\n\\nI've got a portable command prompt on my external drive, and it has a nice .bat file to configure some initial settings, but I'd like more! \\n\\nHere's what I know how to set from .bat:\\n\\n* Colors = (color XY) where x and y are hex digits for the predefined colors\\n* Prompt = (prompt $p$g) sets the prompt to \"C:\\etc\\etc &gt;\" the default prompt\\n* Title = (title \"text\") sets the window title to \"text\"\\n* Screen Size = (mode con: cols=XX lines=YY) sets the columns and lines size of the window\\n* Path = (SET PATH=%~d0\\bin;%PATH%) sets up local path to my tools and appends the computer's path\\n\\nSo that's all great. But there are a few settings I can't seem to set from the bat. Like, how would I set these up wihtout using the Properties dialogue:\\n\\n* Buffer = not screen size, but the buffer\\n* Options like quick edit mode and autocomplete\\n* Popup colors\\n* Font. And can you use a font on the portable drive, or must it be installed to work?\\n* ...</td>\n",
       "      <td>6000A.COM is a notable fixed-document JScript notepad,[2] started by Jamie Zawinski in 1997. It is a rewrite of some bits of the original and continues 6000B Джон Сисьорс портал. 6000A.COM is notable because it replaced Notepad in Windows as the default platform documentation\\n\\nComment: Oh aprendí esto en la Hamilton High School en Phoenix!\\n\\nComment: To be fair, John Siassor's hoax was pretty good.</td>\n",
       "      <td>1. OS environment variables, such as path, color, prompt, title and window size can be set as normally as we did in Windows commandline.\\n2. Options can be set by command prompt by `set` command, but it will are run command prompt loader, so you must gained administrator privileges . For example, `set transient` will set Windows command prompt(default) \\ Transient mode.\\n\\n3. Font can be set by prompt too, but the limitation that we can not simploy change the output like colorful prompt etc. More information of prompt font,</td>\n",
       "      <td>1.826687</td>\n",
       "      <td>2.877825</td>\n",
       "      <td>1.051139</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Question: ![http://farm4.static.flickr.com/3338/4564960137_0d3c09192d_o.png](https://i.stack.imgur.com/Y2gpf.png)\\n\\ncan we change text in this lightbox\\n\\nwant to change \"This site requires that you Connect with Facebook.\" and \"Connect with Facebook to Continue\"\\n\\nAnswer:</td>\n",
       "      <td>1-Get a HTML file or something to have the option\\n\\n2-When the lightbox appear and if onClick is run, just change the text by looads of strings...\\n\\nComment: i want to change the text for whole lightbox\\njust like it is mentioned in the image\\n\\nComment: there must be a \"This</td>\n",
       "      <td>1) File : \\strong{wwwvars.php} (located here 'C:\\xampp\\htdocs\\YOURSITENAME\\img\\wwwvars.php')\\nmay be you will find this string is at line 58 as follow;\\n\\n\\begin{code}\\n$exInstance = ES(\"facebook</td>\n",
       "      <td>1.869028</td>\n",
       "      <td>2.913131</td>\n",
       "      <td>1.044103</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Question: I have an array of objects:\\n\\n```\\nthis.array = [{name: null}, {name: null}, {name: null}]\\n\\n```\\n\\nand array of reservend names:\\n\\n```\\nthis.reserved = [\"name2\", \"name3\"]\\n\\n```\\n\\nI loop through array and try to set uniques name (not included inside `reserved` array)\\n\\n```\\n  for (let i = 0; i &lt; array.length; i++) {\\n    this.setDefaultName(array[i], 1);\\n  }\\n\\n  private setDefaultName(obj, index){\\n    if (!this.reserved.includes(`name${index}`)) {\\n      obj.name = `name${index}`;\\n      this.reserved.push(`name${index}`);\\n    } else {\\n      return this.setDefaultName(obj, index + 1);\\n    }\\n  }\\n\\n```\\n\\nAfter that all objects from array have name \"name3\". The expected result is to have sequence unique name: \"name1\", \"name4\", \"name5\".\\nCould anyone help me?\\n\\nAnswer:</td>\n",
       "      <td>1) if you want some variable to be called \"name\", never do like \"let name = name;\". Set the value as name, not assign name to name. You shouldn't do like this, every one in the world should know, you are not attacking me to spread this mis-thought.</td>\n",
       "      <td>3rd value of `array` should be equal to length of `reserved`.\\n\\nCreate temp array with all names, it should be equal to `reserved` when `index` loop considered, insert `reserved` while will be non existing, remove `reserved` when it will be considered.\\n</td>\n",
       "      <td>1.780465</td>\n",
       "      <td>2.677735</td>\n",
       "      <td>0.897270</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>95</th>\n",
       "      <td>Question: In a C# enumeration, are there any negative side effects of using a negative number?\\n\\nI am modelling response codes and one of the codes in negative. This compiles but I want to know if there are any negative side effects to this.\\n\\n```\\npublic enum ResponseCodes\\n{\\n    InvalidServerUserPasswordCombo = -1,\\n\\n    // etc.\\n}\\n\\n```\\n\\nAnswer:</td>\n",
       "      <td>1) It won't break the compiler, because the compiler is more concerned with syntax than with semantics.\\n\\n2) Parameter passing is very particular:\\n\\n\\begin{blockquote}\\n\\begin{itemize}\\n\\item \\strong{-1} cannot be passed to function with parameter of primitive type.\\n\\item it can be passed</td>\n",
       "      <td>1.\\nNo, Those are just an indication of \"status\" or \"error\". If you like to keep \"actual code\" inside these classes and apart from them.\\n\\n2. If you like to keep \"actual code\" inside these classes and apart from them, you should use special attention and remarque to read its value in</td>\n",
       "      <td>2.860401</td>\n",
       "      <td>1.996818</td>\n",
       "      <td>-0.863583</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>96</th>\n",
       "      <td>Question: I need to take some online tests for school.\\nThis website tells me I need Flash Player 11.3.0 or higher. As far as I can see that is not yet avaible for Linux.\\nI use Ubuntu 12.04 LTS and Chromium. Is there a way I can work around it?\\n\\nGreetz. Rob.\\n\\nAnswer:</td>\n",
       "      <td>32-bit Linux, macOS, or Windows systems using Adobe Flash, you should be able to install the SWC flv playback extension. They say</td>\n",
       "      <td>11.3 is i don't know for which Operating Systems, but I think it is for Windows and Mac only.\\n\\nI will solve your problem</td>\n",
       "      <td>3.973511</td>\n",
       "      <td>3.108645</td>\n",
       "      <td>-0.864866</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>97</th>\n",
       "      <td>Question: **Rules**\\n\\n1. Place some pentominoes into an 8 x 8 grid. They do not touch each other. They can touch only diagonally (with corner).\\n2. Pentominoes cannot repeat in the grid. Rotations and reflections of a pentomino are considered the same shape.\\n3. Grid is 8 x 8.\\n\\nAnswer:</td>\n",
       "      <td>\\n&lt;a href=\"https://i.stack.imgur.com/iVVTd.png\" rel=\"nofollow noreferrer\"&gt;&lt;IMAGE&gt;&lt;/a&gt;\\n\\nComment: [These](http://www.dan.com/pentominos) are what I</td>\n",
       "      <td>158, 208, 282, 318, 350, 377, 395, 411, 444, 512, 556,</td>\n",
       "      <td>2.568738</td>\n",
       "      <td>1.622943</td>\n",
       "      <td>-0.945795</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>98</th>\n",
       "      <td>Question: In his book \"Men of Mathematics\", Eric Temple Bell repeatedly makes the point that a student of mathematics must read the classics.\\n\\nMy question is what are some classic books in mathematics ( Dictionary definition : judged over a period of time to be of the highest quality and outstanding of its kind.) that can be used by a high school/undergraduate student to start the study of higher mathematics? \\n\\nSome subjects I would like reference in particular, otherwise state any book you consider a classic, are:\\n\\n1) Analysis\\n 2) Abstract algebra\\n 3) Linear Algebra\\n 4) Number theory\\n5) Combinatorics and Graph theory, etc.\\n\\nAnswer:</td>\n",
       "      <td>1) Real and complex analysis (Esrealy we sould not use here term \"analysis\" because there are many branches which have nothing to do with, real Dirac analysis if course is very strict. and complex analysis befour Abel about lesbure and fourebrook, but there was fourebrook worked with bridges and pits 100 years after him, and he had no calculus (Thirty Bridges, B</td>\n",
       "      <td>1) Analysis: Richard Courant and David Friedrich [Frederick Patrick Hudenberg, PhD](https://en.wikipedia.org/wiki/David_Hudenberg) \"Analysis of a Text-Book of Mathematical Analysis\", used as a modern-language version of the original \"Introduction to mathematical analysis\" by P.A.M Dirac.\\n\\n[Richard Courant was originally his cousin. Both were refugees from Nazi</td>\n",
       "      <td>3.889789</td>\n",
       "      <td>2.767745</td>\n",
       "      <td>-1.122044</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>99</th>\n",
       "      <td>Question: **Rules**\\n\\n1. Place some pentominoes into an 8 x 8 grid. They do not touch each other. They can touch only diagonally (with corner).\\n2. Pentominoes cannot repeat in the grid. Rotations and reflections of a pentomino are considered the same shape.\\n3. Grid is 8 x 8.\\n\\nAnswer:</td>\n",
       "      <td>596205 have 49 pentominoes. We can use 45 pentominoes. Pentominos are placed diagonally such that no pair of pentominoes touch (horizontally or vertically). This is the solution.\\n\\n&lt;a href=\"https://i.stack.imgur.com/K0OjD.png\" rel=\"nofollow noreferrer\"&gt;&lt;IMAGE&gt;&lt;/a&gt;\\n\\nBy the way, we did not solve the question. The question says that there are 49 structure possible, if we choose the orientation /</td>\n",
       "      <td>90 - rotations\\n\\n2935 - reflections\\n\\n18 - cycle of operations over 2935\\n\\nCombinations: 90 x 2935 + 18 ^ 10 = 4.333333E+35 (way, way too much to brute force)\\nTry brute force with 3000 digits to the right of the decimal!\\n\\n4.8730E+13  - rotations reflecting adjacent square.\\n\\n11.</td>\n",
       "      <td>3.014141</td>\n",
       "      <td>1.658466</td>\n",
       "      <td>-1.355675</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>100 rows × 6 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                      query  \\\n",
       "0                                                                                                                                                                                                                                                                                  Question: I was studying for icing and a tailplane stall. I have looked up some internet pages and instrument flying handbook, and found the procedure below.\\n\\n1. raise flaps to the previous setting. (To reduce down wash from the main wing so that reducing negative angle of attack of the tail and break the stall)\\n2. apply nose up elevator pressure (I don't get it. The nose up pressure will make the elevator to go up and wouldn't this increase the negative angle of attack and worsen the stall?)\\n3. do not increase airspeed unless it is necessary to avoid a wing stall. (Why shouldn't we increase airspeed?)\\n\\nSo now I'm trying to understand the reason why should a pilot do such actions. Can you help me out?\\n\\nAnswer:    \n",
       "1                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                    Question: In a C# enumeration, are there any negative side effects of using a negative number?\\n\\nI am modelling response codes and one of the codes in negative. This compiles but I want to know if there are any negative side effects to this.\\n\\n```\\npublic enum ResponseCodes\\n{\\n    InvalidServerUserPasswordCombo = -1,\\n\\n    // etc.\\n}\\n\\n```\\n\\nAnswer:    \n",
       "2   Question: Isn't that nicely recursive? \\n\\nI've got a portable command prompt on my external drive, and it has a nice .bat file to configure some initial settings, but I'd like more! \\n\\nHere's what I know how to set from .bat:\\n\\n* Colors = (color XY) where x and y are hex digits for the predefined colors\\n* Prompt = (prompt $p$g) sets the prompt to \"C:\\etc\\etc >\" the default prompt\\n* Title = (title \"text\") sets the window title to \"text\"\\n* Screen Size = (mode con: cols=XX lines=YY) sets the columns and lines size of the window\\n* Path = (SET PATH=%~d0\\bin;%PATH%) sets up local path to my tools and appends the computer's path\\n\\nSo that's all great. But there are a few settings I can't seem to set from the bat. Like, how would I set these up wihtout using the Properties dialogue:\\n\\n* Buffer = not screen size, but the buffer\\n* Options like quick edit mode and autocomplete\\n* Popup colors\\n* Font. And can you use a font on the portable drive, or must it be installed to work?\\n* ...   \n",
       "3                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                       Question: ![http://farm4.static.flickr.com/3338/4564960137_0d3c09192d_o.png](https://i.stack.imgur.com/Y2gpf.png)\\n\\ncan we change text in this lightbox\\n\\nwant to change \"This site requires that you Connect with Facebook.\" and \"Connect with Facebook to Continue\"\\n\\nAnswer:    \n",
       "4                                                                                                                                                                                                        Question: I have an array of objects:\\n\\n```\\nthis.array = [{name: null}, {name: null}, {name: null}]\\n\\n```\\n\\nand array of reservend names:\\n\\n```\\nthis.reserved = [\"name2\", \"name3\"]\\n\\n```\\n\\nI loop through array and try to set uniques name (not included inside `reserved` array)\\n\\n```\\n  for (let i = 0; i < array.length; i++) {\\n    this.setDefaultName(array[i], 1);\\n  }\\n\\n  private setDefaultName(obj, index){\\n    if (!this.reserved.includes(`name${index}`)) {\\n      obj.name = `name${index}`;\\n      this.reserved.push(`name${index}`);\\n    } else {\\n      return this.setDefaultName(obj, index + 1);\\n    }\\n  }\\n\\n```\\n\\nAfter that all objects from array have name \"name3\". The expected result is to have sequence unique name: \"name1\", \"name4\", \"name5\".\\nCould anyone help me?\\n\\nAnswer:    \n",
       "..                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                      ...   \n",
       "95                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                   Question: In a C# enumeration, are there any negative side effects of using a negative number?\\n\\nI am modelling response codes and one of the codes in negative. This compiles but I want to know if there are any negative side effects to this.\\n\\n```\\npublic enum ResponseCodes\\n{\\n    InvalidServerUserPasswordCombo = -1,\\n\\n    // etc.\\n}\\n\\n```\\n\\nAnswer:    \n",
       "96                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                        Question: I need to take some online tests for school.\\nThis website tells me I need Flash Player 11.3.0 or higher. As far as I can see that is not yet avaible for Linux.\\nI use Ubuntu 12.04 LTS and Chromium. Is there a way I can work around it?\\n\\nGreetz. Rob.\\n\\nAnswer:    \n",
       "97                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                       Question: **Rules**\\n\\n1. Place some pentominoes into an 8 x 8 grid. They do not touch each other. They can touch only diagonally (with corner).\\n2. Pentominoes cannot repeat in the grid. Rotations and reflections of a pentomino are considered the same shape.\\n3. Grid is 8 x 8.\\n\\nAnswer:    \n",
       "98                                                                                                                                                                                                                                                                                                                                                            Question: In his book \"Men of Mathematics\", Eric Temple Bell repeatedly makes the point that a student of mathematics must read the classics.\\n\\nMy question is what are some classic books in mathematics ( Dictionary definition : judged over a period of time to be of the highest quality and outstanding of its kind.) that can be used by a high school/undergraduate student to start the study of higher mathematics? \\n\\nSome subjects I would like reference in particular, otherwise state any book you consider a classic, are:\\n\\n1) Analysis\\n 2) Abstract algebra\\n 3) Linear Algebra\\n 4) Number theory\\n5) Combinatorics and Graph theory, etc.\\n\\nAnswer:    \n",
       "99                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                       Question: **Rules**\\n\\n1. Place some pentominoes into an 8 x 8 grid. They do not touch each other. They can touch only diagonally (with corner).\\n2. Pentominoes cannot repeat in the grid. Rotations and reflections of a pentomino are considered the same shape.\\n3. Grid is 8 x 8.\\n\\nAnswer:    \n",
       "\n",
       "                                                                                                                                                                                                                                                                                                                                                                                                         response_before  \\\n",
       "0                                                                                                                                                                                                                                                                              1) I don't know what it is, but assume it is wrong. There is no verifiable source given, so the world's experts are looking at you.\\n\\n2)   \n",
       "1                                                                                                           1) Positive number is always useful.\\nPositive numbers increase, negative numbers decrease.\\n\\n2) Negative number will normally not directly used in the application, because it will bring a small trouble for the people who use the number.\\n\\n3) If you use it, you must also declare it.\\n\\n\\begin{code   \n",
       "2   6000A.COM is a notable fixed-document JScript notepad,[2] started by Jamie Zawinski in 1997. It is a rewrite of some bits of the original and continues 6000B Джон Сисьорс портал. 6000A.COM is notable because it replaced Notepad in Windows as the default platform documentation\\n\\nComment: Oh aprendí esto en la Hamilton High School en Phoenix!\\n\\nComment: To be fair, John Siassor's hoax was pretty good.   \n",
       "3                                                                                                                                 1-Get a HTML file or something to have the option\\n\\n2-When the lightbox appear and if onClick is run, just change the text by looads of strings...\\n\\nComment: i want to change the text for whole lightbox\\njust like it is mentioned in the image\\n\\nComment: there must be a \"This   \n",
       "4                                                                                                                                                               1) if you want some variable to be called \"name\", never do like \"let name = name;\". Set the value as name, not assign name to name. You shouldn't do like this, every one in the world should know, you are not attacking me to spread this mis-thought.   \n",
       "..                                                                                                                                                                                                                                                                                                                                                                                                                   ...   \n",
       "95                                                                                                                  1) It won't break the compiler, because the compiler is more concerned with syntax than with semantics.\\n\\n2) Parameter passing is very particular:\\n\\n\\begin{blockquote}\\n\\begin{itemize}\\n\\item \\strong{-1} cannot be passed to function with parameter of primitive type.\\n\\item it can be passed   \n",
       "96                                                                                                                                                                                                                                                                                     32-bit Linux, macOS, or Windows systems using Adobe Flash, you should be able to install the SWC flv playback extension. They say   \n",
       "97                                                                                                                                                                                                                                                                   \\n<a href=\"https://i.stack.imgur.com/iVVTd.png\" rel=\"nofollow noreferrer\"><IMAGE></a>\\n\\nComment: [These](http://www.dan.com/pentominos) are what I   \n",
       "98                                          1) Real and complex analysis (Esrealy we sould not use here term \"analysis\" because there are many branches which have nothing to do with, real Dirac analysis if course is very strict. and complex analysis befour Abel about lesbure and fourebrook, but there was fourebrook worked with bridges and pits 100 years after him, and he had no calculus (Thirty Bridges, B   \n",
       "99        596205 have 49 pentominoes. We can use 45 pentominoes. Pentominos are placed diagonally such that no pair of pentominoes touch (horizontally or vertically). This is the solution.\\n\\n<a href=\"https://i.stack.imgur.com/K0OjD.png\" rel=\"nofollow noreferrer\"><IMAGE></a>\\n\\nBy the way, we did not solve the question. The question says that there are 49 structure possible, if we choose the orientation /   \n",
       "\n",
       "                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                       response_after  \\\n",
       "0                                                                                                                                                                                                                                                                                                                                                                                                        605.3. Pilot Actions at Stall Entry\\n\\n(a) When the aircraft has a natural or induced stall, the pilot applies up elevator, neutralize ailer   \n",
       "1                                                                                                                                                                                                                                                                    0 would be a more appropriate value, but AFAIK, there's no difference between any negative-numbered enumeration.</s><s> Tags: c#, .net, caching, ninject\\n\\nQuestion: Ninject .NET in C# .NET cache dependencies\\n\\nI would like to store some data in cache (the data are fetch   \n",
       "2   1. OS environment variables, such as path, color, prompt, title and window size can be set as normally as we did in Windows commandline.\\n2. Options can be set by command prompt by `set` command, but it will are run command prompt loader, so you must gained administrator privileges . For example, `set transient` will set Windows command prompt(default) \\ Transient mode.\\n\\n3. Font can be set by prompt too, but the limitation that we can not simploy change the output like colorful prompt etc. More information of prompt font,   \n",
       "3                                                                                                                                                                                                                                                                                                                                                 1) File : \\strong{wwwvars.php} (located here 'C:\\xampp\\htdocs\\YOURSITENAME\\img\\wwwvars.php')\\nmay be you will find this string is at line 58 as follow;\\n\\n\\begin{code}\\n$exInstance = ES(\"facebook   \n",
       "4                                                                                                                                                                                                                                                                                     3rd value of `array` should be equal to length of `reserved`.\\n\\nCreate temp array with all names, it should be equal to `reserved` when `index` loop considered, insert `reserved` while will be non existing, remove `reserved` when it will be considered.\\n   \n",
       "..                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                ...   \n",
       "95                                                                                                                                                                                                                                                      1.\\nNo, Those are just an indication of \"status\" or \"error\". If you like to keep \"actual code\" inside these classes and apart from them.\\n\\n2. If you like to keep \"actual code\" inside these classes and apart from them, you should use special attention and remarque to read its value in   \n",
       "96                                                                                                                                                                                                                                                                                                                                                                                                                         11.3 is i don't know for which Operating Systems, but I think it is for Windows and Mac only.\\n\\nI will solve your problem   \n",
       "97                                                                                                                                                                                                                                                                                                                                                                                                                                                                                            158, 208, 282, 318, 350, 377, 395, 411, 444, 512, 556,    \n",
       "98                                                                                                                                                                       1) Analysis: Richard Courant and David Friedrich [Frederick Patrick Hudenberg, PhD](https://en.wikipedia.org/wiki/David_Hudenberg) \"Analysis of a Text-Book of Mathematical Analysis\", used as a modern-language version of the original \"Introduction to mathematical analysis\" by P.A.M Dirac.\\n\\n[Richard Courant was originally his cousin. Both were refugees from Nazi   \n",
       "99                                                                                                                                                                                                                                                     90 - rotations\\n\\n2935 - reflections\\n\\n18 - cycle of operations over 2935\\n\\nCombinations: 90 x 2935 + 18 ^ 10 = 4.333333E+35 (way, way too much to brute force)\\nTry brute force with 3000 digits to the right of the decimal!\\n\\n4.8730E+13  - rotations reflecting adjacent square.\\n\\n11.   \n",
       "\n",
       "    reward_before  reward_after  reward_diff  \n",
       "0        2.287549      3.497573     1.210024  \n",
       "1        2.192516      3.255872     1.063356  \n",
       "2        1.826687      2.877825     1.051139  \n",
       "3        1.869028      2.913131     1.044103  \n",
       "4        1.780465      2.677735     0.897270  \n",
       "..            ...           ...          ...  \n",
       "95       2.860401      1.996818    -0.863583  \n",
       "96       3.973511      3.108645    -0.864866  \n",
       "97       2.568738      1.622943    -0.945795  \n",
       "98       3.889789      2.767745    -1.122044  \n",
       "99       3.014141      1.658466    -1.355675  \n",
       "\n",
       "[100 rows x 6 columns]"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "pd.set_option('display.max_colwidth', 1000)\n",
    "df_compare_results = pd.DataFrame(compare_results)\n",
    "df_compare_results[\"reward_diff\"] = df_compare_results['reward_after'] - df_compare_results['reward_before']\n",
    "df_compare_results_sorted = df_compare_results.sort_values(by=['reward_diff'], ascending=False).reset_index(drop=True)\n",
    "df_compare_results_sorted"
   ]
  }
 ],
 "metadata": {
  "availableInstances": [
   {
    "_defaultOrder": 0,
    "_isFastLaunch": true,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 4,
    "name": "ml.t3.medium",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 1,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 8,
    "name": "ml.t3.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 2,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.t3.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 3,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.t3.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 4,
    "_isFastLaunch": true,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 8,
    "name": "ml.m5.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 5,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.m5.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 6,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.m5.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 7,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 64,
    "name": "ml.m5.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 8,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 128,
    "name": "ml.m5.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 9,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 192,
    "name": "ml.m5.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 10,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 256,
    "name": "ml.m5.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 11,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 384,
    "name": "ml.m5.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 12,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 8,
    "name": "ml.m5d.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 13,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.m5d.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 14,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.m5d.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 15,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 64,
    "name": "ml.m5d.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 16,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 128,
    "name": "ml.m5d.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 17,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 192,
    "name": "ml.m5d.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 18,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 256,
    "name": "ml.m5d.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 19,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 384,
    "name": "ml.m5d.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 20,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": true,
    "memoryGiB": 0,
    "name": "ml.geospatial.interactive",
    "supportedImageNames": [
     "sagemaker-geospatial-v1-0"
    ],
    "vcpuNum": 0
   },
   {
    "_defaultOrder": 21,
    "_isFastLaunch": true,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 4,
    "name": "ml.c5.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 22,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 8,
    "name": "ml.c5.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 23,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.c5.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 24,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.c5.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 25,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 72,
    "name": "ml.c5.9xlarge",
    "vcpuNum": 36
   },
   {
    "_defaultOrder": 26,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 96,
    "name": "ml.c5.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 27,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 144,
    "name": "ml.c5.18xlarge",
    "vcpuNum": 72
   },
   {
    "_defaultOrder": 28,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 192,
    "name": "ml.c5.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 29,
    "_isFastLaunch": true,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.g4dn.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 30,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.g4dn.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 31,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 64,
    "name": "ml.g4dn.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 32,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 128,
    "name": "ml.g4dn.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 33,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 4,
    "hideHardwareSpecs": false,
    "memoryGiB": 192,
    "name": "ml.g4dn.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 34,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 256,
    "name": "ml.g4dn.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 35,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 61,
    "name": "ml.p3.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 36,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 4,
    "hideHardwareSpecs": false,
    "memoryGiB": 244,
    "name": "ml.p3.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 37,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 8,
    "hideHardwareSpecs": false,
    "memoryGiB": 488,
    "name": "ml.p3.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 38,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 8,
    "hideHardwareSpecs": false,
    "memoryGiB": 768,
    "name": "ml.p3dn.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 39,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.r5.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 40,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.r5.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 41,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 64,
    "name": "ml.r5.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 42,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 128,
    "name": "ml.r5.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 43,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 256,
    "name": "ml.r5.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 44,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 384,
    "name": "ml.r5.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 45,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 512,
    "name": "ml.r5.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 46,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 768,
    "name": "ml.r5.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 47,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.g5.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 48,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.g5.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 49,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 64,
    "name": "ml.g5.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 50,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 128,
    "name": "ml.g5.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 51,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 256,
    "name": "ml.g5.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 52,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 4,
    "hideHardwareSpecs": false,
    "memoryGiB": 192,
    "name": "ml.g5.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 53,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 4,
    "hideHardwareSpecs": false,
    "memoryGiB": 384,
    "name": "ml.g5.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 54,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 8,
    "hideHardwareSpecs": false,
    "memoryGiB": 768,
    "name": "ml.g5.48xlarge",
    "vcpuNum": 192
   },
   {
    "_defaultOrder": 55,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 8,
    "hideHardwareSpecs": false,
    "memoryGiB": 1152,
    "name": "ml.p4d.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 56,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 8,
    "hideHardwareSpecs": false,
    "memoryGiB": 1152,
    "name": "ml.p4de.24xlarge",
    "vcpuNum": 96
   }
  ],
  "kernelspec": {
   "name": "python3",
   "language": "python",
   "display_name": "Python 3 (ipykernel)"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  },
  "colab": {
   "provenance": []
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
