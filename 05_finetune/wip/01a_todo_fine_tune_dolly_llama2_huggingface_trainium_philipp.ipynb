{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "# AWS Ìä∏Î†àÏù¥ÎãàÏõÄ(Trainium)ÏóêÏÑú Llama Î™®Îç∏ ÎØ∏ÏÑ∏ Ï°∞Ï†ïÌïòÍ∏∞\n",
    "\n",
    "Ïù¥Î≤à Ïã§ÏäµÏóêÏÑúÎäî AWS Ìä∏Î†àÏù¥ÎãàÏõÄ(Trainium)ÏóêÏÑú [Llama 2](https://huggingface.co/meta-llama/Llama-2-70b-hf)ÏôÄ Í∞ôÏùÄ Ïò§Ìîà LLMÏùÑ ÎØ∏ÏÑ∏ Ï°∞Ï†ïÌïòÎäî Î∞©Î≤ïÏùÑ Î∞∞ÏõÅÎãàÎã§. ÏòàÏ†úÏóêÏÑúÎäî ÌóàÍπÖÌéòÏù¥Ïä§ ÏµúÏ†ÅÏùò Îâ¥Îü∞(Optimum Neuron), [Transformers](https://huggingface.co/docs/transformers/index), Í∑∏Î¶¨Í≥† Îç∞Ïù¥ÌÑ∞ ÏÑ∏Ìä∏Î•º ÌôúÏö©Ìï©ÎãàÎã§.\n",
    "\n",
    "Ïù¥Î≤à Ïã§ÏäµÏùÑ ÌÜµÌï¥ Îã§ÏùåÏùÑ Î∞∞Ïö∞Í≤å Îê©ÎãàÎã§:\n",
    "\n",
    "1. [AWS ÌôòÍ≤Ω ÏÑ§Ï†ï](#1-aws-ÌôòÍ≤Ω-ÏÑ§Ï†ï)\n",
    "2. [Îç∞Ïù¥ÌÑ∞ ÏÑ∏Ìä∏ Î∂àÎü¨Ïò§Í∏∞ Î∞è Ï§ÄÎπÑ](#2-Îç∞Ïù¥ÌÑ∞-ÏÑ∏Ìä∏-Î∂àÎü¨Ïò§Í∏∞-Î∞è-Ï§ÄÎπÑ)\n",
    "3. [AWS TrainiumÏùÑ ÏÇ¨Ïö©ÌïòÏó¨ `NeuronTrainer`Î°ú Llama Î™®Îç∏ ÎØ∏ÏÑ∏ Ï°∞Ï†ïÌïòÍ∏∞](#3-aws-trainiumÏùÑ-ÏÇ¨Ïö©ÌïòÏó¨-neurontrainerÎ°ú-llama-Î™®Îç∏-ÎØ∏ÏÑ∏-Ï°∞Ï†ïÌïòÍ∏∞)\n",
    "4. [ÎØ∏ÏÑ∏ Ï°∞Ï†ïÎêú Llama Î™®Îç∏ ÌèâÍ∞Ä Î∞è ÌÖåÏä§Ìä∏](#4-ÎØ∏ÏÑ∏-Ï°∞Ï†ïÎêú-llama-Î™®Îç∏-ÌèâÍ∞Ä-Î∞è-ÌÖåÏä§Ìä∏)\n",
    "\n",
    "---\n",
    "\n",
    "## Í∞ÑÎã®Ìïú ÏÜåÍ∞ú: AWS Trainium\n",
    "\n",
    "[AWS Trainium (Trn1)](https://aws.amazon.com/ko/ec2/instance-types/trn1/?nc1=h_ls)ÏùÄ Îî• Îü¨Îãù(DL) ÌõàÎ†® ÏûëÏóÖÏùÑ ÏúÑÌï¥ ÌäπÎ≥ÑÌûà Ï†úÏûëÎêú EC2 Ïù∏Ïä§ÌÑ¥Ïä§ÏûÖÎãàÎã§. TrainiumÏùÄ Í≥†ÏÑ±Îä• ÌõàÎ†® ÏûëÏóÖÏóê Ï§ëÏ†êÏùÑ Îëî [AWS Inferentia](https://aws.amazon.com/ko/ec2/instance-types/inf1/?nc1=h_ls)Ïùò ÌõÑÏÜç Ï†úÌíàÏûÖÎãàÎã§. TrainiumÏùÄ ÏûêÏó∞Ïñ¥ Ï≤òÎ¶¨, Ïª¥Ìì®ÌÑ∞ ÎπÑÏ†Ñ, Ï∂îÏ≤ú Î™®Îç∏ÏùÑ ÌõàÎ†®ÌïòÎäîÎç∞ ÏµúÏ†ÅÌôîÎêòÏñ¥ ÏûàÏäµÎãàÎã§. Ïù¥ Í∞ÄÏÜçÍ∏∞Îäî FP32, TF32, BF16, FP16, UINT8 Î∞è Íµ¨ÏÑ± Í∞ÄÎä•Ìïú FP8ÏùÑ Ìè¨Ìï®Ìïú Îã§ÏñëÌïú Îç∞Ïù¥ÌÑ∞ Ïú†ÌòïÏùÑ ÏßÄÏõêÌï©ÎãàÎã§.\n",
    "\n",
    "Í∞ÄÏû• ÌÅ∞ Trainium Ïù∏Ïä§ÌÑ¥Ïä§Ïù∏ `trn1.32xlarge`Îäî 500GB Ïù¥ÏÉÅÏùò Î©îÎ™®Î¶¨Î•º Ï†úÍ≥µÌïòÏó¨ ÏïΩ 100Ïñµ Í∞úÏùò ÌååÎùºÎØ∏ÌÑ∞ Î™®Îç∏ÏùÑ Îã®Ïùº Ïù∏Ïä§ÌÑ¥Ïä§ÏóêÏÑú ÏâΩÍ≤å ÎØ∏ÏÑ∏ Ï°∞Ï†ïÌï† Ïàò ÏûàÏäµÎãàÎã§. ÏïÑÎûòÏóêÏÑú ÏÇ¨Ïö© Í∞ÄÎä•Ìïú Ïù∏Ïä§ÌÑ¥Ïä§ Ïú†ÌòïÏóê ÎåÄÌïú Í∞úÏöîÎ•º ÌôïÏù∏Ìï† Ïàò ÏûàÏäµÎãàÎã§. Îçî ÏûêÏÑ∏Ìïú ÎÇ¥Ïö©ÏùÄ [Ïó¨Í∏∞](https://aws.amazon.com/ko/ec2/instance-types/trn1/?nc1=h_ls)ÏóêÏÑú ÌôïÏù∏Ìï† Ïàò ÏûàÏäµÎãàÎã§:\n",
    "\n",
    "| Ïù∏Ïä§ÌÑ¥Ïä§ ÌÅ¨Í∏∞     | ÏóëÏÖÄÎü¨Î†àÏù¥ÌÑ∞ | ÏóëÏÖÄÎü¨Î†àÏù¥ÌÑ∞ Î©îÎ™®Î¶¨ (GB) | vCPU | Ïù∏Ïä§ÌÑ¥Ïä§ Î©îÎ™®Î¶¨ (GiB) | ÏãúÍ∞ÑÎãπ ÏöîÍ∏à (ÎØ∏Íµ≠ Îã¨Îü¨, USD) |\n",
    "|-------------|--------|-----------------|------|----------------|---------------------|\n",
    "| trn1.2xlarge | 1      | 32              | 8    | 32             | 1.34                |\n",
    "| trn1.32xlarge| 16     | 512             | 128  | 512            | 21.50               |\n",
    "| trn1n.32xlarge| 16     | 512             | 128  | 512            | 24.78               |\n",
    "\n",
    "\n",
    "---\n",
    " \n",
    "*Ï∞∏Í≥†: Ïù¥Î≤à Ïã§ÏäµÏùÄ trn1.32xlarge AWS EC2 Ïù∏Ïä§ÌÑ¥Ïä§ÏóêÏÑú ÏàòÌñâÌñàÏäµÎãàÎã§.*\n",
    "\n",
    "\n",
    "## 1. AWS ÌôòÍ≤Ω ÏÑ§Ï†ï\n",
    "\n",
    "Ïù¥Î≤à ÏòàÏ†úÏóêÏÑúÎäî 32Í∞úÏùò Îâ¥Îü∞ ÏΩîÏñ¥ÏôÄ 16Í∞úÏùò Í∞ÄÏÜçÍ∏∞Î•º Ìè¨Ìï®Ìïú `trn1.32xlarge` Ïù∏Ïä§ÌÑ¥Ïä§ÏôÄ [ÌóàÍπÖÌéòÏù¥Ïä§ Îâ¥Îü∞ Îî•Îü¨Îãù(Hugging Face Neuron Deep Learning)](https://aws.amazon.com/marketplace/pp/prodview-gr3e6yiscria2) AMIÎ•º ÏÇ¨Ïö©Ìï©ÎãàÎã§. Ìï¥Îãπ ÌóàÍπÖÌéòÏù¥Ïä§ AMIÏóêÎäî Transformers, Datasets, Optimum, Neuron Ìå®ÌÇ§ÏßÄ Í∞ôÏù¥ Ï§ëÏöîÌïú ÎùºÏù¥Î∏åÎü¨Î¶¨Í∞Ä ÎØ∏Î¶¨ ÏÑ§ÏπòÎêòÏñ¥ ÏûàÏñ¥ ÌôòÍ≤Ω Í¥ÄÎ¶¨ Ìï† ÌïÑÏöî ÏóÜÏù¥ Îß§Ïö∞ ÏâΩÍ≤å ÏãúÏûëÌï† Ïàò ÏûàÏäµÎãàÎã§.\n",
    "\n",
    "Ïù¥Î≤à Ïã§ÏäµÏóêÏÑúÎäî Ïù∏Ïä§ÌÑ¥Ïä§Î•º ÏÉùÏÑ±ÌïòÎäî Î∞©Î≤ïÏùÑ Îã§Î£®ÏßÄ ÏïäÏäµÎãàÎã§. ÌôòÍ≤Ω ÏÑ§Ï†ïÏóê ÎåÄÌïú Îã®Í≥ÑÎ≥Ñ Í∞ÄÏù¥ÎìúÍ∞Ä Ìè¨Ìï®Îêú [‚ÄúÌóàÍπÖÌéòÏù¥Ïä§ TransformersÎ•º ÏúÑÌïú AWS Trainium ÏÑ§Ï†ïÌïòÍ∏∞‚Äù](https://www.philschmid.de/setup-aws-trainium)Ïóê ÎåÄÌïú Î∏îÎ°úÍ∑∏ Ìè¨Ïä§Ìä∏ÏóêÏÑú Ìï¥Îãπ ÎÇ¥Ïö©ÏùÑ ÌôïÏù∏Ìï† Ïàò ÏûàÏäµÎãàÎã§.\n",
    "\n",
    "Ïù∏Ïä§ÌÑ¥Ïä§Î•º ÏÑ§Ï†ïÌïòÍ≥† Ïã§ÌñâÌïòÎ©¥ SSHÎ°ú Ï†ëÏÜçÌï† Ïàò ÏûàÏäµÎãàÎã§. ÌïòÏßÄÎßå ÌÑ∞ÎØ∏ÎÑê ÎÇ¥ÏóêÏÑú Í∞úÎ∞úÌïòÎäî ÎåÄÏã† Îç∞Ïù¥ÌÑ∞ ÏÑ∏Ìä∏ Ï§ÄÎπÑÏôÄ ÌõàÎ†® ÏãúÏûëÏùÑ ÏúÑÌï¥ `Ï•¨ÌîºÌÑ∞(Jupyter)` ÌôòÍ≤ΩÏùÑ ÏÇ¨Ïö©ÌïòÍ≥†Ïûê Ìï©ÎãàÎã§. Ïù¥Î•º ÏúÑÌï¥ `ssh` Î™ÖÎ†πÏñ¥Ïóê Ìè¨Ìä∏Î•º Ï∂îÍ∞ÄÌïòÏó¨ Ìè¨ÏõåÎî©ÌïòÍ≥† Î°úÏª¨ Ìò∏Ïä§Ìä∏ Ìä∏ÎûòÌîΩÏùÑ Trainium Ïù∏Ïä§ÌÑ¥Ïä§Î°ú ÌÑ∞ÎÑêÎßÅÌï¥Ïïº Ìï©ÎãàÎã§.\n",
    "\n",
    "```bash\n",
    "PUBLIC_DNS=\"\" # IP Ï£ºÏÜå, ÏòàÎ•º Îì§Ïñ¥. ec2-3-80-....\n",
    "KEY_PATH=\"\" # ÌÇ§ Î≥¥Í¥Ä Í≤ΩÎ°ú, ÏòàÎ•º Îì§Ïñ¥. ssh/trn.pem\n",
    "\n",
    "ssh -L 8080:localhost:8080 -i ${KEY_NAME}.pem ubuntu@$PUBLIC_DNS\n",
    "```\n",
    "\n",
    "Ïù¥Ï†ú [ÏòàÏ†ú ÎÖ∏Ìä∏Î∂ÅÍ≥º Ïä§ÌÅ¨Î¶ΩÌä∏](https://github.com/huggingface/optimum-neuron/tree/main/notebooks/text-generation)Í∞Ä Ìè¨Ìï®Îêú Optimum Ï†ÄÏû•ÏÜåÎ•º Í∞ÄÏ†∏ÏòµÎãàÎã§.\n",
    "\n",
    "```bash\n",
    "git clone https://github.com/huggingface/optimum-neuron.git\n",
    "```\n",
    "\n",
    "Îã§ÏùåÏúºÎ°ú ÎîîÎ†âÌÜ†Î¶¨Î•º `notebooks/text-generation`ÏúºÎ°ú Î≥ÄÍ≤ΩÌïòÍ≥† `Ï•¨ÌîºÌÑ∞` ÌôòÍ≤ΩÏùÑ Ïã§ÌñâÌï©ÎãàÎã§.\n",
    "\n",
    "\n",
    "```bash\n",
    "# Í≤ΩÎ°ú Î≥ÄÍ≤Ω\n",
    "cd optimum-neuron/notebooks/text-generation\n",
    "# jupyter Ïã§Ìñâ\n",
    "python -m notebook --allow-root --port=8080\n",
    "```\n",
    "\n",
    "**`Ï•¨ÌîºÌÑ∞`** Ïùò Ï∂úÎ†•Í≥º Ìï®Íªò ÎÖ∏Ìä∏Î∂ÅÏúºÎ°ú Ïó∞Í≤∞ÎêòÎäî URLÏù¥ ÏùµÏàôÌïòÍ≤å Î≥¥Ïùº Í≤ÉÏûÖÎãàÎã§.\n",
    "\n",
    "**`http://localhost:8080/?token=8c1739aff1755bd7958c4cfccc8d08cb5da5234f61f129a9`**\n",
    "\n",
    "Ïù¥ ÎßÅÌÅ¨Î•º ÌÅ¥Î¶≠ÌïòÎ©¥ **`Ï•¨ÌîºÌÑ∞`** ÌôòÍ≤ΩÏù¥ Î°úÏª¨ Î∏åÎùºÏö∞Ï†ÄÏóêÏÑú Ïó¥Î¶ΩÎãàÎã§. `llama2-7b-fine-tuning.ipynb` ÎÖ∏Ìä∏Î∂ÅÏùÑ Ïó¥Í≥† ÏãúÏûëÌï¥ Î¥ÖÏãúÎã§.\n",
    "\n",
    "_Ï∞∏Í≥†: Ï•¨ÌîºÌÑ∞ ÌôòÍ≤ΩÏùÑ Îç∞Ïù¥ÌÑ∞ ÏÑ∏Ìä∏ Ï§ÄÎπÑÏóêÎßå ÏÇ¨Ïö©Ìï† Í≤ÉÏù¥Î©∞ Í∑∏ ÌõÑ `torchrun`ÏùÑ ÏÇ¨Ïö©ÌïòÏó¨ Î∂ÑÏÇ∞ ÌõàÎ†®ÏùÑ ÏúÑÌïú ÌõàÎ†® Ïä§ÌÅ¨Î¶ΩÌä∏Î•º Ïã§ÌñâÌï©ÎãàÎã§._"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "tags": []
   },
   "source": [
    "%pip install optimum-neuron torch-neuronx"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Í≥µÏãù Llama 2 Ï≤¥ÌÅ¨Ìè¨Ïù∏Ìä∏Î•º ÏÇ¨Ïö©ÌïòÎ†§Î©¥ Î™®Îç∏Ïóê Ï†ëÍ∑ºÌï† Ïàò ÏûàÎäî ÌóàÍπÖÌéòÏù¥Ïä§ Í≥ÑÏ†ïÏóê Î°úÍ∑∏Ïù∏ÌïòÍ≥† Í≤åÏù¥Ìä∏Îêú Ï†ÄÏû•ÏÜåÏóê Ï†ëÍ∑ºÌï† Ïàò ÏûàÎäî ÌÜ†ÌÅ∞ÏùÑ ÏÇ¨Ïö©Ìï¥Ïïº Ìï©ÎãàÎã§. Îã§Ïùå Î™ÖÎ†πÏñ¥Î•º Ïã§ÌñâÌïòÏó¨ Ïù¥ ÏûëÏóÖÏùÑ ÏàòÌñâÌï† Ïàò ÏûàÏäµÎãàÎã§:\n",
    "\n",
    "_Ï∞∏Í≥†: Í≤åÏù¥Ìä∏ ÏóÜÎäî Ï≤¥ÌÅ¨Ìè¨Ïù∏Ìä∏ÎèÑ Ï†úÍ≥µÌï©ÎãàÎã§._"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": "# !huggingface-cli login --token ÌÜ†ÌÅ∞_ÏûÖÎ†•",
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Îç∞Ïù¥ÌÑ∞ ÏÑ∏Ìä∏ Î∂àÎü¨Ïò§Í∏∞ Î∞è Ï§ÄÎπÑ\n",
    "\n",
    "[InstructGPT ÎÖºÎ¨∏](https://arxiv.org/abs/2203.02155)Ïóê ÏÑ§Î™ÖÎêú Ïó¨Îü¨ ÌñâÎèô Ïπ¥ÌÖåÍ≥†Î¶¨(Î∏åÎ†àÏù∏Ïä§ÌÜ†Î∞ç, Î∂ÑÎ•ò, ÌèêÏáÑÌòï QA, ÏÉùÏÑ±, Ï†ïÎ≥¥ Ï∂îÏ∂ú, Í∞úÎ∞©Ìòï QA, ÏöîÏïΩ) Î∂ÑÏïºÏóê ÏàòÏ≤ú Î™ÖÏùò Databricks ÏßÅÏõêÎì§Ïù¥ ÏÉùÏÑ±Ìïú Î™ÖÎ†πÏñ¥-ÏùëÎãµ Ìï≠Î™©Îì§ÏùÑ Ìè¨Ìï®Ìïú Ïò§Ìîà ÏÜåÏä§ Îç∞Ïù¥ÌÑ∞ ÏÑ∏Ìä∏ [dolly](https://huggingface.co/datasets/databricks/databricks-dolly-15k)Î•º ÏÇ¨Ïö©Ìï©ÎãàÎã§.\n",
    "\n",
    "```python\n",
    "{\n",
    "  \"instruction\": \"World of WarcraftÎûÄ Î¨¥ÏóáÏù∏Í∞Ä?\",\n",
    "  \"context\": \"\",\n",
    "  \"response\": \"World of WarcraftÎäî ÎåÄÍ∑úÎ™® Ïò®ÎùºÏù∏ Î©ÄÌã∞ ÌîåÎ†àÏù¥Ïñ¥ Î°§ÌîåÎ†àÏûâ Í≤åÏûÑÏûÖÎãàÎã§. 2004ÎÖÑÏóê Blizzard EntertainmentÏóêÏÑú Ï∂úÏãúÌñàÏäµÎãàÎã§.\"\n",
    "}\n",
    "```\n",
    "\n",
    "`samsum` Îç∞Ïù¥ÌÑ∞ ÏÑ∏Ìä∏Î•º Î∂àÎü¨Ïò§Í∏∞ ÏúÑÌï¥ ÌóàÍπÖÌéòÏù¥Ïä§ Datasets ÎùºÏù¥Î∏åÎü¨Î¶¨Ïùò `load_dataset()` Î©îÏÜåÎìúÎ•º ÏÇ¨Ïö©Ìï©ÎãàÎã§."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "tags": []
   },
   "source": [
    "from datasets import load_dataset\n",
    "from random import randrange\n",
    "\n",
    "# Load dataset from the hub\n",
    "dataset = load_dataset(\"databricks/databricks-dolly-15k\", split=\"train\")\n",
    "\n",
    "print(f\"dataset size: {len(dataset)}\")\n",
    "print(dataset[randrange(len(dataset))])\n",
    "# dataset size: 15011\n"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "Î™®Îç∏ÏùÑ Î™ÖÎ†πÏñ¥Î°ú Ï°∞Ï†ïÌïòÎ†§Î©¥ Íµ¨Ï°∞ÌôîÎêú ÏòàÏ†úÎì§ÏùÑ Î™ÖÎ†πÏñ¥Î°ú ÌëúÌòÑÎêú ÏûëÏóÖÎì§Ïùò Î™®ÏùåÏúºÎ°ú Î≥ÄÌôòÌï¥Ïïº Ìï©ÎãàÎã§. ÏÉòÌîåÏùÑ ÏûÖÎ†• Î∞õÏïÑÏÑú ÏßÄÏ†ïÎêú ÌòïÏãùÏùò Î™ÖÎ†πÏñ¥ Î¨∏ÏûêÏó¥ÏùÑ Î∞òÌôòÌïòÎäî Î©îÏÜåÎìú `formatting_function`ÏùÑ Ï†ïÏùòÌï©ÎãàÎã§."
  },
  {
   "cell_type": "code",
   "metadata": {
    "tags": []
   },
   "source": [
    "def format_dolly(sample):\n",
    "    instruction = f\"### Instruction\\n{sample['instruction']}\"\n",
    "    context = f\"### Context\\n{sample['context']}\" if len(sample[\"context\"]) > 0 else None\n",
    "    response = f\"### Answer\\n{sample['response']}\"\n",
    "    # join all the parts together\n",
    "    prompt = \"\\n\\n\".join([i for i in [instruction, context, response] if i is not None])\n",
    "    return prompt\n"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "Ìè¨Îß∑ÌåÖ Ìï®ÏàòÎ•º ÏûÑÏùòÏùò ÏòàÏ†úÎ°ú ÌÖåÏä§Ìä∏ Ìï©ÎãàÎã§."
  },
  {
   "cell_type": "code",
   "metadata": {
    "tags": []
   },
   "source": [
    "from random import randrange\n",
    "\n",
    "print(format_dolly(dataset[randrange(len(dataset))]))"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "Ï∂îÍ∞ÄÎ°ú, ÏÉòÌîåÏùÑ Ìè¨Îß∑ÌïòÎäî Í≤É Ïô∏Ïóê Îçî Ìö®Ïú®Ï†ÅÏù∏ ÌõàÎ†®ÏùÑ ÏúÑÌï¥ Ïó¨Îü¨ ÏÉòÌîåÏùÑ ÌïòÎÇòÏùò ÏãúÌÄÄÏä§Î°ú Î¨∂Îäî ÏûëÏóÖÏùÑ ÏàòÌñâÌï©ÎãàÎã§. Ïù¥Îäî Ïó¨Îü¨ ÏÉòÌîåÏùÑ ÌïòÎÇòÏùò ÏãúÌÄÄÏä§Î°ú Ïä§ÌÉùÌïòÍ≥† EOS ÌÜ†ÌÅ∞ÏúºÎ°ú Íµ¨Î∂ÑÌïòÎäî Í≤ÉÏùÑ ÏùòÎØ∏Ìï©ÎãàÎã§. Ïù¥Î†áÍ≤å ÌïòÎ©¥ ÌõàÎ†®Ïù¥ Îçî Ìö®Ïú®Ï†ÅÏûÖÎãàÎã§. ÏÉòÌîåÏùÑ Ìå®ÌÇπ/Ïä§ÌÉúÌÇπÌïòÎäî ÏûëÏóÖÏùÄ ÌõàÎ†® Ï†Ñ ÎòêÎäî ÌõàÎ†® Ï§ëÏóê ÏàòÌñâÌï† Ïàò ÏûàÏäµÎãàÎã§. Ïù¥Î≤à Ïã§ÏäµÏóêÏÑúÎäî ÏãúÍ∞Ñ Ï†àÏïΩÏùÑ ÏúÑÌï¥ ÌõàÎ†® Ï†ÑÏóê Ïù¥ ÏûëÏóÖÏùÑ ÏàòÌñâÌï©ÎãàÎã§. Îç∞Ïù¥ÌÑ∞ ÏÑ∏Ìä∏ÏôÄ Ìå®ÌÇπ Ìï®ÏàòÎ•º Î∞õÏïÑ Ìå®ÌÇπÎêú Îç∞Ïù¥ÌÑ∞ ÏÑ∏Ìä∏Î•º Î∞òÌôòÌïòÎäî Ïú†Ìã∏Î¶¨Ìã∞ Î©îÏÑúÎìú [pack_dataset](../scripts/utils/pack_dataset.py)ÏùÑ ÎßåÎì§ÏóàÏäµÎãàÎã§."
  },
  {
   "cell_type": "code",
   "metadata": {
    "tags": []
   },
   "source": [
    "from transformers import AutoTokenizer\n",
    "\n",
    "# Hugging Face model id\n",
    "#model_id = \"NousResearch/Llama-2-7b-hf\"\n",
    "model_id = \"philschmid/Llama-2-7b-hf\" # ungated\n",
    "# model_id = \"meta-llama/Llama-2-7b-hf\" # gated\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_id)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Îç∞Ïù¥ÌÑ∞ ÏÑ∏Ìä∏Î•º Ìå®ÌÇπ/Ïä§ÌÉúÌÇπÌïòÎ†§Î©¥ Î®ºÏ†Ä ÌÜ†ÌÅ∞ÌôîÌïú Îã§Ïùå `pack_dataset` Î©îÏÑúÎìúÎ•º ÏÇ¨Ïö©ÌïòÏó¨ Ìå®ÌÇπÌï† Ïàò ÏûàÏäµÎãàÎã§. Îç∞Ïù¥ÌÑ∞Î•º Ï§ÄÎπÑÌïòÍ∏∞ ÏúÑÌï¥ Îã§ÏùåÏùÑ ÏàòÌñâÌï©ÎãàÎã§:\n",
    "\n",
    "1. ÌÖúÌîåÎ¶ø Î©îÏÑúÎìúÎ•º ÏÇ¨Ïö©ÌïòÏó¨ ÏÉòÌîåÏùÑ Ìè¨Îß∑ÌïòÍ≥† Í∞Å ÏÉòÌîåÏùò ÎÅùÏóê EOS ÌÜ†ÌÅ∞ÏùÑ Ï∂îÍ∞ÄÌï©ÎãàÎã§.\n",
    "2. Îç∞Ïù¥ÌÑ∞ ÏÑ∏Ìä∏Î•º ÌÖçÏä§Ìä∏ÏóêÏÑú ÌÜ†ÌÅ∞ÏúºÎ°ú Î≥ÄÌôòÌïòÍ∏∞ ÏúÑÌï¥ ÌÜ†ÌÅ∞ÌôîÌï©ÎãàÎã§.\n",
    "3. Îç∞Ïù¥ÌÑ∞ ÏÑ∏Ìä∏Î•º 2048Í∞úÏùò ÌÜ†ÌÅ∞ÏúºÎ°ú Ìå®ÌÇπÌï©ÎãàÎã§."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "tags": []
   },
   "source": [
    "from random import randint\n",
    "# add utils method to path for loading dataset\n",
    "import sys\n",
    "sys.path.append(\"./scripts/utils\") # make sure you change this to the correct path \n",
    "from pack_dataset import pack_dataset\n",
    "\n",
    "\n",
    "# template dataset to add prompt to each sample\n",
    "def template_dataset(sample):\n",
    "    sample[\"text\"] = f\"{format_dolly(sample)}{tokenizer.eos_token}\"\n",
    "    return sample\n",
    "\n",
    "# apply prompt template per sample\n",
    "dataset = dataset.map(template_dataset, remove_columns=list(dataset.features))\n",
    "# print random sample\n",
    "print(dataset[randint(0, len(dataset))][\"text\"])\n",
    "\n",
    "# tokenize dataset\n",
    "dataset = dataset.map(\n",
    "    lambda sample: tokenizer(sample[\"text\"]), batched=True, remove_columns=list(dataset.features)\n",
    ")\n",
    "\n",
    "# chunk dataset\n",
    "lm_dataset = pack_dataset(dataset, chunk_length=2048) # We use 2048 as the maximum length for packing"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Îç∞Ïù¥ÌÑ∞ ÏÑ∏Ìä∏Î•º Ï≤òÎ¶¨Ìïú ÌõÑ ÎîîÏä§ÌÅ¨Ïóê Ï†ÄÏû•Ìï©ÎãàÎã§. ÎÇòÏ§ëÏóê ÏÇ¨Ïö©ÌïòÍ∏∞ ÏúÑÌï¥ S3 ÎòêÎäî ÌóàÍπÖÌéòÏù¥Ïä§ ÌóàÎ∏åÏóê Ï†ÄÏû•Ìï† ÏàòÎèÑ ÏûàÏäµÎãàÎã§.\n",
    "\n",
    "_Ï∞∏Í≥†: Îç∞Ïù¥ÌÑ∞ÏÑ∏Ìä∏Î•º Ìå®ÌÇπÌïòÍ≥† Ï†ÑÏ≤òÎ¶¨ÌïòÎäî ÏûëÏóÖÏùÄ Trainium Ïù∏Ïä§ÌÑ¥Ïä§ Ïô∏Î∂ÄÏóêÏÑú Ïã§ÌñâÌï† Ïàò ÏûàÏäµÎãàÎã§._"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "tags": []
   },
   "source": [
    "# ÌõàÎ†® Îç∞Ïù¥ÌÑ∞ ÏÑ∏Ìä∏ ÎîîÏä§ÌÅ¨Ïóê Ï†ÄÏû•\n",
    "dataset_path = \"tokenized_dolly\"\n",
    "lm_dataset.save_to_disk(dataset_path)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. AWS TrainiumÏùÑ ÏÇ¨Ïö©ÌïòÏó¨ `NeuronTrainer`Î°ú Llama Î™®Îç∏ ÎØ∏ÏÑ∏ Ï°∞Ï†ïÌïòÍ∏∞\n",
    "\n",
    "Î≥¥ÌÜµÏùÄ **[Trainer](https://huggingface.co/docs/transformers/v4.19.4/en/main_classes/trainer#transformers.Trainer)**ÏôÄ **[TrainingArguments](https://huggingface.co/docs/transformers/v4.19.4/en/main_classes/trainer#transformers.TrainingArguments)**Î•º ÏÇ¨Ïö©ÌïòÏó¨ PyTorch Í∏∞Î∞òÏùò Ìä∏ÎûúÏä§Ìè¨Î®∏ Î™®Îç∏ÏùÑ ÎØ∏ÏÑ∏ Ï°∞Ï†ïÌï©ÎãàÎã§.\n",
    "\n",
    "ÌïòÏßÄÎßå AWS Trainium Ïù∏Ïä§ÌÑ¥Ïä§ÏóêÏÑú ÌõàÎ†®Ìï† ÎïåÎäî ÏÑ±Îä•, Í≤¨Í≥†ÏÑ± Î∞è ÏïàÏ†ÑÏÑ±ÏùÑ Ìñ•ÏÉÅ ÏãúÌÇ¨ Ïàò ÏûàÎäî `NeuronTrainer`Î•º ÏÇ¨Ïö©Ìï©ÎãàÎã§. `NeuronTrainer`Îäî `optimum-neuron` ÎùºÏù¥Î∏åÎü¨Î¶¨Ïùò ÏùºÎ∂ÄÎ°ú `Trainer`Î•º 1:1Î°ú ÎåÄÏ≤¥Ìï† Ïàò ÏûàÏäµÎãàÎã§.\n",
    "\n",
    "AWS TrainiumÏóêÏÑú Î∂ÑÏÇ∞ ÌõàÎ†® Ìï† ÎïåÎäî Î™á Í∞ÄÏßÄ Ï£ºÏùòÏÇ¨Ìï≠Ïù¥ ÏûàÏäµÎãàÎã§. LlamaÎäî ÌÅ∞ Î™®Îç∏Ïù¥Í∏∞ ÎïåÎ¨∏Ïóê Îã®Ïùº Í∞ÄÏÜçÍ∏∞Ïóê ÎßûÏßÄ ÏïäÏùÑ Ïàò ÏûàÏäµÎãàÎã§. Í∑∏ÎûòÏÑú `NeuronTrainer`Ïóê Îã§ÏñëÌïú Î∂ÑÏÇ∞ ÌõàÎ†® Ï†ÑÎûµÏùÑ ÏßÄÏõêÌïòÍ≥† ÏûàÏäµÎãàÎã§. Îã§Ïùå ÌõàÎ†® Ï†ÑÎûµÏù¥ Ìè¨Ìï®Îê©ÎãàÎã§:\n",
    "\n",
    "* [ZeRO-1](https://awsdocs-neuron.readthedocs-hosted.com/en/latest/frameworks/torch/torch-neuronx/tutorials/training/zero1_gpt2.html): ÏòµÌã∞ÎßàÏù¥Ï†Ä ÏÉÅÌÉúÎ•º Ïó¨Îü¨ Ïû•ÏπòÏóê Î∂ÑÌï†Ìï©ÎãàÎã§.\n",
    "* [Tensor Parallelism](https://awsdocs-neuron.readthedocs-hosted.com/en/latest/libraries/neuronx-distributed/tensor_parallelism_overview.html):Î™®Îç∏ ÌååÎùºÎØ∏ÌÑ∞Î•º Ïó¨Îü¨ Ïû•ÏπòÏùò ÏßÄÏ†ïÎêú Ï∞®ÏõêÏóê Îî∞Îùº Î∂ÑÌï†ÌïòÎ©∞ `tensor_parallel_size`Î°ú Ï†ïÏùòÌï©ÎãàÎã§. \n",
    "* [Sequence parallelism](https://arxiv.org/pdf/2205.05198.pdf): ÌÖêÏÑú Î≥ëÎ†¨ ÏòÅÏó≠ Ïô∏Î∂ÄÏùò ÏãúÌÄÄÏä§ Ï∂ïÏóêÏÑú ÌôúÏÑ±ÌôîÎ•º Î∂ÑÌï†Ìï©ÎãàÎã§. Î©îÎ™®Î¶¨Î•º Ï†àÏïΩÌïòÎ©¥ÏÑú ÌôúÏÑ±ÌôîÎ•º Î∂ÑÌï†ÌïòÎäîÎç∞ Ïú†Ïö©Ìï©ÎãàÎã§.\n",
    "* [Pipeline Parallelism](https://awsdocs-neuron.readthedocs-hosted.com/en/latest/libraries/neuronx-distributed/pipeline_parallelism_overview.html): _ÏßÄÏõê ÏòàÏ†ï_\n",
    "\n",
    "`optimum-neuron` ÎùºÏù¥Î∏åÎü¨Î¶¨Îäî Ïù¥ÎØ∏ Ïù¥Îü¨Ìïú Î∂ÑÏÇ∞ ÌõàÎ†® Ï†ÑÎûµÏùÑ Íµ¨ÌòÑÌïú `run_clm.py`Î•º Ï§ÄÎπÑÌï¥ÎÜ®ÏäµÎãàÎã§. ÎùºÏù¥Î∏åÎü¨Î¶¨ÏóêÏÑú ÏßÄÏõêÌïòÎäî Î∂ÑÏÇ∞ ÌõàÎ†® Ï†ÑÎûµÏùÄ [Í≥µÏãù Î¨∏ÏÑú](https://huggingface.co/docs/optimum-neuron/guides/distributed_training)ÏóêÏÑú ÌôïÏù∏Ìï† Ïàò ÏûàÏäµÎãàÎã§. AWS Í∞ÄÏÜçÍ∏∞ÏóêÏÑúÎäî Î™®Îç∏ÏùÑ ÌõàÎ†®Ìï† Îïå Î®ºÏ†Ä ÌõàÎ†® Ïù∏ÏàòÏôÄ Ìï®Íªò Î™®Îç∏ÏùÑ Ïª¥ÌååÏùºÌï¥Ïïº Ìï©ÎãàÎã§.\n",
    "\n",
    "Ïù¥Î•º Í∑πÎ≥µÌïòÍ∏∞ ÏúÑÌï¥ Ìï¥Îãπ ÎùºÏù¥Î∏åÎü¨Î¶¨Ïóê [Î™®Îç∏ Ï∫êÏãú](https://huggingface.co/docs/optimum-neuron/guides/cache_system)Î•º Ìè¨Ìï®ÎêòÏñ¥ ÏûàÏäµÎãàÎã§. Ïù¥Î•º ÌÜµÌï¥ ÌóàÍπÖÌéòÏù¥Ïä§ ÌóàÎ∏åÏóêÏÑú ÎØ∏Î¶¨ Ïª¥ÌååÏùºÎêú Î™®Îç∏Í≥º Íµ¨ÏÑ±ÏùÑ ÏÇ¨Ïö©ÌïòÏó¨ Ïª¥ÌååÏùº Îã®Í≥ÑÎ•º Í±¥ÎÑàÎõ∏ Ïàò ÏûàÏäµÎãàÎã§. ÌïòÏßÄÎßå Íµ¨ÏÑ±ÏóêÏÑú Î≥ÄÍ≤ΩÏù¥ ÏûàÏùÑ Í≤ΩÏö∞ ÏÉàÎ°úÏö¥ Ïª¥ÌååÏùºÏù¥ Î∞úÏÉùÌï† Ïàò ÏûàÏúºÎ©∞, Ïù¥Îäî ÏùºÎ∂Ä Ï∫êÏãú ÎàÑÎùΩÏúºÎ°ú Ïù¥Ïñ¥Ïßà Ïàò ÏûàÏäµÎãàÎã§.\n",
    "\n",
    "_Ï∞∏Í≥†: Íµ¨ÏÑ± ÌååÏùºÏù¥ Ï∫êÏãúÏóê ÏóÜÏúºÎ©¥ ÎùºÏù¥Î∏åÎü¨Î¶¨ [Github](https://github.com/huggingface/optimum-neuron/issues)Ïóê Ïù¥ÏäàÎ•º Ïó¥Ïñ¥ Ï£ºÏÑ∏Ïöî. Íµ¨ÏÑ± ÌååÏùº Ï∂îÍ∞ÄÏóê ÎèÑÏõÄ Ï§Ñ Ïàò ÏûàÏäµÎãàÎã§._\n",
    "\n",
    "Ïù¥ÎØ∏ ÌõàÎ†®ÏùÑ ÏúÑÌïú Íµ¨ÏÑ±ÏùÑ ÎØ∏Î¶¨ Ïª¥ÌååÏùºÌñàÏäµÎãàÎã§. Îî∞ÎùºÏÑú ÏïÑÎûò ÏÖÄÏùÑ Í±¥ÎÑàÎõ∞Í±∞ÎÇò Îã§Ïãú Ïã§ÌñâÌïòÎçîÎùºÎèÑ Ï∫êÏãúÎêú Íµ¨ÏÑ±ÏùÑ Ïû¨ÏÇ¨Ïö©ÌïòÍ∏∞ ÎïåÎ¨∏Ïóê Î™á Î∂Ñ Ï†ïÎèÑÎßå ÏÜåÏöîÎê† Í≤ÉÏûÖÎãàÎã§."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "tags": []
   },
   "source": [
    "%%bash\n",
    "apt-get install apt-utils gnupg  -y\n",
    "\n",
    "# Configure Linux for Neuron repository updates\n",
    ". /etc/os-release\n",
    "tee /etc/apt/sources.list.d/neuron.list > /dev/null <<EOF\n",
    "deb https://apt.repos.neuron.amazonaws.com ${VERSION_CODENAME} main\n",
    "EOF\n",
    "wget -qO - https://apt.repos.neuron.amazonaws.com/GPG-PUB-KEY-AMAZON-AWS-NEURON.PUB | apt-key add -"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "source": [
    "### ÏÑ§Ï†ï\n",
    "\n",
    "---\n",
    "\n",
    "ÌïÑÏöîÌïú Ìå®ÌÇ§ÏßÄÎ•º ÏÑ§ÏπòÌïòÍ≥† ÏóÖÍ∑∏Î†àÏù¥ÎìúÌï©ÎãàÎã§. ÏïÑÎûò ÏÖÄÏùÑ Ï≤òÏùå Ïã§ÌñâÌïú ÌõÑ Ï•¨ÌîºÌÑ∞ ÎÖ∏Ìä∏Î∂Å Ïª§ÎÑêÏùÑ Ïû¨ÏãúÏûëÌïòÏÑ∏Ïöî.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "tags": []
   },
   "source": [
    "%pip config set global.extra-index-url https://pip.repos.neuron.amazonaws.com"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {
    "tags": []
   },
   "source": [
    "!apt-get update -y"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {
    "tags": []
   },
   "source": [
    "!apt install -y aws-neuronx-dkms=2.*"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {
    "tags": []
   },
   "source": [
    "%%bash\n",
    "\n",
    "# Update OS packages \n",
    "#apt-get update -y\n",
    "\n",
    "# Install OS headers \n",
    "#apt-get install linux-headers-$(uname -r) -y\n",
    "\n",
    "# Install git \n",
    "#apt-get install git -y\n",
    "\n",
    "# install Neuron Driver\n",
    "apt-get install aws-neuronx-dkms=2.* -y\n",
    "\n",
    "# Install Neuron Runtime \n",
    "apt-get install aws-neuronx-collectives=2.* -y\n",
    "apt-get install aws-neuronx-runtime-lib=2.* -y\n",
    "\n",
    "# Install Neuron Tools \n",
    "apt-get install aws-neuronx-tools=2.* -y\n",
    "\n",
    "# Add PATH\n",
    "export PATH=/opt/aws/neuron/bin:$PATH"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {
    "tags": []
   },
   "source": [
    "%pip install -U neuronx-cc==2.* torch-neuronx torch"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {
    "tags": []
   },
   "source": [
    "# precompilation command \n",
    "!MALLOC_ARENA_MAX=64 neuron_parallel_compile torchrun --nproc_per_node=32 scripts/run_clm.py \\\n",
    " --model_id {model_id} \\\n",
    " --dataset_path {dataset_path} \\\n",
    " --bf16 True \\\n",
    " --learning_rate 5e-5 \\\n",
    " --output_dir dolly_llama \\\n",
    " --overwrite_output_dir True \\\n",
    " --skip_cache_push True \\\n",
    " --per_device_train_batch_size 1 \\\n",
    " --gradient_checkpointing True \\\n",
    " --tensor_parallel_size 8 \\\n",
    " --max_steps 10 \\\n",
    " --logging_steps 10 \\\n",
    " --gradient_accumulation_steps 16"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "_Ï∞∏Í≥†: Ï∫êÏãú ÏóÜÏù¥ Ïª¥ÌååÏùºÌïòÎäî Îç∞ ÏïΩ 40Î∂ÑÏù¥ ÏÜåÏöîÎê† Ïàò ÏûàÏäµÎãàÎã§. Ïª¥ÌååÏùº Ï§ëÏóê `dolly_llama_sharded`Ïóê ÏûÑÏãú ÌååÏùºÏù¥ ÏÉùÏÑ±ÎêòÎØÄÎ°ú ÎÇòÏ§ëÏóê Ï†úÍ±∞Ìï¥Ïïº Ìï©ÎãàÎã§. ÎòêÌïú Ïû†Ïû¨Ï†ÅÏù∏ Ï∂©ÎèåÏùÑ ÌîºÌïòÍ∏∞ ÏúÑÌï¥ CPU Ìï†ÎãπÏùÑ Ï†úÌïúÌïòÎäî `MALLOC_ARENA_MAX=64`Î•º Ï∂îÍ∞ÄÌï¥Ïïº ÌïòÎØÄÎ°ú Ïù¥ Í∞íÏùÑ ÏßÄÍ∏àÏùÄ Ï†úÍ±∞ÌïòÏßÄ ÎßàÏÑ∏Ïöî._"
  },
  {
   "cell_type": "code",
   "metadata": {
    "tags": []
   },
   "source": [
    "# remove dummy artifacts which are created by the precompilation command\n",
    "!rm -rf dolly_llama"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ïª¥ÌååÏùºÏù¥ ÏôÑÎ£åÎêòÎ©¥ ÎπÑÏä∑Ìïú Î™ÖÎ†πÏñ¥Î°ú ÌõàÎ†®ÏùÑ ÏãúÏûëÌï† Ïàò ÏûàÏäµÎãàÎã§. Îã§Îßå `neuron_parallel_compile`ÏùÑ Ï†úÍ±∞Ìï¥Ïïº Ìï©ÎãàÎã§. `torchrun`ÏùÑ ÏÇ¨Ïö©ÌïòÏó¨ ÌõàÎ†® Ïä§ÌÅ¨Î¶ΩÌä∏Î•º Ïã§ÌñâÌï©ÎãàÎã§. `torchrun`ÏùÄ PyTorch Î™®Îç∏ÏùÑ Ïó¨Îü¨ Í∞ÄÏÜçÍ∏∞ÏóêÏÑú ÏûêÎèôÏúºÎ°ú Î∂ÑÏÇ∞ÏãúÌÇ§Îäî ÎèÑÍµ¨ÏûÖÎãàÎã§. `nproc_per_node` Ïù∏ÏàòÎ°ú Í∞ÄÏÜçÍ∏∞Ïùò ÏàòÎ•º ÌïòÏù¥ÌçºÌååÎùºÎØ∏ÌÑ∞ÏôÄ Ìï®Íªò Ï†ÑÎã¨Ìï† Ïàò ÏûàÏäµÎãàÎã§. Ïª¥ÌååÏùº Î™ÖÎ†πÏñ¥ÏôÄÏùò Ï∞®Ïù¥Ï†êÏùÄ `max_steps=10`ÏóêÏÑú `num_train_epochs=3`ÏúºÎ°ú Î≥ÄÍ≤ΩÌïú Í≤ÉÏûÖÎãàÎã§.\n",
    "\n",
    "Îã§Ïùå Î™ÖÎ†πÏñ¥Î•º ÏÇ¨Ïö©ÌïòÏó¨ ÌõàÎ†®ÏùÑ ÏãúÏûëÌï©ÎãàÎã§."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "tags": []
   },
   "source": [
    "!MALLOC_ARENA_MAX=64 torchrun --nproc_per_node=32 scripts/run_clm.py \\\n",
    " --model_id {model_id} \\\n",
    " --dataset_path {dataset_path} \\\n",
    " --bf16 True \\\n",
    " --learning_rate 5e-5 \\\n",
    " --output_dir dolly_llama \\\n",
    " --overwrite_output_dir True \\\n",
    " --per_device_train_batch_size 1 \\\n",
    " --gradient_checkpointing True \\\n",
    " --tensor_parallel_size 8 \\\n",
    " --num_train_epochs 3 \\\n",
    " --logging_steps 10 \\\n",
    " --gradient_accumulation_steps 16"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ïù¥Î†áÍ≤å Ìï¥ÏÑú AWS TrainiumÏóêÏÑú Llama 7B Î™®Îç∏ÏùÑ ÏÑ±Í≥µÏ†ÅÏúºÎ°ú ÌõàÎ†®ÌñàÏäµÎãàÎã§. dolly Îç∞Ïù¥ÌÑ∞ ÏÑ∏Ìä∏(15,000Í∞úÏùò ÏÉòÌîå)Î°ú 3 ÏóêÌè¨ÌÅ¨ ÎèôÏïà ÌõàÎ†®ÌïòÎäîÎç∞ Ï¥ù 43Î∂Ñ 24Ï¥àÍ∞Ä ÏÜåÏöîÎêòÏóàÏúºÎ©∞ Ïã§Ï†ú ÌõàÎ†® ÏãúÍ∞ÑÏùÄ 31Î∂Ñ 46Ï¥àÏóê Î∂àÍ≥ºÌñàÏäµÎãàÎã§. Ïù¥Îäî trn1.32xlarge Ïù∏Ïä§ÌÑ¥Ïä§ÏóêÏÑú Ï†ÑÏ≤¥ ÌõàÎ†®ÏùÑ ÏàòÌñâÌïòÎäî Îç∞ ÏïΩ $15.5Ïùò ÎπÑÏö©Ïù¥ Îì†Îã§Îäî Í≤ÉÏùÑ ÏùòÎØ∏Ìï©ÎãàÎã§. ÎÇòÏÅòÏßÄ ÏïäÎÑ§Ïöî!\n",
    "\n",
    "ÌïòÏßÄÎßå Î™®Îç∏ÏùÑ Í≥µÏú†ÌïòÍ≥† ÌÖåÏä§Ìä∏ÌïòÍ∏∞ Ï†ÑÏóê Î™®Îç∏ÏùÑ ÌÜµÌï©Ìï¥Ïïº Ìï©ÎãàÎã§. ÌõàÎ†® Ï§ëÏóê ÌÖêÏÑú Î≥ëÎ†¨ Ï≤òÎ¶¨Î•º ÏÇ¨Ïö©ÌñàÍ∏∞ ÎïåÎ¨∏Ïóê Î™®Îç∏ÏùÑ ÏÇ¨Ïö©ÌïòÍ∏∞ Ï†ÑÏóê Î™®Îç∏ Í∞ÄÏ§ëÏπòÎ•º ÌÜµÌï©Ìï¥Ïïº Ìï©ÎãàÎã§. ÌÖêÏÑú Î≥ëÎ†¨ Ï≤òÎ¶¨Îäî Î™®Îç∏ Í∞ÄÏ§ëÏπòÎ•º Ïó¨Îü¨ ÏûëÏóÖÏûêÏóê Í±∏Ï≥ê Î∂ÑÌï†ÌïòÎ©∞ ÌõàÎ†® Ï§ëÏóêÎäî Î∂ÑÌï†Îêú Ï≤¥ÌÅ¨Ìè¨Ïù∏Ìä∏Îßå Ï†ÄÏû•Îê©ÎãàÎã§.\n",
    "\n",
    "Optimum CLIÎäî `optimum neuron consolidate` Î™ÖÎ†πÏñ¥Î•º ÌÜµÌï¥ Ïù¥Î•º Îß§Ïö∞ ÏâΩÍ≤å ÏàòÌñâÌï† Ïàò ÏûàÎäî Î∞©Î≤ïÏùÑ Ï†úÍ≥µÌï©ÎãàÎã§:"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "tags": []
   },
   "source": [
    "!optimum-cli neuron consolidate dolly_llama/tensor_parallel_shards dolly_llama"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "Ïù¥ÎØ∏ safetensorsÎ°ú ÌÜµÌï©ÌñàÏúºÎØÄÎ°ú \"Î∂ÑÌï†Îêú\" Ï≤¥ÌÅ¨Ìè¨Ïù∏Ìä∏Î•º Ï†úÍ±∞Ìï©ÏãúÎã§."
  },
  {
   "cell_type": "code",
   "metadata": {
    "tags": []
   },
   "source": [
    "!rm -rf dolly_llama/tensor_parallel_shards"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. ÎØ∏ÏÑ∏ Ï°∞Ï†ïÎêú Llama Î™®Îç∏ ÌèâÍ∞Ä Î∞è ÌÖåÏä§Ìä∏\n",
    "\n",
    "ÌõàÎ†®Í≥º ÎßàÏ∞¨Í∞ÄÏßÄÎ°ú AWS Trainium ÎòêÎäî AWS Inferentia2ÏóêÏÑú Ï∂îÎ°†ÏùÑ Ïã§ÌñâÌïòÎ†§Î©¥ Ïò¨Î∞îÎ•¥Í≤å ÏÇ¨Ïö©Ìï† Ïàò ÏûàÎèÑÎ°ù Î™®Îç∏ÏùÑ Ïª¥ÌååÏùºÌï¥Ïïº Ìï©ÎãàÎã§. Ïù¥Î≤à Ïã§ÏäµÏóêÏÑúÎäî Ï∂îÎ°† ÌÖåÏä§Ìä∏Î•º ÏúÑÌï¥ Trainium Ïù∏Ïä§ÌÑ¥Ïä§Î•º ÏÇ¨Ïö©ÌïòÏßÄÎßå Î≥¥ÌÜµ Ï∂îÎ°†ÏóêÎäî Inferentia2Î•º ÏÇ¨Ïö©ÌïòÎäîÍ≤å Ï¢ãÏäµÎãàÎã§.\n",
    "\n",
    "Optimum NeuronÏùÄ Í∞ÑÎã®Ìïú Ï∂îÎ°†ÏùÑ ÏúÑÌï¥ TransformersÏùò AutoModel ÌÅ¥ÎûòÏä§ÏôÄ Ïú†ÏÇ¨ÌïòÍ≤å Íµ¨ÌòÑÎêòÏóàÏäµÎãàÎã§. `NeuronModelForCausalLM` ÌÅ¥ÎûòÏä§Î•º ÏÇ¨Ïö©ÌïòÏó¨ Í∏∞Î≥∏ Ìä∏ÎûúÏä§Ìè¨Î®∏ Ï≤¥ÌÅ¨Ìè¨Ïù∏Ìä∏Î•º Î°úÎìúÌïòÍ≥† Ïù¥Î•º Îâ¥Îü∞ Î™®Îç∏Î°ú Î≥ÄÌôòÌï©ÎãàÎã§. "
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "tags": []
   },
   "source": [
    "from optimum.neuron import NeuronModelForCausalLM\n",
    "from transformers import AutoTokenizer\n",
    "\n",
    "compiler_args = {\"num_cores\": 2, \"auto_cast_type\": 'fp16'}\n",
    "input_shapes = {\"batch_size\": 1, \"sequence_length\": 2048}\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"dolly_llama\")\n",
    "model = NeuronModelForCausalLM.from_pretrained(\n",
    "        \"dolly_llama\",\n",
    "        export=True,\n",
    "        **compiler_args,\n",
    "        **input_shapes)\n"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "_Ï∞∏Í≥†: Ï∂îÎ°† Ïª¥ÌååÏùºÏóêÎäî ÏïΩ 25Î∂ÑÏù¥ ÏÜåÏöîÎê† Ïàò ÏûàÏäµÎãàÎã§. Îã§ÌñâÌûàÎèÑ Ïù¥ ÏûëÏóÖÏùÄ Ìïú Î≤àÎßå ÏàòÌñâÌïòÎ©¥ Îê©ÎãàÎã§. ÎÇòÏ§ëÏóê Î™®Îç∏ÏùÑ Ï†ÄÏû•Ìï† Ïàò ÏûàÍ∏∞ ÎïåÎ¨∏ÏûÖÎãàÎã§. Inferentia2ÏóêÏÑú Ïã§ÌñâÌïòÎ†§Î©¥ Îã§Ïãú Ïª¥ÌååÏùºÌï¥Ïïº Ìï©ÎãàÎã§. Ïª¥ÌååÏùºÏùÄ Îß§Í∞úÎ≥ÄÏàòÏôÄ ÌïòÎìúÏõ®Ïñ¥Ïóê Îî∞Îùº Îã¨ÎùºÏßëÎãàÎã§._"
  },
  {
   "cell_type": "code",
   "metadata": {
    "tags": []
   },
   "source": [
    "# COMMENT IN if you want to save the compiled model\n",
    "# model.save_pretrained(\"compiled_dolly_llama\")"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "Ïù¥Ï†ú Ï∂îÎ°†ÏùÑ ÌÖåÏä§Ìä∏Ìï† Ïàò ÏûàÏßÄÎßå ÎØ∏ÏÑ∏ Ï°∞Ï†ïÏùÑ ÏúÑÌï¥ ÏÇ¨Ïö©Ìïú ÌîÑÎ°¨ÌîÑÌä∏ ÌòïÏãùÏóê ÎßûÍ≤å ÏûÖÎ†•Ïùò ÌòïÏãùÏùÑ Î≥ÄÍ≤ΩÌï¥Ïïº Ìï©ÎãàÎã§. Ïù¥Î•º ÏúÑÌï¥ `instruction`Í≥º ÏÑ†ÌÉùÏ†ÅÏúºÎ°ú `context`Í∞Ä Ìè¨Ìï®Îêú `dict`Î•º Ïù∏ÏûêÎ°ú Î∞õÎäî Î©îÏÜåÎìúÎ•º ÎßåÎì≠ÎãàÎã§."
  },
  {
   "cell_type": "code",
   "metadata": {
    "tags": []
   },
   "source": [
    "def format_dolly_infernece(sample):\n",
    "    instruction = f\"### Instruction\\n{sample['instruction']}\"\n",
    "    context = f\"### Context\\n{sample['context']}\" if \"context\" in sample else None\n",
    "    response = f\"### Answer\\n\"\n",
    "    # join all the parts together\n",
    "    prompt = \"\\n\\n\".join([i for i in [instruction, context, response] if i is not None])\n",
    "    return prompt\n",
    "\n",
    "\n",
    "def generate(sample): \n",
    "    prompt = format_dolly_infernece(sample)\n",
    "    inputs = tokenizer(prompt, return_tensors=\"pt\")\n",
    "    outputs = model.generate(**inputs,\n",
    "                         max_new_tokens=512,\n",
    "                         do_sample=True,\n",
    "                         temperature=0.9,\n",
    "                         top_k=50,\n",
    "                         top_p=0.9)\n",
    "    return tokenizer.decode(outputs[0], skip_special_tokens=False)[len(prompt):]"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ïù¥Ï†ú Ï∂îÎ°†ÏùÑ ÌÖåÏä§Ìä∏Ìï¥ Î≥¥Í≤†ÏäµÎãàÎã§. Î®ºÏ†Ä Ïª®ÌÖçÏä§Ìä∏ ÏóÜÏù¥ ÌÖåÏä§Ìä∏Ìï¥ Î≥¥Í≤†ÏäµÎãàÎã§.\n",
    "\n",
    "_Ï∞∏Í≥†: AWS TrainiumÏóêÏÑú 2Í∞úÏùò ÏΩîÏñ¥Î•º ÏÇ¨Ïö©ÌïòÏó¨ Ï∂îÎ°†Ìï† Îïå Îß§Ïö∞ Îπ†Î•¥Í≤å ÏàòÌñâÎê† Í≤ÉÏùÑ Í∏∞ÎåÄÌïòÏßÄ ÎßàÏÑ∏Ïöî. Ï∂îÎ°†ÏóêÎäî Inferentia2Î•º ÏÇ¨Ïö©ÌïòÎäî Í≤ÉÏù¥ Ï¢ãÏäµÎãàÎã§._"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "tags": []
   },
   "source": [
    "prompt = {\n",
    "  \"instruction\": \"Can you tell me something about AWS?\"\n",
    "}\n",
    "res = generate(prompt)\n",
    "\n",
    "print(res)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> AWS stands for Amazon Web Services. AWS is a suite of remote computing services offered by Amazon. The most widely used of these include Amazon Elastic Compute Cloud (Amazon EC2), which provides resizable compute capacity in the cloud; Amazon Simple Storage Service (Amazon S3), which is an object storage service; and Amazon Elastic Block Store (Amazon EBS), which is designed to provide high performance, durable block storage volumes for use with AWS instances. AWS also provides other services, such as AWS Identity and Access Management (IAM), a service that enables organizations to control access to their AWS resources, and AWS Key Management Service (AWS KMS), which helps customers create and control the use of encryption keys.</s>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "Ïò¨Î∞îÎ•¥Í≤å ÏûëÎèôÌïòÎäî Í≤É Í∞ôÏäµÎãàÎã§. Ïù¥Ï†ú RAG Ïï†ÌîåÎ¶¨ÏºÄÏù¥ÏÖòÏ≤òÎüº ÏùºÎ∂Ä Ïª®ÌÖçÏä§Ìä∏Î•º Ï∂îÍ∞ÄÌï¥ Î≥¥Í≤†ÏäµÎãàÎã§."
  },
  {
   "cell_type": "code",
   "metadata": {
    "tags": []
   },
   "source": [
    "prompt = {\n",
    "  \"instruction\": \"How can train models on AWS Trainium?\",\n",
    "  \"context\": \"ü§ó Optimum Neuron is the interface between the ü§ó Transformers library and AWS Accelerators¬†including [AWS Trainium](https://aws.amazon.com/machine-learning/trainium/?nc1=h_ls) and [AWS Inferentia](https://aws.amazon.com/machine-learning/inferentia/?nc1=h_ls). It provides a set of tools enabling easy model loading, training and inference on single- and multi-Accelerator settings for different downstream tasks.\"\n",
    "}\n",
    "res = generate(prompt)\n",
    "\n",
    "print(res)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> You can use the Optimum Neuron interface to train models on AWS Trainium.</s> "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "Î©ãÏßÄÎÑ§Ïöî. Î™®Îç∏Ïù¥ Ï†úÍ≥µÎêú Ïª®ÌÖçÏä§Ìä∏Î•º Ï†ïÌôïÌûà ÏÇ¨Ïö©ÌïòÍ≥† ÏûàÏäµÎãàÎã§. Ïó¨Í∏∞ÍπåÏßÄÏûÖÎãàÎã§. AWS TrainiumÏóêÏÑú LlamaÎ•º ÎØ∏ÏÑ∏ Ï°∞Ï†ïÌïú Í≤ÉÏùÑ Ï∂ïÌïòÎìúÎ¶ΩÎãàÎã§."
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "availableInstances": [
   {
    "_defaultOrder": 0,
    "_isFastLaunch": true,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 4,
    "name": "ml.t3.medium",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 1,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 8,
    "name": "ml.t3.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 2,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.t3.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 3,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.t3.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 4,
    "_isFastLaunch": true,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 8,
    "name": "ml.m5.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 5,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.m5.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 6,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.m5.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 7,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 64,
    "name": "ml.m5.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 8,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 128,
    "name": "ml.m5.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 9,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 192,
    "name": "ml.m5.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 10,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 256,
    "name": "ml.m5.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 11,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 384,
    "name": "ml.m5.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 12,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 8,
    "name": "ml.m5d.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 13,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.m5d.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 14,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.m5d.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 15,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 64,
    "name": "ml.m5d.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 16,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 128,
    "name": "ml.m5d.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 17,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 192,
    "name": "ml.m5d.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 18,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 256,
    "name": "ml.m5d.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 19,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 384,
    "name": "ml.m5d.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 20,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": true,
    "memoryGiB": 0,
    "name": "ml.geospatial.interactive",
    "supportedImageNames": [
     "sagemaker-geospatial-v1-0"
    ],
    "vcpuNum": 0
   },
   {
    "_defaultOrder": 21,
    "_isFastLaunch": true,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 4,
    "name": "ml.c5.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 22,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 8,
    "name": "ml.c5.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 23,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.c5.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 24,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.c5.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 25,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 72,
    "name": "ml.c5.9xlarge",
    "vcpuNum": 36
   },
   {
    "_defaultOrder": 26,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 96,
    "name": "ml.c5.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 27,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 144,
    "name": "ml.c5.18xlarge",
    "vcpuNum": 72
   },
   {
    "_defaultOrder": 28,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 192,
    "name": "ml.c5.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 29,
    "_isFastLaunch": true,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.g4dn.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 30,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.g4dn.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 31,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 64,
    "name": "ml.g4dn.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 32,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 128,
    "name": "ml.g4dn.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 33,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 4,
    "hideHardwareSpecs": false,
    "memoryGiB": 192,
    "name": "ml.g4dn.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 34,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 256,
    "name": "ml.g4dn.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 35,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 61,
    "name": "ml.p3.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 36,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 4,
    "hideHardwareSpecs": false,
    "memoryGiB": 244,
    "name": "ml.p3.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 37,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 8,
    "hideHardwareSpecs": false,
    "memoryGiB": 488,
    "name": "ml.p3.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 38,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 8,
    "hideHardwareSpecs": false,
    "memoryGiB": 768,
    "name": "ml.p3dn.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 39,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.r5.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 40,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.r5.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 41,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 64,
    "name": "ml.r5.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 42,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 128,
    "name": "ml.r5.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 43,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 256,
    "name": "ml.r5.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 44,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 384,
    "name": "ml.r5.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 45,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 512,
    "name": "ml.r5.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 46,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 768,
    "name": "ml.r5.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 47,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.g5.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 48,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.g5.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 49,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 64,
    "name": "ml.g5.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 50,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 128,
    "name": "ml.g5.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 51,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 256,
    "name": "ml.g5.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 52,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 4,
    "hideHardwareSpecs": false,
    "memoryGiB": 192,
    "name": "ml.g5.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 53,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 4,
    "hideHardwareSpecs": false,
    "memoryGiB": 384,
    "name": "ml.g5.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 54,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 8,
    "hideHardwareSpecs": false,
    "memoryGiB": 768,
    "name": "ml.g5.48xlarge",
    "vcpuNum": 192
   },
   {
    "_defaultOrder": 55,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 8,
    "hideHardwareSpecs": false,
    "memoryGiB": 1152,
    "name": "ml.p4d.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 56,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 8,
    "hideHardwareSpecs": false,
    "memoryGiB": 1152,
    "name": "ml.p4de.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 57,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.trn1.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 58,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 512,
    "name": "ml.trn1.32xlarge",
    "vcpuNum": 128
   },
   {
    "_defaultOrder": 59,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 512,
    "name": "ml.trn1n.32xlarge",
    "vcpuNum": 128
   }
  ],
  "instance_type": "ml.trn1.32xlarge",
  "kernelspec": {
   "display_name": "Python 3 (Data Science 3.0)",
   "language": "python",
   "name": "python3__SAGEMAKER_INTERNAL__arn:aws:sagemaker:us-east-1:081325390199:image/sagemaker-data-science-310-v1"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  },
  "vscode": {
   "interpreter": {
    "hash": "2d58e898dde0263bc564c6968b04150abacfd33eed9b19aaa8e45c040360e146"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
