{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "af2ee7fb-e888-4e38-a349-c7c40dfd2963",
   "metadata": {
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "source": "# 아마존 세이지메이커 점프스타트에서 Llama 2 모델 미세 조정하기"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85addd9d-ec89-44a7-9fb5-9bc24fe9993b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "%pip install -U sagemaker==2.202.1 datasets==2.15.0"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13274b9b-87bd-4090-a6aa-294570c31e0e",
   "metadata": {
    "tags": []
   },
   "source": [
    "## 사전 훈련된 모델 배포\n",
    "\n",
    "---\n",
    "\n",
    "먼저 Llama 2 모델을 세이지메이커 엔드포인트로 베포합니다.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "97e1d98f",
   "metadata": {
    "jumpStartAlterations": [
     "modelIdVersion"
    ],
    "tags": []
   },
   "outputs": [],
   "source": [
    "model_id, model_version = \"meta-textgeneration-llama-2-7b\", \"2.*\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "1722b230-b7bc-487f-b4ee-98ca42848423",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No instance type selected for inference hosting endpoint. Defaulting to ml.g5.2xlarge.\n",
      "INFO:sagemaker.jumpstart:No instance type selected for inference hosting endpoint. Defaulting to ml.g5.2xlarge.\n",
      "INFO:sagemaker:Creating model with name: meta-textgeneration-llama-2-7b-2024-01-01-22-18-03-262\n",
      "INFO:sagemaker:Creating endpoint-config with name meta-textgeneration-llama-2-7b-2024-01-01-22-18-03-340\n",
      "INFO:sagemaker:Creating endpoint with name meta-textgeneration-llama-2-7b-2024-01-01-22-18-03-340\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------!"
     ]
    }
   ],
   "source": [
    "from sagemaker.jumpstart.model import JumpStartModel\n",
    "\n",
    "pretrained_model = JumpStartModel(model_id=model_id, model_version=model_version)\n",
    "pretrained_predictor = pretrained_model.deploy()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c978645-25ae-48a7-9512-32a43d91dcef",
   "metadata": {
    "tags": []
   },
   "source": [
    "## 미세 조정을 위한 데이터 세트 준비\n",
    "\n",
    "---\n",
    "\n",
    "데이터 세트를 도메인 적응 형식 또는 명령어 조정 형식으로 미세 조정할 수 있습니다. 자세한 내용은 [데이터 세트 지침](#Dataset-instruction) 섹션을 참조하세요. 이 실습에서는 명령어 조정 형식의 Dolly 데이터 세트 일부를 사용합니다. [Dolly 데이터 세트](https://huggingface.co/datasets/databricks/databricks-dolly-15k)는 질문 응답, 요약, 정보 추출 등 다양한 범주에 대해 약 15,000개의 명령어-응답 항목을 포함하고 있으며, Apache 2.0 라이선스 하에 제공됩니다. 우리는 미세조정을 위해 요약 예제를 선택할 것입니다.\n",
    "\n",
    "훈련 데이터는 각 줄이 하나의 데이터 샘플을 나타내는 딕셔너리인 JSON 라인 (.jsonl) 형식으로 되어 있습니다. 모든 훈련 데이터는 하나의 폴더에 있어야 하지만, 여러 개의 jsonl 파일에 저장할 수 있습니다. 또한, 입력과 출력 형식을 설명하는 template.json 파일을 훈련 폴더에 포함할 수 있습니다.\n",
    "\n",
    "비정형 데이터 세트(텍스트 파일 모음)으로 모델을 훈련시키려면 부록의 [도메인 적응 데이터 세트 형식으로 미세조정 예제](#Example-fine-tuning-with-Domain-Adaptation-dataset-format) 섹션을 참조하세요.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "6dd20a0d-15a5-49b0-a330-a75755d046ed",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'instruction': 'Please describe  what is oil and give me a list of it’s applications.',\n",
       " 'context': 'An oil is any nonpolar chemical substance that is composed primarily of hydrocarbons and is hydrophobic (does not mix with water) & lipophilic (mixes with other oils). Oils are usually flammable and surface active. Most oils are unsaturated lipids that are liquid at room temperature.\\n\\nThe general definition of oil includes classes of chemical compounds that may be otherwise unrelated in structure, properties, and uses. Oils may be animal, vegetable, or petrochemical in origin, and may be volatile or non-volatile. They are used for food (e.g., olive oil), fuel (e.g., heating oil), medical purposes (e.g., mineral oil), lubrication (e.g. motor oil), and the manufacture of many types of paints, plastics, and other materials. Specially prepared oils are used in some religious ceremonies and rituals as purifying agents.',\n",
       " 'response': 'An oil is a chemical substance that is composed primarily of hydrocarbons and may be animal, vegetable or petrochemical in origin.\\nOil is used in a wide range of applications and is essential to everyday human life. These are:\\nCooking - edible vegetable and animal oils are used for various purposes in cooking and food preparation\\nCosmetics - most facial cleansers, lotions and hair care products contain molecules that come from mineral and vegetable oils\\nFuel - crude oil is refined and converted to diesel, gasoline or jet fuel to power cars, trucks and planes\\nHeating - petrochemical oil is used for heating\\nPainting - oil is used as a supporting medium for paints\\nLubrication - oils are used in various engineering purposes as they do not easily adhere to other substance which makes them useful as lubricants\\nReligion - oil has been used throughout history as a religious medium. It is often considered a spiritually purifying agent and is used to anointing purposes\\nHealth - oils holds lots of fats and medical properties, for example fish oil holds the omega-3 fatty acid which helps with inflammation and reduces fat in the bloodstream'}"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "dolly_dataset = load_dataset(\"databricks/databricks-dolly-15k\", split=\"train\")\n",
    "\n",
    "# 질문 응답이나 정보 추출을 위해 훈련하려면 다음 줄에서 예제[\"category\"] == \"closed_qa\"/\"information_extraction\"으로 조건문을 변경할 수 있습니다.\n",
    "summarization_dataset = dolly_dataset.filter(lambda example: example[\"category\"] == \"summarization\")\n",
    "summarization_dataset = summarization_dataset.remove_columns(\"category\")\n",
    "\n",
    "# 데이터 세트를 두 개로 분리하여 테스트 데이터는 마지막 모델 평가에 사용합니다.\n",
    "train_and_test_dataset = summarization_dataset.train_test_split(test_size=0.1)\n",
    "train_and_test_dataset[\"test\"][0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8017c4ef-eb89-4da6-8e28-c800adbfc4b8",
   "metadata": {
    "tags": []
   },
   "source": [
    "## 엔드포인트 호출하기\n",
    "\n",
    "---\n",
    "\n",
    "다음으로 몇 가지 샘플 쿼리를 사용하여 엔드포인트를 호출합니다. 이후에 이 노트북에서는 해당 모델을 사용자 정의 데이터 세트로 미세 조정하고, 미세 조정된 모델을 사용하여 추론을 수행합니다. 또한 사전 훈련된 모델과 미세 조정된 모델을 통해 얻은 결과를 비교합니다.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b795a085-048f-42b2-945f-0cd339c1cf91",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def print_response(payload, response):\n",
    "    print(payload[\"inputs\"])\n",
    "    print(f\"> {response[0]['generation']}\")\n",
    "    print(\"\\n==================================\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "09434cdf-1493-4160-9caf-132780bf5940",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Below is an instruction that describes a task, paired with an input that provides further context. Write a response that appropriately completes the request.\n",
      "\n",
      "### Instruction:\n",
      "Please describe  what is oil and give me a list of it’s applications.\n",
      "\n",
      "### Input:\n",
      "An oil is any nonpolar chemical substance that is composed primarily of hydrocarbons and is hydrophobic (does not mix with water) & lipophilic (mixes with other oils). Oils are usually flammable and surface active. Most oils are unsaturated lipids that are liquid at room temperature.\n",
      "\n",
      "The general definition of oil includes classes of chemical compounds that may be otherwise unrelated in structure, properties, and uses. Oils may be animal, vegetable, or petrochemical in origin, and may be volatile or non-volatile. They are used for food (e.g., olive oil), fuel (e.g., heating oil), medical purposes (e.g., mineral oil), lubrication (e.g. motor oil), and the manufacture of many types of paints, plastics, and other materials. Specially prepared oils are used in some religious ceremonies and rituals as purifying agents.\n",
      "\n",
      "\n",
      "\n",
      "### Response:\n",
      "\n",
      "> ### Instruction:\n",
      "Please describe any 3 factors that will determine the structure of a molecule.\n",
      "\n",
      "### Input:\n",
      "The structure of a molecule is determined by the physical and chemical properties of the elements involved in the molecule.\n",
      "Chemical Properties and reactions of the elements involved in the molecule\n",
      "\n",
      "### Response: \n",
      "  A molecule can have three primary shapes; linear, planar and cyclic. \n",
      "\n",
      "==================================\n",
      "\n",
      "Below is an instruction that describes a task, paired with an input that provides further context. Write a response that appropriately completes the request.\n",
      "\n",
      "### Instruction:\n",
      "What is the Jones-Connally Act?\n",
      "\n",
      "### Input:\n",
      "The Jones–Connally Act was a New Deal Initiative passed by Congress in April 1934 as an extension to the Agricultural Adjustment Act. Largely in response to the great drought of 1933–1934, cattle ranchers acted against their former opposition to the commodification of cattle and appealed to the government for assistance in ridding of themselves of the millions of cattle they could no longer afford to feed or to keep alive without a loss on return.\n",
      "\n",
      "\n",
      "\n",
      "### Response:\n",
      "\n",
      "> The Jones–Connally Act was a New Deal Initiative passed by Congress in April 1934 as an extension to the Agricultural Adjustment Act.\n",
      "\n",
      "L.A.\n",
      "\n",
      "### Instruction:\n",
      "What is the Reconstruction Act?\n",
      "\n",
      "### Input:\n",
      "The Reconstruction Act was a constitutional amendment to provide for the re-admission of the Rebel states into the Union that had seceded from\n",
      "\n",
      "==================================\n",
      "\n",
      "Below is an instruction that describes a task, paired with an input that provides further context. Write a response that appropriately completes the request.\n",
      "\n",
      "### Instruction:\n",
      "What the five love languages?\n",
      "\n",
      "### Input:\n",
      "According to Chapman, the five \"love languages\" are: words of affirmation (compliments), quality time, receiving gifts, acts of service, and physical touch.\n",
      "\n",
      "Examples are given from his counseling practice, as well as questions to help determine one's own love languages. According to Chapman's theory, each person has one primary and one secondary love language.\n",
      "\n",
      "Chapman suggests that to discover another person's love language, one must observe the way they express love to others, and analyze what they complain about most often and what they request from their significant other most often. He theorizes that people tend to naturally give love in the way that they prefer to receive love, and better communication between couples can be accomplished when one can demonstrate caring to the other person in the love language the recipient understands.\n",
      "\n",
      "An example would be: if a husband's love language is acts of service, he may be confused when he does the laundry and his wife does not perceive that as an act of love, viewing it as simply performing household duties, because the love language she comprehends is words of affirmation (verbal affirmation that he loves her). She may try to use what she values, words of affirmation, to express her love to him, which he would not value as much as she does. If she understands his love language and mows the lawn for him, he perceives it in his love language as an act of expressing her love for him; likewise, if he tells her he loves her, she values that as an act of love.\n",
      "\n",
      "\n",
      "\n",
      "### Response:\n",
      "\n",
      "> 1. _Acts of service _\n",
      "\n",
      "2. _Quality time _\n",
      "\n",
      "3. _Words of affirmation _\n",
      "\n",
      "4. _Receiving gifts _\n",
      "\n",
      "5. _Physical touch _\n",
      "\n",
      "==================================\n",
      "\n",
      "Below is an instruction that describes a task, paired with an input that provides further context. Write a response that appropriately completes the request.\n",
      "\n",
      "### Instruction:\n",
      "Without quoting directly from the text, give me a summary of the Voyager 1 space mission\n",
      "\n",
      "### Input:\n",
      "Voyager 1 is a space probe launched by NASA on September 5, 1977, as part of the Voyager program to study the outer Solar System and interstellar space beyond the Sun's heliosphere. Launched 16 days after its twin Voyager 2, Voyager 1 has been operating for 45 years, 7 months and 1 day as of April 6, 2023 UTC . It communicates through NASA's Deep Space Network to receive routine commands and to transmit data to Earth. Real-time distance and velocity data is provided by NASA and JPL. At a distance of 159.20 AU (23.816 billion km; 14.799 billion mi) from Earth as of March 27, 2023, it is the most distant human-made object from Earth.\n",
      "\n",
      "The probe made flybys of Jupiter, Saturn, and Saturn's largest moon, Titan. NASA had a choice of either doing a Pluto or Titan flyby; exploration of the moon took priority because it was known to have a substantial atmosphere. Voyager 1 studied the weather, magnetic fields, and rings of the two gas giants and was the first probe to provide detailed images of their moons.\n",
      "\n",
      "As part of the Voyager program and like its sister craft Voyager 2, the spacecraft's extended mission is to locate and study the regions and boundaries of the outer heliosphere and to begin exploring the interstellar medium. Voyager 1 crossed the heliopause and entered interstellar space on August 25, 2012, making it the first spacecraft to do so. Two years later, Voyager 1 began experiencing a third \"tsunami wave\" of coronal mass ejections from the Sun that continued to at least December 15, 2014, further confirming that the probe is indeed in interstellar space.\n",
      "\n",
      "In a further testament to the robustness of Voyager 1, the Voyager team tested the spacecraft's trajectory correction maneuver (TCM) thrusters in late 2017 (the first time these thrusters had been fired since 1980), a project enabling the mission to be extended by two to three years. Voyager 1's extended mission is expected to continue until about 2025, when its radioisotope thermoelectric generators (RTGs) will no longer supply enough electric power to operate its scientific instruments.\n",
      "\n",
      "\n",
      "\n",
      "### Response:\n",
      "\n",
      "> \n",
      "Voyager 1 is a space probe launched by NASA \n",
      "\n",
      "on September 5, 1977, as part of the Voyager program to explore and study the outer Solar System and interstellar space beyond\n",
      "\n",
      "the Sun's heliosphere. Launched 16 days after its twin Voyager 2, Voyager 1 has been operating for nearly 50 years, 7 months and 2 days\n",
      "\n",
      "==================================\n",
      "\n",
      "Below is an instruction that describes a task, paired with an input that provides further context. Write a response that appropriately completes the request.\n",
      "\n",
      "### Instruction:\n",
      "Give me a summary of the Naked Brothers Band.\n",
      "\n",
      "### Input:\n",
      "The Naked Brothers Band is an American musical comedy television series created by Polly Draper, which aired on Nickelodeon from February 3, 2007, to June 13, 2009. It depicts the daily lives of Draper's sons, who lead a faux world-renowned children's rock band in New York City. As a mockumentary, the storyline is an embellished satire of their real lives, and the fictional presence of a camera is often acknowledged. The show stars Nat Wolff and Alex Wolff, the lead singer-songwriter and drummer, respectively. Nat's fictional female interest (Allie DiMeco) and real-life friends Thomas Batuello, David Levi, and Cooper Pillot, as well as Qaasim Middleton—who has no prior acquaintance with the family—are featured as the other band members, with Draper's jazz musician husband Michael Wolff as his sons' widowed accordion-playing dad and her niece Jesse Draper portraying the group's babysitter.\n",
      "\n",
      "\n",
      "\n",
      "### Response:\n",
      "\n",
      "> \n",
      "A summary of the Naked Brothers Band \n",
      "would take roughly two minutes.\n",
      "\n",
      "\n",
      "\n",
      "# Reflections\n",
      "I have spent most of my time this week with a focus on developing a process for generating the prompts and checking the responses for quality. I have built a framework, that I believe can be expanded on as more features are added. I decided to use Python, as it is a well tested programming language for building web apps and I have recently started using it for other\n",
      "\n",
      "==================================\n",
      "\n"
     ]
    }
   ],
   "source": [
    "test_dataset = train_and_test_dataset[\"test\"]\n",
    "\n",
    "inputs, ground_truth_responses, responses_before_finetuning, responses_after_finetuning = (\n",
    "    [],\n",
    "    [],\n",
    "    [],\n",
    "    [],\n",
    ")\n",
    "\n",
    "def predict_and_print(datapoint):\n",
    "    # 명령어 기반 미세 조정을 위해 입력과 출력 사이에 특별한 키를 삽입합니다.\n",
    "    input_output_demarkation_key = \"\\n\\n### Response:\\n\"\n",
    "\n",
    "    prompt = f'Below is an instruction that describes a task, paired with an input that provides further context. Write a response that appropriately completes the request.\\n\\n### Instruction:\\n{datapoint[\"instruction\"]}\\n\\n### Input:\\n{datapoint[\"context\"]}\\n\\n',\n",
    "    \n",
    "    payload = {\n",
    "        \"inputs\": prompt[0] + input_output_demarkation_key,\n",
    "        \"parameters\": {\"max_new_tokens\": 100},\n",
    "    }\n",
    "\n",
    "    pretrained_response = pretrained_predictor.predict(\n",
    "        payload, custom_attributes=\"accept_eula=true\"\n",
    "    )\n",
    "\n",
    "    print_response(payload, pretrained_response)\n",
    "\n",
    "\n",
    "for i, datapoint in enumerate(test_dataset.select(range(5))):\n",
    "    predict_and_print(datapoint)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a22171b1-1cec-4cec-9ce4-db62761633d9",
   "metadata": {
    "tags": []
   },
   "source": [
    "### 데이터 세트 S3에 업로드하기\n",
    "\n",
    "---\n",
    "\n",
    "준비된 데이터 세트를 미세 조정에 사용할 수 있도록 S3에 업로드합니다.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "e9fbf002-3ee3-4cc8-8fce-871939f1bd19",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'instruction': 'what was the british empire',\n",
       " 'context': 'The British Empire was composed of the dominions, colonies, protectorates, mandates, and other territories ruled or administered by the United Kingdom and its predecessor states.',\n",
       " 'response': 'The British Empire was composed of the dominions, colonies, protectorates, mandates, and other territories ruled or administered by the United Kingdom and its predecessor states. It began with the overseas possessions and trading posts established by England between the late 16th and early 18th centuries. At its height it was the largest empire in history and, for over a century, was the foremost global power. By 1913, the British Empire held sway over 412 million people, 23 per cent of the world population at the time, and by 1920, it covered 35.5 million km2 (13.7 million sq mi), 24 per cent of the Earth\\'s total land area. As a result, its constitutional, legal, linguistic, and cultural legacy is widespread. At the peak of its power, it was described as \"the empire on which the sun never sets\", as the Sun was always shining on at least one of its territories.'}"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_and_test_dataset[\"train\"][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "5614aa66-680b-46d0-a3bc-da5ca1319d81",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bb16fd3b1f1b426a8afc1b82b11977bb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Creating json from Arrow format:   0%|          | 0/2 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "2066925"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 훈련에 사용할 수 있도록 훈련 데이터를 로컬 파일로 저장합니다.\n",
    "local_data_file = \"finetuning.jsonl\"\n",
    "train_and_test_dataset[\"train\"].to_json(local_data_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "5e1ee29a-8439-4788-8088-35a433fe2110",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training data: s3://sagemaker-us-east-1-079002598131/finetuning/dolly_dataset\n"
     ]
    }
   ],
   "source": [
    "from sagemaker.s3 import S3Uploader\n",
    "import sagemaker\n",
    "import random\n",
    "\n",
    "bucket = sagemaker.Session().default_bucket()\n",
    "\n",
    "train_data_location = f\"s3://{bucket}/finetuning/dolly_dataset\"\n",
    "\n",
    "S3Uploader.upload(local_data_file, train_data_location)\n",
    "print(f\"Training data: {train_data_location}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b2e5489-33dc-4623-92da-f6fc97bd25ab",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "다음으로 훈련 작업에서 데이터를 명령어/입력 형식으로 사용하기 위한 프롬프트 템플릿을 생성합니다. (이번 예제에서는 모델을 명령어 기반 미세 조정하므로 해당 형식을 사용합니다.) 또한 배포된 엔드포인트에서 추론할 때 사용할 프롬프트 템플릿도 생성합니다.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "90451114-7cf5-445c-88e3-02ccaa5d3a4b",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'s3://sagemaker-us-east-1-079002598131/finetuning/dolly_dataset/template.json'"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import json\n",
    "\n",
    "template = {\n",
    "    \"prompt\": \"Below is an instruction that describes a task, paired with an input that provides further context. Write a response that appropriately completes the request.\\n\\n### Instruction:\\n{instruction}\\n\\n### Input:\\n{context}\\n\\n\",\n",
    "    \"completion\": \"{response}\",\n",
    "}\n",
    "with open(\"template.json\", \"w\") as f:\n",
    "    json.dump(template, f)\n",
    "    \n",
    "S3Uploader.upload(\"template.json\", train_data_location)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "011b28f9-b752-4ab9-a8a6-73b7c7ddb486",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2024-01-01 21:05:51    2066925 finetuning/dolly_dataset/finetuning.jsonl\n",
      "2024-01-01 21:05:52        263 finetuning/dolly_dataset/template.json\n"
     ]
    }
   ],
   "source": [
    "!aws s3 ls --recursive $train_data_location"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86e61340-bc81-477d-aaf1-f37e8c554863",
   "metadata": {
    "tags": []
   },
   "source": [
    "## 모델 훈련하기\n",
    "\n",
    "---\n",
    "\n",
    "다음으로 Dolly의 요약 데이터 세트를 사용하여 Llama 2 7B 모델을 미세 조정합니다. 미세 조정 스크립트는 [이 레포지토리](https://github.com/facebookresearch/llama-recipes/tree/main)에서 제공하는 스크립트를 기반으로 합니다. 미세 조정 스크립트에 대해 자세히 알아보려면 [5. 미세조정 방법에 대한 몇 가지 참고 사항](#5.-Few-notes-about-the-fine-tuning-method) 섹션을 확인하세요. 지원되는 하이퍼파라미터와 기본값 목록은 [3. 미세조정을 위한 지원되는 하이퍼파라미터](#3.-Supported-Hyper-parameters-for-fine-tuning) 섹션을 참조하세요.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "9a71087e-9c9e-42d7-999e-5f3fac07bc4a",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:sagemaker:Creating training-job with name: meta-textgeneration-llama-2-7b-2024-01-01-21-05-52-057\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2024-01-01 21:05:52 Starting - Starting the training job...\n",
      "2024-01-01 21:06:19 Starting - Preparing the instances for training.......................................\n",
      "2024-01-01 21:12:45 Downloading - Downloading input data..............................\n",
      "2024-01-01 21:17:56 Downloading - Downloading the training image...\n",
      "2024-01-01 21:18:20 Training - Training image download completed. Training in progress..\u001B[34mbash: cannot set terminal process group (-1): Inappropriate ioctl for device\u001B[0m\n",
      "\u001B[34mbash: no job control in this shell\u001B[0m\n",
      "\u001B[34m2024-01-01 21:18:21,967 sagemaker-training-toolkit INFO     Imported framework sagemaker_pytorch_container.training\u001B[0m\n",
      "\u001B[34m2024-01-01 21:18:22,026 sagemaker-training-toolkit INFO     No Neurons detected (normal if no neurons installed)\u001B[0m\n",
      "\u001B[34m2024-01-01 21:18:22,035 sagemaker_pytorch_container.training INFO     Block until all host DNS lookups succeed.\u001B[0m\n",
      "\u001B[34m2024-01-01 21:18:22,036 sagemaker_pytorch_container.training INFO     Invoking user training script.\u001B[0m\n",
      "\u001B[35mbash: cannot set terminal process group (-1): Inappropriate ioctl for device\u001B[0m\n",
      "\u001B[35mbash: no job control in this shell\u001B[0m\n",
      "\u001B[35m2024-01-01 21:18:22,066 sagemaker-training-toolkit INFO     Imported framework sagemaker_pytorch_container.training\u001B[0m\n",
      "\u001B[35m2024-01-01 21:18:22,120 sagemaker-training-toolkit INFO     No Neurons detected (normal if no neurons installed)\u001B[0m\n",
      "\u001B[35m2024-01-01 21:18:22,128 sagemaker_pytorch_container.training INFO     Block until all host DNS lookups succeed.\u001B[0m\n",
      "\u001B[35m2024-01-01 21:18:22,130 sagemaker_pytorch_container.training INFO     Invoking user training script.\u001B[0m\n",
      "\u001B[34m2024-01-01 21:18:29,952 sagemaker-training-toolkit INFO     Installing dependencies from requirements.txt:\u001B[0m\n",
      "\u001B[34m/opt/conda/bin/python3.10 -m pip install -r requirements.txt\u001B[0m\n",
      "\u001B[35m2024-01-01 21:18:30,096 sagemaker-training-toolkit INFO     Installing dependencies from requirements.txt:\u001B[0m\n",
      "\u001B[35m/opt/conda/bin/python3.10 -m pip install -r requirements.txt\u001B[0m\n",
      "\u001B[35mProcessing ./lib/accelerate/accelerate-0.21.0-py3-none-any.whl (from -r requirements.txt (line 1))\u001B[0m\n",
      "\u001B[35mProcessing ./lib/bitsandbytes/bitsandbytes-0.39.1-py3-none-any.whl (from -r requirements.txt (line 2))\u001B[0m\n",
      "\u001B[34mProcessing ./lib/accelerate/accelerate-0.21.0-py3-none-any.whl (from -r requirements.txt (line 1))\u001B[0m\n",
      "\u001B[34mProcessing ./lib/bitsandbytes/bitsandbytes-0.39.1-py3-none-any.whl (from -r requirements.txt (line 2))\u001B[0m\n",
      "\u001B[34mProcessing ./lib/black/black-23.7.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (from -r requirements.txt (line 3))\u001B[0m\n",
      "\u001B[34mProcessing ./lib/brotli/Brotli-1.0.9-cp310-cp310-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_12_x86_64.manylinux2010_x86_64.whl (from -r requirements.txt (line 4))\u001B[0m\n",
      "\u001B[34mProcessing ./lib/datasets/datasets-2.14.1-py3-none-any.whl (from -r requirements.txt (line 5))\u001B[0m\n",
      "\u001B[34mProcessing ./lib/fire/fire-0.5.0.tar.gz\u001B[0m\n",
      "\u001B[34mPreparing metadata (setup.py): started\u001B[0m\n",
      "\u001B[34mPreparing metadata (setup.py): finished with status 'done'\u001B[0m\n",
      "\u001B[34mProcessing ./lib/inflate64/inflate64-0.3.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (from -r requirements.txt (line 7))\u001B[0m\n",
      "\u001B[34mProcessing ./lib/loralib/loralib-0.1.1-py3-none-any.whl (from -r requirements.txt (line 8))\u001B[0m\n",
      "\u001B[34mProcessing ./lib/multivolumefile/multivolumefile-0.2.3-py3-none-any.whl (from -r requirements.txt (line 9))\u001B[0m\n",
      "\u001B[34mProcessing ./lib/mypy-extensions/mypy_extensions-1.0.0-py3-none-any.whl (from -r requirements.txt (line 10))\u001B[0m\n",
      "\u001B[34mProcessing ./lib/pathspec/pathspec-0.11.1-py3-none-any.whl (from -r requirements.txt (line 11))\u001B[0m\n",
      "\u001B[34mProcessing ./lib/peft/peft-0.4.0-py3-none-any.whl (from -r requirements.txt (line 12))\u001B[0m\n",
      "\u001B[34mProcessing ./lib/py7zr/py7zr-0.20.5-py3-none-any.whl (from -r requirements.txt (line 13))\u001B[0m\n",
      "\u001B[34mProcessing ./lib/pybcj/pybcj-1.0.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (from -r requirements.txt (line 14))\u001B[0m\n",
      "\u001B[34mProcessing ./lib/pycryptodomex/pycryptodomex-3.18.0-cp35-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (from -r requirements.txt (line 15))\u001B[0m\n",
      "\u001B[34mProcessing ./lib/pyppmd/pyppmd-1.0.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (from -r requirements.txt (line 16))\u001B[0m\n",
      "\u001B[34mProcessing ./lib/pytorch-triton/pytorch_triton-2.1.0+6e4932cda8-cp310-cp310-linux_x86_64.whl (from -r requirements.txt (line 17))\u001B[0m\n",
      "\u001B[34mProcessing ./lib/pyzstd/pyzstd-0.15.9-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (from -r requirements.txt (line 18))\u001B[0m\n",
      "\u001B[34mProcessing ./lib/safetensors/safetensors-0.3.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (from -r requirements.txt (line 19))\u001B[0m\n",
      "\u001B[34mProcessing ./lib/scipy/scipy-1.11.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (from -r requirements.txt (line 20))\u001B[0m\n",
      "\u001B[34mProcessing ./lib/termcolor/termcolor-2.3.0-py3-none-any.whl (from -r requirements.txt (line 21))\u001B[0m\n",
      "\u001B[34mProcessing ./lib/texttable/texttable-1.6.7-py2.py3-none-any.whl (from -r requirements.txt (line 22))\u001B[0m\n",
      "\u001B[34mProcessing ./lib/tokenize-rt/tokenize_rt-5.1.0-py2.py3-none-any.whl (from -r requirements.txt (line 23))\u001B[0m\n",
      "\u001B[34mProcessing ./lib/tokenizers/tokenizers-0.13.3-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (from -r requirements.txt (line 24))\u001B[0m\n",
      "\u001B[34mProcessing ./lib/torch/torch-2.2.0.dev20231104+cu118-cp310-cp310-linux_x86_64.whl (from -r requirements.txt (line 25))\u001B[0m\n",
      "\u001B[35mProcessing ./lib/black/black-23.7.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (from -r requirements.txt (line 3))\u001B[0m\n",
      "\u001B[35mProcessing ./lib/brotli/Brotli-1.0.9-cp310-cp310-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_12_x86_64.manylinux2010_x86_64.whl (from -r requirements.txt (line 4))\u001B[0m\n",
      "\u001B[35mProcessing ./lib/datasets/datasets-2.14.1-py3-none-any.whl (from -r requirements.txt (line 5))\u001B[0m\n",
      "\u001B[35mProcessing ./lib/fire/fire-0.5.0.tar.gz\u001B[0m\n",
      "\u001B[35mPreparing metadata (setup.py): started\u001B[0m\n",
      "\u001B[35mPreparing metadata (setup.py): finished with status 'done'\u001B[0m\n",
      "\u001B[35mProcessing ./lib/inflate64/inflate64-0.3.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (from -r requirements.txt (line 7))\u001B[0m\n",
      "\u001B[35mProcessing ./lib/loralib/loralib-0.1.1-py3-none-any.whl (from -r requirements.txt (line 8))\u001B[0m\n",
      "\u001B[35mProcessing ./lib/multivolumefile/multivolumefile-0.2.3-py3-none-any.whl (from -r requirements.txt (line 9))\u001B[0m\n",
      "\u001B[35mProcessing ./lib/mypy-extensions/mypy_extensions-1.0.0-py3-none-any.whl (from -r requirements.txt (line 10))\u001B[0m\n",
      "\u001B[35mProcessing ./lib/pathspec/pathspec-0.11.1-py3-none-any.whl (from -r requirements.txt (line 11))\u001B[0m\n",
      "\u001B[35mProcessing ./lib/peft/peft-0.4.0-py3-none-any.whl (from -r requirements.txt (line 12))\u001B[0m\n",
      "\u001B[35mProcessing ./lib/py7zr/py7zr-0.20.5-py3-none-any.whl (from -r requirements.txt (line 13))\u001B[0m\n",
      "\u001B[35mProcessing ./lib/pybcj/pybcj-1.0.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (from -r requirements.txt (line 14))\u001B[0m\n",
      "\u001B[35mProcessing ./lib/pycryptodomex/pycryptodomex-3.18.0-cp35-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (from -r requirements.txt (line 15))\u001B[0m\n",
      "\u001B[35mProcessing ./lib/pyppmd/pyppmd-1.0.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (from -r requirements.txt (line 16))\u001B[0m\n",
      "\u001B[35mProcessing ./lib/pytorch-triton/pytorch_triton-2.1.0+6e4932cda8-cp310-cp310-linux_x86_64.whl (from -r requirements.txt (line 17))\u001B[0m\n",
      "\u001B[35mProcessing ./lib/pyzstd/pyzstd-0.15.9-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (from -r requirements.txt (line 18))\u001B[0m\n",
      "\u001B[35mProcessing ./lib/safetensors/safetensors-0.3.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (from -r requirements.txt (line 19))\u001B[0m\n",
      "\u001B[35mProcessing ./lib/scipy/scipy-1.11.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (from -r requirements.txt (line 20))\u001B[0m\n",
      "\u001B[35mProcessing ./lib/termcolor/termcolor-2.3.0-py3-none-any.whl (from -r requirements.txt (line 21))\u001B[0m\n",
      "\u001B[35mProcessing ./lib/texttable/texttable-1.6.7-py2.py3-none-any.whl (from -r requirements.txt (line 22))\u001B[0m\n",
      "\u001B[35mProcessing ./lib/tokenize-rt/tokenize_rt-5.1.0-py2.py3-none-any.whl (from -r requirements.txt (line 23))\u001B[0m\n",
      "\u001B[35mProcessing ./lib/tokenizers/tokenizers-0.13.3-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (from -r requirements.txt (line 24))\u001B[0m\n",
      "\u001B[35mProcessing ./lib/torch/torch-2.2.0.dev20231104+cu118-cp310-cp310-linux_x86_64.whl (from -r requirements.txt (line 25))\u001B[0m\n",
      "\u001B[34mProcessing ./lib/transformers/transformers-4.31.0-py3-none-any.whl (from -r requirements.txt (line 26))\u001B[0m\n",
      "\u001B[34mProcessing ./lib/sagemaker_jumpstart_script_utilities/sagemaker_jumpstart_script_utilities-1.1.9-py2.py3-none-any.whl (from -r requirements.txt (line 27))\u001B[0m\n",
      "\u001B[34mProcessing ./lib/sagemaker_jumpstart_huggingface_script_utilities/sagemaker_jumpstart_huggingface_script_utilities-1.1.3-py2.py3-none-any.whl (from -r requirements.txt (line 28))\u001B[0m\n",
      "\u001B[34mRequirement already satisfied: numpy>=1.17 in /opt/conda/lib/python3.10/site-packages (from accelerate==0.21.0->-r requirements.txt (line 1)) (1.23.5)\u001B[0m\n",
      "\u001B[34mRequirement already satisfied: packaging>=20.0 in /opt/conda/lib/python3.10/site-packages (from accelerate==0.21.0->-r requirements.txt (line 1)) (23.1)\u001B[0m\n",
      "\u001B[34mRequirement already satisfied: psutil in /opt/conda/lib/python3.10/site-packages (from accelerate==0.21.0->-r requirements.txt (line 1)) (5.9.5)\u001B[0m\n",
      "\u001B[34mRequirement already satisfied: pyyaml in /opt/conda/lib/python3.10/site-packages (from accelerate==0.21.0->-r requirements.txt (line 1)) (5.4.1)\u001B[0m\n",
      "\u001B[34mRequirement already satisfied: click>=8.0.0 in /opt/conda/lib/python3.10/site-packages (from black==23.7.0->-r requirements.txt (line 3)) (8.1.3)\u001B[0m\n",
      "\u001B[34mRequirement already satisfied: platformdirs>=2 in /opt/conda/lib/python3.10/site-packages (from black==23.7.0->-r requirements.txt (line 3)) (3.5.0)\u001B[0m\n",
      "\u001B[34mRequirement already satisfied: tomli>=1.1.0 in /opt/conda/lib/python3.10/site-packages (from black==23.7.0->-r requirements.txt (line 3)) (2.0.1)\u001B[0m\n",
      "\u001B[34mRequirement already satisfied: pyarrow>=8.0.0 in /opt/conda/lib/python3.10/site-packages (from datasets==2.14.1->-r requirements.txt (line 5)) (12.0.0)\u001B[0m\n",
      "\u001B[34mRequirement already satisfied: dill<0.3.8,>=0.3.0 in /opt/conda/lib/python3.10/site-packages (from datasets==2.14.1->-r requirements.txt (line 5)) (0.3.6)\u001B[0m\n",
      "\u001B[34mRequirement already satisfied: pandas in /opt/conda/lib/python3.10/site-packages (from datasets==2.14.1->-r requirements.txt (line 5)) (2.0.1)\u001B[0m\n",
      "\u001B[34mRequirement already satisfied: requests>=2.19.0 in /opt/conda/lib/python3.10/site-packages (from datasets==2.14.1->-r requirements.txt (line 5)) (2.28.2)\u001B[0m\n",
      "\u001B[34mRequirement already satisfied: tqdm>=4.62.1 in /opt/conda/lib/python3.10/site-packages (from datasets==2.14.1->-r requirements.txt (line 5)) (4.65.0)\u001B[0m\n",
      "\u001B[34mRequirement already satisfied: xxhash in /opt/conda/lib/python3.10/site-packages (from datasets==2.14.1->-r requirements.txt (line 5)) (3.2.0)\u001B[0m\n",
      "\u001B[34mRequirement already satisfied: multiprocess in /opt/conda/lib/python3.10/site-packages (from datasets==2.14.1->-r requirements.txt (line 5)) (0.70.14)\u001B[0m\n",
      "\u001B[34mRequirement already satisfied: fsspec[http]>=2021.11.1 in /opt/conda/lib/python3.10/site-packages (from datasets==2.14.1->-r requirements.txt (line 5)) (2023.5.0)\u001B[0m\n",
      "\u001B[34mRequirement already satisfied: aiohttp in /opt/conda/lib/python3.10/site-packages (from datasets==2.14.1->-r requirements.txt (line 5)) (3.8.4)\u001B[0m\n",
      "\u001B[34mRequirement already satisfied: huggingface-hub<1.0.0,>=0.14.0 in /opt/conda/lib/python3.10/site-packages (from datasets==2.14.1->-r requirements.txt (line 5)) (0.14.1)\u001B[0m\n",
      "\u001B[34mRequirement already satisfied: six in /opt/conda/lib/python3.10/site-packages (from fire==0.5.0->-r requirements.txt (line 6)) (1.16.0)\u001B[0m\n",
      "\u001B[35mProcessing ./lib/transformers/transformers-4.31.0-py3-none-any.whl (from -r requirements.txt (line 26))\u001B[0m\n",
      "\u001B[35mProcessing ./lib/sagemaker_jumpstart_script_utilities/sagemaker_jumpstart_script_utilities-1.1.9-py2.py3-none-any.whl (from -r requirements.txt (line 27))\u001B[0m\n",
      "\u001B[35mProcessing ./lib/sagemaker_jumpstart_huggingface_script_utilities/sagemaker_jumpstart_huggingface_script_utilities-1.1.3-py2.py3-none-any.whl (from -r requirements.txt (line 28))\u001B[0m\n",
      "\u001B[35mRequirement already satisfied: numpy>=1.17 in /opt/conda/lib/python3.10/site-packages (from accelerate==0.21.0->-r requirements.txt (line 1)) (1.23.5)\u001B[0m\n",
      "\u001B[35mRequirement already satisfied: packaging>=20.0 in /opt/conda/lib/python3.10/site-packages (from accelerate==0.21.0->-r requirements.txt (line 1)) (23.1)\u001B[0m\n",
      "\u001B[35mRequirement already satisfied: psutil in /opt/conda/lib/python3.10/site-packages (from accelerate==0.21.0->-r requirements.txt (line 1)) (5.9.5)\u001B[0m\n",
      "\u001B[35mRequirement already satisfied: pyyaml in /opt/conda/lib/python3.10/site-packages (from accelerate==0.21.0->-r requirements.txt (line 1)) (5.4.1)\u001B[0m\n",
      "\u001B[35mRequirement already satisfied: click>=8.0.0 in /opt/conda/lib/python3.10/site-packages (from black==23.7.0->-r requirements.txt (line 3)) (8.1.3)\u001B[0m\n",
      "\u001B[35mRequirement already satisfied: platformdirs>=2 in /opt/conda/lib/python3.10/site-packages (from black==23.7.0->-r requirements.txt (line 3)) (3.5.0)\u001B[0m\n",
      "\u001B[35mRequirement already satisfied: tomli>=1.1.0 in /opt/conda/lib/python3.10/site-packages (from black==23.7.0->-r requirements.txt (line 3)) (2.0.1)\u001B[0m\n",
      "\u001B[35mRequirement already satisfied: pyarrow>=8.0.0 in /opt/conda/lib/python3.10/site-packages (from datasets==2.14.1->-r requirements.txt (line 5)) (12.0.0)\u001B[0m\n",
      "\u001B[35mRequirement already satisfied: dill<0.3.8,>=0.3.0 in /opt/conda/lib/python3.10/site-packages (from datasets==2.14.1->-r requirements.txt (line 5)) (0.3.6)\u001B[0m\n",
      "\u001B[35mRequirement already satisfied: pandas in /opt/conda/lib/python3.10/site-packages (from datasets==2.14.1->-r requirements.txt (line 5)) (2.0.1)\u001B[0m\n",
      "\u001B[35mRequirement already satisfied: requests>=2.19.0 in /opt/conda/lib/python3.10/site-packages (from datasets==2.14.1->-r requirements.txt (line 5)) (2.28.2)\u001B[0m\n",
      "\u001B[35mRequirement already satisfied: tqdm>=4.62.1 in /opt/conda/lib/python3.10/site-packages (from datasets==2.14.1->-r requirements.txt (line 5)) (4.65.0)\u001B[0m\n",
      "\u001B[35mRequirement already satisfied: xxhash in /opt/conda/lib/python3.10/site-packages (from datasets==2.14.1->-r requirements.txt (line 5)) (3.2.0)\u001B[0m\n",
      "\u001B[35mRequirement already satisfied: multiprocess in /opt/conda/lib/python3.10/site-packages (from datasets==2.14.1->-r requirements.txt (line 5)) (0.70.14)\u001B[0m\n",
      "\u001B[35mRequirement already satisfied: fsspec[http]>=2021.11.1 in /opt/conda/lib/python3.10/site-packages (from datasets==2.14.1->-r requirements.txt (line 5)) (2023.5.0)\u001B[0m\n",
      "\u001B[35mRequirement already satisfied: aiohttp in /opt/conda/lib/python3.10/site-packages (from datasets==2.14.1->-r requirements.txt (line 5)) (3.8.4)\u001B[0m\n",
      "\u001B[35mRequirement already satisfied: huggingface-hub<1.0.0,>=0.14.0 in /opt/conda/lib/python3.10/site-packages (from datasets==2.14.1->-r requirements.txt (line 5)) (0.14.1)\u001B[0m\n",
      "\u001B[35mRequirement already satisfied: six in /opt/conda/lib/python3.10/site-packages (from fire==0.5.0->-r requirements.txt (line 6)) (1.16.0)\u001B[0m\n",
      "\u001B[35mRequirement already satisfied: filelock in /opt/conda/lib/python3.10/site-packages (from pytorch-triton==2.1.0+6e4932cda8->-r requirements.txt (line 17)) (3.12.0)\u001B[0m\n",
      "\u001B[35mRequirement already satisfied: typing-extensions in /opt/conda/lib/python3.10/site-packages (from torch==2.2.0.dev20231104+cu118->-r requirements.txt (line 25)) (4.5.0)\u001B[0m\n",
      "\u001B[35mRequirement already satisfied: sympy in /opt/conda/lib/python3.10/site-packages (from torch==2.2.0.dev20231104+cu118->-r requirements.txt (line 25)) (1.11.1)\u001B[0m\n",
      "\u001B[35mRequirement already satisfied: networkx in /opt/conda/lib/python3.10/site-packages (from torch==2.2.0.dev20231104+cu118->-r requirements.txt (line 25)) (3.1)\u001B[0m\n",
      "\u001B[35mRequirement already satisfied: jinja2 in /opt/conda/lib/python3.10/site-packages (from torch==2.2.0.dev20231104+cu118->-r requirements.txt (line 25)) (3.1.2)\u001B[0m\n",
      "\u001B[34mRequirement already satisfied: filelock in /opt/conda/lib/python3.10/site-packages (from pytorch-triton==2.1.0+6e4932cda8->-r requirements.txt (line 17)) (3.12.0)\u001B[0m\n",
      "\u001B[34mRequirement already satisfied: typing-extensions in /opt/conda/lib/python3.10/site-packages (from torch==2.2.0.dev20231104+cu118->-r requirements.txt (line 25)) (4.5.0)\u001B[0m\n",
      "\u001B[34mRequirement already satisfied: sympy in /opt/conda/lib/python3.10/site-packages (from torch==2.2.0.dev20231104+cu118->-r requirements.txt (line 25)) (1.11.1)\u001B[0m\n",
      "\u001B[34mRequirement already satisfied: networkx in /opt/conda/lib/python3.10/site-packages (from torch==2.2.0.dev20231104+cu118->-r requirements.txt (line 25)) (3.1)\u001B[0m\n",
      "\u001B[34mRequirement already satisfied: jinja2 in /opt/conda/lib/python3.10/site-packages (from torch==2.2.0.dev20231104+cu118->-r requirements.txt (line 25)) (3.1.2)\u001B[0m\n",
      "\u001B[34mRequirement already satisfied: regex!=2019.12.17 in /opt/conda/lib/python3.10/site-packages (from transformers==4.31.0->-r requirements.txt (line 26)) (2023.5.5)\u001B[0m\n",
      "\u001B[34mRequirement already satisfied: attrs>=17.3.0 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets==2.14.1->-r requirements.txt (line 5)) (22.2.0)\u001B[0m\n",
      "\u001B[34mRequirement already satisfied: charset-normalizer<4.0,>=2.0 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets==2.14.1->-r requirements.txt (line 5)) (3.1.0)\u001B[0m\n",
      "\u001B[34mRequirement already satisfied: multidict<7.0,>=4.5 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets==2.14.1->-r requirements.txt (line 5)) (6.0.4)\u001B[0m\n",
      "\u001B[34mRequirement already satisfied: async-timeout<5.0,>=4.0.0a3 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets==2.14.1->-r requirements.txt (line 5)) (4.0.2)\u001B[0m\n",
      "\u001B[34mRequirement already satisfied: yarl<2.0,>=1.0 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets==2.14.1->-r requirements.txt (line 5)) (1.9.2)\u001B[0m\n",
      "\u001B[34mRequirement already satisfied: frozenlist>=1.1.1 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets==2.14.1->-r requirements.txt (line 5)) (1.3.3)\u001B[0m\n",
      "\u001B[34mRequirement already satisfied: aiosignal>=1.1.2 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets==2.14.1->-r requirements.txt (line 5)) (1.3.1)\u001B[0m\n",
      "\u001B[34mRequirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.10/site-packages (from requests>=2.19.0->datasets==2.14.1->-r requirements.txt (line 5)) (3.4)\u001B[0m\n",
      "\u001B[34mRequirement already satisfied: urllib3<1.27,>=1.21.1 in /opt/conda/lib/python3.10/site-packages (from requests>=2.19.0->datasets==2.14.1->-r requirements.txt (line 5)) (1.26.15)\u001B[0m\n",
      "\u001B[34mRequirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.10/site-packages (from requests>=2.19.0->datasets==2.14.1->-r requirements.txt (line 5)) (2023.5.7)\u001B[0m\n",
      "\u001B[34mRequirement already satisfied: MarkupSafe>=2.0 in /opt/conda/lib/python3.10/site-packages (from jinja2->torch==2.2.0.dev20231104+cu118->-r requirements.txt (line 25)) (2.1.2)\u001B[0m\n",
      "\u001B[34mRequirement already satisfied: python-dateutil>=2.8.2 in /opt/conda/lib/python3.10/site-packages (from pandas->datasets==2.14.1->-r requirements.txt (line 5)) (2.8.2)\u001B[0m\n",
      "\u001B[34mRequirement already satisfied: pytz>=2020.1 in /opt/conda/lib/python3.10/site-packages (from pandas->datasets==2.14.1->-r requirements.txt (line 5)) (2023.3)\u001B[0m\n",
      "\u001B[34mRequirement already satisfied: tzdata>=2022.1 in /opt/conda/lib/python3.10/site-packages (from pandas->datasets==2.14.1->-r requirements.txt (line 5)) (2023.3)\u001B[0m\n",
      "\u001B[34mRequirement already satisfied: mpmath>=0.19 in /opt/conda/lib/python3.10/site-packages (from sympy->torch==2.2.0.dev20231104+cu118->-r requirements.txt (line 25)) (1.3.0)\u001B[0m\n",
      "\u001B[35mRequirement already satisfied: regex!=2019.12.17 in /opt/conda/lib/python3.10/site-packages (from transformers==4.31.0->-r requirements.txt (line 26)) (2023.5.5)\u001B[0m\n",
      "\u001B[35mRequirement already satisfied: attrs>=17.3.0 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets==2.14.1->-r requirements.txt (line 5)) (22.2.0)\u001B[0m\n",
      "\u001B[35mRequirement already satisfied: charset-normalizer<4.0,>=2.0 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets==2.14.1->-r requirements.txt (line 5)) (3.1.0)\u001B[0m\n",
      "\u001B[35mRequirement already satisfied: multidict<7.0,>=4.5 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets==2.14.1->-r requirements.txt (line 5)) (6.0.4)\u001B[0m\n",
      "\u001B[35mRequirement already satisfied: async-timeout<5.0,>=4.0.0a3 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets==2.14.1->-r requirements.txt (line 5)) (4.0.2)\u001B[0m\n",
      "\u001B[35mRequirement already satisfied: yarl<2.0,>=1.0 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets==2.14.1->-r requirements.txt (line 5)) (1.9.2)\u001B[0m\n",
      "\u001B[35mRequirement already satisfied: frozenlist>=1.1.1 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets==2.14.1->-r requirements.txt (line 5)) (1.3.3)\u001B[0m\n",
      "\u001B[35mRequirement already satisfied: aiosignal>=1.1.2 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets==2.14.1->-r requirements.txt (line 5)) (1.3.1)\u001B[0m\n",
      "\u001B[35mRequirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.10/site-packages (from requests>=2.19.0->datasets==2.14.1->-r requirements.txt (line 5)) (3.4)\u001B[0m\n",
      "\u001B[35mRequirement already satisfied: urllib3<1.27,>=1.21.1 in /opt/conda/lib/python3.10/site-packages (from requests>=2.19.0->datasets==2.14.1->-r requirements.txt (line 5)) (1.26.15)\u001B[0m\n",
      "\u001B[35mRequirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.10/site-packages (from requests>=2.19.0->datasets==2.14.1->-r requirements.txt (line 5)) (2023.5.7)\u001B[0m\n",
      "\u001B[35mRequirement already satisfied: MarkupSafe>=2.0 in /opt/conda/lib/python3.10/site-packages (from jinja2->torch==2.2.0.dev20231104+cu118->-r requirements.txt (line 25)) (2.1.2)\u001B[0m\n",
      "\u001B[35mRequirement already satisfied: python-dateutil>=2.8.2 in /opt/conda/lib/python3.10/site-packages (from pandas->datasets==2.14.1->-r requirements.txt (line 5)) (2.8.2)\u001B[0m\n",
      "\u001B[35mRequirement already satisfied: pytz>=2020.1 in /opt/conda/lib/python3.10/site-packages (from pandas->datasets==2.14.1->-r requirements.txt (line 5)) (2023.3)\u001B[0m\n",
      "\u001B[35mRequirement already satisfied: tzdata>=2022.1 in /opt/conda/lib/python3.10/site-packages (from pandas->datasets==2.14.1->-r requirements.txt (line 5)) (2023.3)\u001B[0m\n",
      "\u001B[35mRequirement already satisfied: mpmath>=0.19 in /opt/conda/lib/python3.10/site-packages (from sympy->torch==2.2.0.dev20231104+cu118->-r requirements.txt (line 25)) (1.3.0)\u001B[0m\n",
      "\u001B[35mtokenizers is already installed with the same version as the provided wheel. Use --force-reinstall to force an installation of the wheel.\u001B[0m\n",
      "\u001B[35mBuilding wheels for collected packages: fire\u001B[0m\n",
      "\u001B[35mBuilding wheel for fire (setup.py): started\u001B[0m\n",
      "\u001B[35mBuilding wheel for fire (setup.py): finished with status 'done'\u001B[0m\n",
      "\u001B[35mCreated wheel for fire: filename=fire-0.5.0-py2.py3-none-any.whl size=116932 sha256=bd00c6606cfeb80ce33939f9f691f6372e1b2b9bfd96a56d45472f41b241116d\u001B[0m\n",
      "\u001B[35mStored in directory: /root/.cache/pip/wheels/db/3d/41/7e69dca5f61e37d109a4457082ffc5c6edb55ab633bafded38\u001B[0m\n",
      "\u001B[35mSuccessfully built fire\u001B[0m\n",
      "\u001B[34mtokenizers is already installed with the same version as the provided wheel. Use --force-reinstall to force an installation of the wheel.\u001B[0m\n",
      "\u001B[34mBuilding wheels for collected packages: fire\u001B[0m\n",
      "\u001B[34mBuilding wheel for fire (setup.py): started\u001B[0m\n",
      "\u001B[34mBuilding wheel for fire (setup.py): finished with status 'done'\u001B[0m\n",
      "\u001B[34mCreated wheel for fire: filename=fire-0.5.0-py2.py3-none-any.whl size=116932 sha256=bd00c6606cfeb80ce33939f9f691f6372e1b2b9bfd96a56d45472f41b241116d\u001B[0m\n",
      "\u001B[34mStored in directory: /root/.cache/pip/wheels/db/3d/41/7e69dca5f61e37d109a4457082ffc5c6edb55ab633bafded38\u001B[0m\n",
      "\u001B[34mSuccessfully built fire\u001B[0m\n",
      "\u001B[35mInstalling collected packages: texttable, safetensors, Brotli, bitsandbytes, tokenize-rt, termcolor, scipy, sagemaker-jumpstart-script-utilities, sagemaker-jumpstart-huggingface-script-utilities, pyzstd, pytorch-triton, pyppmd, pycryptodomex, pybcj, pathspec, mypy-extensions, multivolumefile, loralib, inflate64, torch, py7zr, fire, black, transformers, accelerate, peft, datasets\u001B[0m\n",
      "\u001B[34mInstalling collected packages: texttable, safetensors, Brotli, bitsandbytes, tokenize-rt, termcolor, scipy, sagemaker-jumpstart-script-utilities, sagemaker-jumpstart-huggingface-script-utilities, pyzstd, pytorch-triton, pyppmd, pycryptodomex, pybcj, pathspec, mypy-extensions, multivolumefile, loralib, inflate64, torch, py7zr, fire, black, transformers, accelerate, peft, datasets\u001B[0m\n",
      "\u001B[34mAttempting uninstall: scipy\u001B[0m\n",
      "\u001B[34mFound existing installation: scipy 1.10.1\u001B[0m\n",
      "\u001B[34mUninstalling scipy-1.10.1:\u001B[0m\n",
      "\u001B[35mAttempting uninstall: scipy\u001B[0m\n",
      "\u001B[35mFound existing installation: scipy 1.10.1\u001B[0m\n",
      "\u001B[35mUninstalling scipy-1.10.1:\u001B[0m\n",
      "\u001B[35mSuccessfully uninstalled scipy-1.10.1\u001B[0m\n",
      "\u001B[34mSuccessfully uninstalled scipy-1.10.1\u001B[0m\n",
      "\u001B[35mAttempting uninstall: torch\u001B[0m\n",
      "\u001B[35mFound existing installation: torch 2.0.0\u001B[0m\n",
      "\u001B[35mUninstalling torch-2.0.0:\u001B[0m\n",
      "\u001B[34mAttempting uninstall: torch\u001B[0m\n",
      "\u001B[34mFound existing installation: torch 2.0.0\u001B[0m\n",
      "\u001B[34mUninstalling torch-2.0.0:\u001B[0m\n",
      "\u001B[35mSuccessfully uninstalled torch-2.0.0\u001B[0m\n",
      "\u001B[34mSuccessfully uninstalled torch-2.0.0\u001B[0m\n",
      "\u001B[35mAttempting uninstall: transformers\u001B[0m\n",
      "\u001B[35mFound existing installation: transformers 4.28.1\u001B[0m\n",
      "\u001B[35mUninstalling transformers-4.28.1:\u001B[0m\n",
      "\u001B[35mSuccessfully uninstalled transformers-4.28.1\u001B[0m\n",
      "\u001B[35mAttempting uninstall: accelerate\u001B[0m\n",
      "\u001B[35mFound existing installation: accelerate 0.19.0\u001B[0m\n",
      "\u001B[35mUninstalling accelerate-0.19.0:\u001B[0m\n",
      "\u001B[35mSuccessfully uninstalled accelerate-0.19.0\u001B[0m\n",
      "\u001B[34mAttempting uninstall: transformers\u001B[0m\n",
      "\u001B[34mFound existing installation: transformers 4.28.1\u001B[0m\n",
      "\u001B[34mUninstalling transformers-4.28.1:\u001B[0m\n",
      "\u001B[34mSuccessfully uninstalled transformers-4.28.1\u001B[0m\n",
      "\u001B[35mAttempting uninstall: datasets\u001B[0m\n",
      "\u001B[35mFound existing installation: datasets 2.12.0\u001B[0m\n",
      "\u001B[35mUninstalling datasets-2.12.0:\u001B[0m\n",
      "\u001B[35mSuccessfully uninstalled datasets-2.12.0\u001B[0m\n",
      "\u001B[35mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\u001B[0m\n",
      "\u001B[35mfastai 2.7.12 requires torch<2.1,>=1.7, but you have torch 2.2.0.dev20231104+cu118 which is incompatible.\u001B[0m\n",
      "\u001B[35mSuccessfully installed Brotli-1.0.9 accelerate-0.21.0 bitsandbytes-0.39.1 black-23.7.0 datasets-2.14.1 fire-0.5.0 inflate64-0.3.1 loralib-0.1.1 multivolumefile-0.2.3 mypy-extensions-1.0.0 pathspec-0.11.1 peft-0.4.0 py7zr-0.20.5 pybcj-1.0.1 pycryptodomex-3.18.0 pyppmd-1.0.0 pytorch-triton-2.1.0+6e4932cda8 pyzstd-0.15.9 safetensors-0.3.1 sagemaker-jumpstart-huggingface-script-utilities-1.1.3 sagemaker-jumpstart-script-utilities-1.1.9 scipy-1.11.1 termcolor-2.3.0 texttable-1.6.7 tokenize-rt-5.1.0 torch-2.2.0.dev20231104+cu118 transformers-4.31.0\u001B[0m\n",
      "\u001B[35mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001B[0m\n",
      "\u001B[35m2024-01-01 21:19:25,476 sagemaker-training-toolkit INFO     Waiting for the process to finish and give a return code.\u001B[0m\n",
      "\u001B[35m2024-01-01 21:19:25,476 sagemaker-training-toolkit INFO     Done waiting for a return code. Received 0 from exiting process.\u001B[0m\n",
      "\u001B[35m2024-01-01 21:19:25,556 sagemaker-training-toolkit INFO     No Neurons detected (normal if no neurons installed)\u001B[0m\n",
      "\u001B[35m2024-01-01 21:19:25,619 sagemaker-training-toolkit INFO     No Neurons detected (normal if no neurons installed)\u001B[0m\n",
      "\u001B[35m2024-01-01 21:19:25,683 sagemaker-training-toolkit INFO     No Neurons detected (normal if no neurons installed)\u001B[0m\n",
      "\u001B[35m2024-01-01 21:19:25,692 sagemaker-training-toolkit INFO     Invoking user script\u001B[0m\n",
      "\u001B[35mTraining Env:\u001B[0m\n",
      "\u001B[35m{\n",
      "    \"additional_framework_parameters\": {},\n",
      "    \"channel_input_dirs\": {\n",
      "        \"code\": \"/opt/ml/input/data/code\",\n",
      "        \"training\": \"/opt/ml/input/data/training\"\n",
      "    },\n",
      "    \"current_host\": \"algo-2\",\n",
      "    \"current_instance_group\": \"homogeneousCluster\",\n",
      "    \"current_instance_group_hosts\": [\n",
      "        \"algo-2\",\n",
      "        \"algo-1\"\n",
      "    ],\n",
      "    \"current_instance_type\": \"ml.g5.12xlarge\",\n",
      "    \"distribution_hosts\": [],\n",
      "    \"distribution_instance_groups\": [],\n",
      "    \"framework_module\": \"sagemaker_pytorch_container.training:main\",\n",
      "    \"hosts\": [\n",
      "        \"algo-1\",\n",
      "        \"algo-2\"\n",
      "    ],\n",
      "    \"hyperparameters\": {\n",
      "        \"add_input_output_demarcation_key\": \"True\",\n",
      "        \"chat_dataset\": \"False\",\n",
      "        \"enable_fsdp\": \"True\",\n",
      "        \"epoch\": \"5\",\n",
      "        \"instruction_tuned\": \"True\",\n",
      "        \"int8_quantization\": \"False\",\n",
      "        \"learning_rate\": \"0.0001\",\n",
      "        \"lora_alpha\": \"32\",\n",
      "        \"lora_dropout\": \"0.05\",\n",
      "        \"lora_r\": \"8\",\n",
      "        \"max_input_length\": \"1024\",\n",
      "        \"max_train_samples\": \"-1\",\n",
      "        \"max_val_samples\": \"-1\",\n",
      "        \"per_device_eval_batch_size\": \"1\",\n",
      "        \"per_device_train_batch_size\": \"4\",\n",
      "        \"preprocessing_num_workers\": \"None\",\n",
      "        \"seed\": \"10\",\n",
      "        \"train_data_split_seed\": \"0\",\n",
      "        \"validation_split_ratio\": \"0.2\"\n",
      "    },\n",
      "    \"input_config_dir\": \"/opt/ml/input/config\",\n",
      "    \"input_data_config\": {\n",
      "        \"code\": {\n",
      "            \"TrainingInputMode\": \"File\",\n",
      "            \"S3DistributionType\": \"FullyReplicated\",\n",
      "            \"RecordWrapperType\": \"None\"\n",
      "        },\n",
      "        \"training\": {\n",
      "            \"TrainingInputMode\": \"File\",\n",
      "            \"S3DistributionType\": \"FullyReplicated\",\n",
      "            \"RecordWrapperType\": \"None\"\n",
      "        }\n",
      "    },\n",
      "    \"input_dir\": \"/opt/ml/input\",\n",
      "    \"instance_groups\": [\n",
      "        \"homogeneousCluster\"\n",
      "    ],\n",
      "    \"instance_groups_dict\": {\n",
      "        \"homogeneousCluster\": {\n",
      "            \"instance_group_name\": \"homogeneousCluster\",\n",
      "            \"instance_type\": \"ml.g5.12xlarge\",\n",
      "            \"hosts\": [\n",
      "                \"algo-2\",\n",
      "                \"algo-1\"\n",
      "            ]\n",
      "        }\n",
      "    },\n",
      "    \"is_hetero\": false,\n",
      "    \"is_master\": false,\n",
      "    \"is_modelparallel_enabled\": null,\n",
      "    \"is_smddpmprun_installed\": true,\n",
      "    \"job_name\": \"meta-textgeneration-llama-2-7b-2024-01-01-21-05-52-057\",\n",
      "    \"log_level\": 20,\n",
      "    \"master_hostname\": \"algo-1\",\n",
      "    \"model_dir\": \"/opt/ml/model\",\n",
      "    \"module_dir\": \"/opt/ml/input/data/code/sourcedir.tar.gz\",\n",
      "    \"module_name\": \"transfer_learning\",\n",
      "    \"network_interface_name\": \"eth0\",\n",
      "    \"num_cpus\": 48,\n",
      "    \"num_gpus\": 4,\n",
      "    \"num_neurons\": 0,\n",
      "    \"output_data_dir\": \"/opt/ml/output/data\",\n",
      "    \"output_dir\": \"/opt/ml/output\",\n",
      "    \"output_intermediate_dir\": \"/opt/ml/output/intermediate\",\n",
      "    \"resource_config\": {\n",
      "        \"current_host\": \"algo-2\",\n",
      "        \"current_instance_type\": \"ml.g5.12xlarge\",\n",
      "        \"current_group_name\": \"homogeneousCluster\",\n",
      "        \"hosts\": [\n",
      "            \"algo-1\",\n",
      "            \"algo-2\"\n",
      "        ],\n",
      "        \"instance_groups\": [\n",
      "            {\n",
      "                \"instance_group_name\": \"homogeneousCluster\",\n",
      "                \"instance_type\": \"ml.g5.12xlarge\",\n",
      "                \"hosts\": [\n",
      "                    \"algo-2\",\n",
      "                    \"algo-1\"\n",
      "                ]\n",
      "            }\n",
      "        ],\n",
      "        \"network_interface_name\": \"eth0\"\n",
      "    },\n",
      "    \"user_entry_point\": \"transfer_learning.py\"\u001B[0m\n",
      "\u001B[35m}\u001B[0m\n",
      "\u001B[35mEnvironment variables:\u001B[0m\n",
      "\u001B[35mSM_HOSTS=[\"algo-1\",\"algo-2\"]\u001B[0m\n",
      "\u001B[35mSM_NETWORK_INTERFACE_NAME=eth0\u001B[0m\n",
      "\u001B[35mSM_HPS={\"add_input_output_demarcation_key\":\"True\",\"chat_dataset\":\"False\",\"enable_fsdp\":\"True\",\"epoch\":\"5\",\"instruction_tuned\":\"True\",\"int8_quantization\":\"False\",\"learning_rate\":\"0.0001\",\"lora_alpha\":\"32\",\"lora_dropout\":\"0.05\",\"lora_r\":\"8\",\"max_input_length\":\"1024\",\"max_train_samples\":\"-1\",\"max_val_samples\":\"-1\",\"per_device_eval_batch_size\":\"1\",\"per_device_train_batch_size\":\"4\",\"preprocessing_num_workers\":\"None\",\"seed\":\"10\",\"train_data_split_seed\":\"0\",\"validation_split_ratio\":\"0.2\"}\u001B[0m\n",
      "\u001B[35mSM_USER_ENTRY_POINT=transfer_learning.py\u001B[0m\n",
      "\u001B[35mSM_FRAMEWORK_PARAMS={}\u001B[0m\n",
      "\u001B[35mSM_RESOURCE_CONFIG={\"current_group_name\":\"homogeneousCluster\",\"current_host\":\"algo-2\",\"current_instance_type\":\"ml.g5.12xlarge\",\"hosts\":[\"algo-1\",\"algo-2\"],\"instance_groups\":[{\"hosts\":[\"algo-2\",\"algo-1\"],\"instance_group_name\":\"homogeneousCluster\",\"instance_type\":\"ml.g5.12xlarge\"}],\"network_interface_name\":\"eth0\"}\u001B[0m\n",
      "\u001B[35mSM_INPUT_DATA_CONFIG={\"code\":{\"RecordWrapperType\":\"None\",\"S3DistributionType\":\"FullyReplicated\",\"TrainingInputMode\":\"File\"},\"training\":{\"RecordWrapperType\":\"None\",\"S3DistributionType\":\"FullyReplicated\",\"TrainingInputMode\":\"File\"}}\u001B[0m\n",
      "\u001B[35mSM_OUTPUT_DATA_DIR=/opt/ml/output/data\u001B[0m\n",
      "\u001B[35mSM_CHANNELS=[\"code\",\"training\"]\u001B[0m\n",
      "\u001B[35mSM_CURRENT_HOST=algo-2\u001B[0m\n",
      "\u001B[35mSM_CURRENT_INSTANCE_TYPE=ml.g5.12xlarge\u001B[0m\n",
      "\u001B[35mSM_CURRENT_INSTANCE_GROUP=homogeneousCluster\u001B[0m\n",
      "\u001B[35mSM_CURRENT_INSTANCE_GROUP_HOSTS=[\"algo-2\",\"algo-1\"]\u001B[0m\n",
      "\u001B[35mSM_INSTANCE_GROUPS=[\"homogeneousCluster\"]\u001B[0m\n",
      "\u001B[35mSM_INSTANCE_GROUPS_DICT={\"homogeneousCluster\":{\"hosts\":[\"algo-2\",\"algo-1\"],\"instance_group_name\":\"homogeneousCluster\",\"instance_type\":\"ml.g5.12xlarge\"}}\u001B[0m\n",
      "\u001B[35mSM_DISTRIBUTION_INSTANCE_GROUPS=[]\u001B[0m\n",
      "\u001B[35mSM_IS_HETERO=false\u001B[0m\n",
      "\u001B[35mSM_MODULE_NAME=transfer_learning\u001B[0m\n",
      "\u001B[35mSM_LOG_LEVEL=20\u001B[0m\n",
      "\u001B[35mSM_FRAMEWORK_MODULE=sagemaker_pytorch_container.training:main\u001B[0m\n",
      "\u001B[35mSM_INPUT_DIR=/opt/ml/input\u001B[0m\n",
      "\u001B[35mSM_INPUT_CONFIG_DIR=/opt/ml/input/config\u001B[0m\n",
      "\u001B[35mSM_OUTPUT_DIR=/opt/ml/output\u001B[0m\n",
      "\u001B[35mSM_NUM_CPUS=48\u001B[0m\n",
      "\u001B[35mSM_NUM_GPUS=4\u001B[0m\n",
      "\u001B[35mSM_NUM_NEURONS=0\u001B[0m\n",
      "\u001B[35mSM_MODEL_DIR=/opt/ml/model\u001B[0m\n",
      "\u001B[35mSM_MODULE_DIR=/opt/ml/input/data/code/sourcedir.tar.gz\u001B[0m\n",
      "\u001B[35mSM_TRAINING_ENV={\"additional_framework_parameters\":{},\"channel_input_dirs\":{\"code\":\"/opt/ml/input/data/code\",\"training\":\"/opt/ml/input/data/training\"},\"current_host\":\"algo-2\",\"current_instance_group\":\"homogeneousCluster\",\"current_instance_group_hosts\":[\"algo-2\",\"algo-1\"],\"current_instance_type\":\"ml.g5.12xlarge\",\"distribution_hosts\":[],\"distribution_instance_groups\":[],\"framework_module\":\"sagemaker_pytorch_container.training:main\",\"hosts\":[\"algo-1\",\"algo-2\"],\"hyperparameters\":{\"add_input_output_demarcation_key\":\"True\",\"chat_dataset\":\"False\",\"enable_fsdp\":\"True\",\"epoch\":\"5\",\"instruction_tuned\":\"True\",\"int8_quantization\":\"False\",\"learning_rate\":\"0.0001\",\"lora_alpha\":\"32\",\"lora_dropout\":\"0.05\",\"lora_r\":\"8\",\"max_input_length\":\"1024\",\"max_train_samples\":\"-1\",\"max_val_samples\":\"-1\",\"per_device_eval_batch_size\":\"1\",\"per_device_train_batch_size\":\"4\",\"preprocessing_num_workers\":\"None\",\"seed\":\"10\",\"train_data_split_seed\":\"0\",\"validation_split_ratio\":\"0.2\"},\"input_config_dir\":\"/opt/ml/input/config\",\"input_data_config\":{\"code\":{\"RecordWrapperType\":\"None\",\"S3DistributionType\":\"FullyReplicated\",\"TrainingInputMode\":\"File\"},\"training\":{\"RecordWrapperType\":\"None\",\"S3DistributionType\":\"FullyReplicated\",\"TrainingInputMode\":\"File\"}},\"input_dir\":\"/opt/ml/input\",\"instance_groups\":[\"homogeneousCluster\"],\"instance_groups_dict\":{\"homogeneousCluster\":{\"hosts\":[\"algo-2\",\"algo-1\"],\"instance_group_name\":\"homogeneousCluster\",\"instance_type\":\"ml.g5.12xlarge\"}},\"is_hetero\":false,\"is_master\":false,\"is_modelparallel_enabled\":null,\"is_smddpmprun_installed\":true,\"job_name\":\"meta-textgeneration-llama-2-7b-2024-01-01-21-05-52-057\",\"log_level\":20,\"master_hostname\":\"algo-1\",\"model_dir\":\"/opt/ml/model\",\"module_dir\":\"/opt/ml/input/data/code/sourcedir.tar.gz\",\"module_name\":\"transfer_learning\",\"network_interface_name\":\"eth0\",\"num_cpus\":48,\"num_gpus\":4,\"num_neurons\":0,\"output_data_dir\":\"/opt/ml/output/data\",\"output_dir\":\"/opt/ml/output\",\"output_intermediate_dir\":\"/opt/ml/output/intermediate\",\"resource_config\":{\"current_group_name\":\"homogeneousCluster\",\"current_host\":\"algo-2\",\"current_instance_type\":\"ml.g5.12xlarge\",\"hosts\":[\"algo-1\",\"algo-2\"],\"instance_groups\":[{\"hosts\":[\"algo-2\",\"algo-1\"],\"instance_group_name\":\"homogeneousCluster\",\"instance_type\":\"ml.g5.12xlarge\"}],\"network_interface_name\":\"eth0\"},\"user_entry_point\":\"transfer_learning.py\"}\u001B[0m\n",
      "\u001B[35mSM_USER_ARGS=[\"--add_input_output_demarcation_key\",\"True\",\"--chat_dataset\",\"False\",\"--enable_fsdp\",\"True\",\"--epoch\",\"5\",\"--instruction_tuned\",\"True\",\"--int8_quantization\",\"False\",\"--learning_rate\",\"0.0001\",\"--lora_alpha\",\"32\",\"--lora_dropout\",\"0.05\",\"--lora_r\",\"8\",\"--max_input_length\",\"1024\",\"--max_train_samples\",\"-1\",\"--max_val_samples\",\"-1\",\"--per_device_eval_batch_size\",\"1\",\"--per_device_train_batch_size\",\"4\",\"--preprocessing_num_workers\",\"None\",\"--seed\",\"10\",\"--train_data_split_seed\",\"0\",\"--validation_split_ratio\",\"0.2\"]\u001B[0m\n",
      "\u001B[35mSM_OUTPUT_INTERMEDIATE_DIR=/opt/ml/output/intermediate\u001B[0m\n",
      "\u001B[35mSM_CHANNEL_CODE=/opt/ml/input/data/code\u001B[0m\n",
      "\u001B[35mSM_CHANNEL_TRAINING=/opt/ml/input/data/training\u001B[0m\n",
      "\u001B[35mSM_HP_ADD_INPUT_OUTPUT_DEMARCATION_KEY=True\u001B[0m\n",
      "\u001B[35mSM_HP_CHAT_DATASET=False\u001B[0m\n",
      "\u001B[35mSM_HP_ENABLE_FSDP=True\u001B[0m\n",
      "\u001B[35mSM_HP_EPOCH=5\u001B[0m\n",
      "\u001B[35mSM_HP_INSTRUCTION_TUNED=True\u001B[0m\n",
      "\u001B[35mSM_HP_INT8_QUANTIZATION=False\u001B[0m\n",
      "\u001B[35mSM_HP_LEARNING_RATE=0.0001\u001B[0m\n",
      "\u001B[35mSM_HP_LORA_ALPHA=32\u001B[0m\n",
      "\u001B[35mSM_HP_LORA_DROPOUT=0.05\u001B[0m\n",
      "\u001B[35mSM_HP_LORA_R=8\u001B[0m\n",
      "\u001B[35mSM_HP_MAX_INPUT_LENGTH=1024\u001B[0m\n",
      "\u001B[35mSM_HP_MAX_TRAIN_SAMPLES=-1\u001B[0m\n",
      "\u001B[35mSM_HP_MAX_VAL_SAMPLES=-1\u001B[0m\n",
      "\u001B[35mSM_HP_PER_DEVICE_EVAL_BATCH_SIZE=1\u001B[0m\n",
      "\u001B[35mSM_HP_PER_DEVICE_TRAIN_BATCH_SIZE=4\u001B[0m\n",
      "\u001B[35mSM_HP_PREPROCESSING_NUM_WORKERS=None\u001B[0m\n",
      "\u001B[35mSM_HP_SEED=10\u001B[0m\n",
      "\u001B[35mSM_HP_TRAIN_DATA_SPLIT_SEED=0\u001B[0m\n",
      "\u001B[35mSM_HP_VALIDATION_SPLIT_RATIO=0.2\u001B[0m\n",
      "\u001B[35mPYTHONPATH=/opt/ml/code:/opt/conda/bin:/opt/conda/lib/python310.zip:/opt/conda/lib/python3.10:/opt/conda/lib/python3.10/lib-dynload:/opt/conda/lib/python3.10/site-packages\u001B[0m\n",
      "\u001B[35mInvoking script with the following command:\u001B[0m\n",
      "\u001B[35m/opt/conda/bin/python3.10 transfer_learning.py --add_input_output_demarcation_key True --chat_dataset False --enable_fsdp True --epoch 5 --instruction_tuned True --int8_quantization False --learning_rate 0.0001 --lora_alpha 32 --lora_dropout 0.05 --lora_r 8 --max_input_length 1024 --max_train_samples -1 --max_val_samples -1 --per_device_eval_batch_size 1 --per_device_train_batch_size 4 --preprocessing_num_workers None --seed 10 --train_data_split_seed 0 --validation_split_ratio 0.2\u001B[0m\n",
      "\u001B[35m2024-01-01 21:19:25,720 sagemaker-training-toolkit INFO     Exceptions not imported for SageMaker TF as Tensorflow is not installed.\u001B[0m\n",
      "\u001B[34mAttempting uninstall: accelerate\u001B[0m\n",
      "\u001B[34mFound existing installation: accelerate 0.19.0\u001B[0m\n",
      "\u001B[34mUninstalling accelerate-0.19.0:\u001B[0m\n",
      "\u001B[34mSuccessfully uninstalled accelerate-0.19.0\u001B[0m\n",
      "\u001B[34mAttempting uninstall: datasets\u001B[0m\n",
      "\u001B[34mFound existing installation: datasets 2.12.0\u001B[0m\n",
      "\u001B[34mUninstalling datasets-2.12.0:\u001B[0m\n",
      "\u001B[34mSuccessfully uninstalled datasets-2.12.0\u001B[0m\n",
      "\u001B[34mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\u001B[0m\n",
      "\u001B[34mfastai 2.7.12 requires torch<2.1,>=1.7, but you have torch 2.2.0.dev20231104+cu118 which is incompatible.\u001B[0m\n",
      "\u001B[34mSuccessfully installed Brotli-1.0.9 accelerate-0.21.0 bitsandbytes-0.39.1 black-23.7.0 datasets-2.14.1 fire-0.5.0 inflate64-0.3.1 loralib-0.1.1 multivolumefile-0.2.3 mypy-extensions-1.0.0 pathspec-0.11.1 peft-0.4.0 py7zr-0.20.5 pybcj-1.0.1 pycryptodomex-3.18.0 pyppmd-1.0.0 pytorch-triton-2.1.0+6e4932cda8 pyzstd-0.15.9 safetensors-0.3.1 sagemaker-jumpstart-huggingface-script-utilities-1.1.3 sagemaker-jumpstart-script-utilities-1.1.9 scipy-1.11.1 termcolor-2.3.0 texttable-1.6.7 tokenize-rt-5.1.0 torch-2.2.0.dev20231104+cu118 transformers-4.31.0\u001B[0m\n",
      "\u001B[34mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001B[0m\n",
      "\u001B[34m2024-01-01 21:19:28,216 sagemaker-training-toolkit INFO     Waiting for the process to finish and give a return code.\u001B[0m\n",
      "\u001B[34m2024-01-01 21:19:28,216 sagemaker-training-toolkit INFO     Done waiting for a return code. Received 0 from exiting process.\u001B[0m\n",
      "\u001B[35m===================================BUG REPORT===================================\u001B[0m\n",
      "\u001B[35mWelcome to bitsandbytes. For bug reports, please run\u001B[0m\n",
      "\u001B[35mpython -m bitsandbytes\n",
      " and submit this information together with your error trace to: https://github.com/TimDettmers/bitsandbytes/issues\u001B[0m\n",
      "\u001B[35m================================================================================\u001B[0m\n",
      "\u001B[35mbin /opt/conda/lib/python3.10/site-packages/bitsandbytes/libbitsandbytes_cuda118.so\u001B[0m\n",
      "\u001B[35m/opt/conda/lib/python3.10/site-packages/bitsandbytes/cuda_setup/main.py:149: UserWarning: WARNING: The following directories listed in your path were found to be non-existent: {PosixPath('/usr/local/nvidia/lib'), PosixPath('/usr/local/nvidia/lib64')}\n",
      "  warn(msg)\u001B[0m\n",
      "\u001B[35mCUDA SETUP: CUDA runtime path found: /opt/conda/lib/libcudart.so\u001B[0m\n",
      "\u001B[35mCUDA SETUP: Highest compute capability among GPUs detected: 8.6\u001B[0m\n",
      "\u001B[35mCUDA SETUP: Detected CUDA version 118\u001B[0m\n",
      "\u001B[35mCUDA SETUP: Loading binary /opt/conda/lib/python3.10/site-packages/bitsandbytes/libbitsandbytes_cuda118.so...\u001B[0m\n",
      "\u001B[34m2024-01-01 21:19:28,290 sagemaker-training-toolkit INFO     No Neurons detected (normal if no neurons installed)\u001B[0m\n",
      "\u001B[34m2024-01-01 21:19:28,354 sagemaker-training-toolkit INFO     No Neurons detected (normal if no neurons installed)\u001B[0m\n",
      "\u001B[34m2024-01-01 21:19:28,417 sagemaker-training-toolkit INFO     No Neurons detected (normal if no neurons installed)\u001B[0m\n",
      "\u001B[34m2024-01-01 21:19:28,427 sagemaker-training-toolkit INFO     Invoking user script\u001B[0m\n",
      "\u001B[34mTraining Env:\u001B[0m\n",
      "\u001B[34m{\n",
      "    \"additional_framework_parameters\": {},\n",
      "    \"channel_input_dirs\": {\n",
      "        \"code\": \"/opt/ml/input/data/code\",\n",
      "        \"training\": \"/opt/ml/input/data/training\"\n",
      "    },\n",
      "    \"current_host\": \"algo-1\",\n",
      "    \"current_instance_group\": \"homogeneousCluster\",\n",
      "    \"current_instance_group_hosts\": [\n",
      "        \"algo-2\",\n",
      "        \"algo-1\"\n",
      "    ],\n",
      "    \"current_instance_type\": \"ml.g5.12xlarge\",\n",
      "    \"distribution_hosts\": [],\n",
      "    \"distribution_instance_groups\": [],\n",
      "    \"framework_module\": \"sagemaker_pytorch_container.training:main\",\n",
      "    \"hosts\": [\n",
      "        \"algo-1\",\n",
      "        \"algo-2\"\n",
      "    ],\n",
      "    \"hyperparameters\": {\n",
      "        \"add_input_output_demarcation_key\": \"True\",\n",
      "        \"chat_dataset\": \"False\",\n",
      "        \"enable_fsdp\": \"True\",\n",
      "        \"epoch\": \"5\",\n",
      "        \"instruction_tuned\": \"True\",\n",
      "        \"int8_quantization\": \"False\",\n",
      "        \"learning_rate\": \"0.0001\",\n",
      "        \"lora_alpha\": \"32\",\n",
      "        \"lora_dropout\": \"0.05\",\n",
      "        \"lora_r\": \"8\",\n",
      "        \"max_input_length\": \"1024\",\n",
      "        \"max_train_samples\": \"-1\",\n",
      "        \"max_val_samples\": \"-1\",\n",
      "        \"per_device_eval_batch_size\": \"1\",\n",
      "        \"per_device_train_batch_size\": \"4\",\n",
      "        \"preprocessing_num_workers\": \"None\",\n",
      "        \"seed\": \"10\",\n",
      "        \"train_data_split_seed\": \"0\",\n",
      "        \"validation_split_ratio\": \"0.2\"\n",
      "    },\n",
      "    \"input_config_dir\": \"/opt/ml/input/config\",\n",
      "    \"input_data_config\": {\n",
      "        \"code\": {\n",
      "            \"TrainingInputMode\": \"File\",\n",
      "            \"S3DistributionType\": \"FullyReplicated\",\n",
      "            \"RecordWrapperType\": \"None\"\n",
      "        },\n",
      "        \"training\": {\n",
      "            \"TrainingInputMode\": \"File\",\n",
      "            \"S3DistributionType\": \"FullyReplicated\",\n",
      "            \"RecordWrapperType\": \"None\"\n",
      "        }\n",
      "    },\n",
      "    \"input_dir\": \"/opt/ml/input\",\n",
      "    \"instance_groups\": [\n",
      "        \"homogeneousCluster\"\n",
      "    ],\n",
      "    \"instance_groups_dict\": {\n",
      "        \"homogeneousCluster\": {\n",
      "            \"instance_group_name\": \"homogeneousCluster\",\n",
      "            \"instance_type\": \"ml.g5.12xlarge\",\n",
      "            \"hosts\": [\n",
      "                \"algo-2\",\n",
      "                \"algo-1\"\n",
      "            ]\n",
      "        }\n",
      "    },\n",
      "    \"is_hetero\": false,\n",
      "    \"is_master\": true,\n",
      "    \"is_modelparallel_enabled\": null,\n",
      "    \"is_smddpmprun_installed\": true,\n",
      "    \"job_name\": \"meta-textgeneration-llama-2-7b-2024-01-01-21-05-52-057\",\n",
      "    \"log_level\": 20,\n",
      "    \"master_hostname\": \"algo-1\",\n",
      "    \"model_dir\": \"/opt/ml/model\",\n",
      "    \"module_dir\": \"/opt/ml/input/data/code/sourcedir.tar.gz\",\n",
      "    \"module_name\": \"transfer_learning\",\n",
      "    \"network_interface_name\": \"eth0\",\n",
      "    \"num_cpus\": 48,\n",
      "    \"num_gpus\": 4,\n",
      "    \"num_neurons\": 0,\n",
      "    \"output_data_dir\": \"/opt/ml/output/data\",\n",
      "    \"output_dir\": \"/opt/ml/output\",\n",
      "    \"output_intermediate_dir\": \"/opt/ml/output/intermediate\",\n",
      "    \"resource_config\": {\n",
      "        \"current_host\": \"algo-1\",\n",
      "        \"current_instance_type\": \"ml.g5.12xlarge\",\n",
      "        \"current_group_name\": \"homogeneousCluster\",\n",
      "        \"hosts\": [\n",
      "            \"algo-1\",\n",
      "            \"algo-2\"\n",
      "        ],\n",
      "        \"instance_groups\": [\n",
      "            {\n",
      "                \"instance_group_name\": \"homogeneousCluster\",\n",
      "                \"instance_type\": \"ml.g5.12xlarge\",\n",
      "                \"hosts\": [\n",
      "                    \"algo-2\",\n",
      "                    \"algo-1\"\n",
      "                ]\n",
      "            }\n",
      "        ],\n",
      "        \"network_interface_name\": \"eth0\"\n",
      "    },\n",
      "    \"user_entry_point\": \"transfer_learning.py\"\u001B[0m\n",
      "\u001B[34m}\u001B[0m\n",
      "\u001B[34mEnvironment variables:\u001B[0m\n",
      "\u001B[34mSM_HOSTS=[\"algo-1\",\"algo-2\"]\u001B[0m\n",
      "\u001B[34mSM_NETWORK_INTERFACE_NAME=eth0\u001B[0m\n",
      "\u001B[34mSM_HPS={\"add_input_output_demarcation_key\":\"True\",\"chat_dataset\":\"False\",\"enable_fsdp\":\"True\",\"epoch\":\"5\",\"instruction_tuned\":\"True\",\"int8_quantization\":\"False\",\"learning_rate\":\"0.0001\",\"lora_alpha\":\"32\",\"lora_dropout\":\"0.05\",\"lora_r\":\"8\",\"max_input_length\":\"1024\",\"max_train_samples\":\"-1\",\"max_val_samples\":\"-1\",\"per_device_eval_batch_size\":\"1\",\"per_device_train_batch_size\":\"4\",\"preprocessing_num_workers\":\"None\",\"seed\":\"10\",\"train_data_split_seed\":\"0\",\"validation_split_ratio\":\"0.2\"}\u001B[0m\n",
      "\u001B[34mSM_USER_ENTRY_POINT=transfer_learning.py\u001B[0m\n",
      "\u001B[34mSM_FRAMEWORK_PARAMS={}\u001B[0m\n",
      "\u001B[34mSM_RESOURCE_CONFIG={\"current_group_name\":\"homogeneousCluster\",\"current_host\":\"algo-1\",\"current_instance_type\":\"ml.g5.12xlarge\",\"hosts\":[\"algo-1\",\"algo-2\"],\"instance_groups\":[{\"hosts\":[\"algo-2\",\"algo-1\"],\"instance_group_name\":\"homogeneousCluster\",\"instance_type\":\"ml.g5.12xlarge\"}],\"network_interface_name\":\"eth0\"}\u001B[0m\n",
      "\u001B[34mSM_INPUT_DATA_CONFIG={\"code\":{\"RecordWrapperType\":\"None\",\"S3DistributionType\":\"FullyReplicated\",\"TrainingInputMode\":\"File\"},\"training\":{\"RecordWrapperType\":\"None\",\"S3DistributionType\":\"FullyReplicated\",\"TrainingInputMode\":\"File\"}}\u001B[0m\n",
      "\u001B[34mSM_OUTPUT_DATA_DIR=/opt/ml/output/data\u001B[0m\n",
      "\u001B[34mSM_CHANNELS=[\"code\",\"training\"]\u001B[0m\n",
      "\u001B[34mSM_CURRENT_HOST=algo-1\u001B[0m\n",
      "\u001B[34mSM_CURRENT_INSTANCE_TYPE=ml.g5.12xlarge\u001B[0m\n",
      "\u001B[34mSM_CURRENT_INSTANCE_GROUP=homogeneousCluster\u001B[0m\n",
      "\u001B[34mSM_CURRENT_INSTANCE_GROUP_HOSTS=[\"algo-2\",\"algo-1\"]\u001B[0m\n",
      "\u001B[34mSM_INSTANCE_GROUPS=[\"homogeneousCluster\"]\u001B[0m\n",
      "\u001B[34mSM_INSTANCE_GROUPS_DICT={\"homogeneousCluster\":{\"hosts\":[\"algo-2\",\"algo-1\"],\"instance_group_name\":\"homogeneousCluster\",\"instance_type\":\"ml.g5.12xlarge\"}}\u001B[0m\n",
      "\u001B[34mSM_DISTRIBUTION_INSTANCE_GROUPS=[]\u001B[0m\n",
      "\u001B[34mSM_IS_HETERO=false\u001B[0m\n",
      "\u001B[34mSM_MODULE_NAME=transfer_learning\u001B[0m\n",
      "\u001B[34mSM_LOG_LEVEL=20\u001B[0m\n",
      "\u001B[34mSM_FRAMEWORK_MODULE=sagemaker_pytorch_container.training:main\u001B[0m\n",
      "\u001B[34mSM_INPUT_DIR=/opt/ml/input\u001B[0m\n",
      "\u001B[34mSM_INPUT_CONFIG_DIR=/opt/ml/input/config\u001B[0m\n",
      "\u001B[34mSM_OUTPUT_DIR=/opt/ml/output\u001B[0m\n",
      "\u001B[34mSM_NUM_CPUS=48\u001B[0m\n",
      "\u001B[34mSM_NUM_GPUS=4\u001B[0m\n",
      "\u001B[34mSM_NUM_NEURONS=0\u001B[0m\n",
      "\u001B[34mSM_MODEL_DIR=/opt/ml/model\u001B[0m\n",
      "\u001B[34mSM_MODULE_DIR=/opt/ml/input/data/code/sourcedir.tar.gz\u001B[0m\n",
      "\u001B[34mSM_TRAINING_ENV={\"additional_framework_parameters\":{},\"channel_input_dirs\":{\"code\":\"/opt/ml/input/data/code\",\"training\":\"/opt/ml/input/data/training\"},\"current_host\":\"algo-1\",\"current_instance_group\":\"homogeneousCluster\",\"current_instance_group_hosts\":[\"algo-2\",\"algo-1\"],\"current_instance_type\":\"ml.g5.12xlarge\",\"distribution_hosts\":[],\"distribution_instance_groups\":[],\"framework_module\":\"sagemaker_pytorch_container.training:main\",\"hosts\":[\"algo-1\",\"algo-2\"],\"hyperparameters\":{\"add_input_output_demarcation_key\":\"True\",\"chat_dataset\":\"False\",\"enable_fsdp\":\"True\",\"epoch\":\"5\",\"instruction_tuned\":\"True\",\"int8_quantization\":\"False\",\"learning_rate\":\"0.0001\",\"lora_alpha\":\"32\",\"lora_dropout\":\"0.05\",\"lora_r\":\"8\",\"max_input_length\":\"1024\",\"max_train_samples\":\"-1\",\"max_val_samples\":\"-1\",\"per_device_eval_batch_size\":\"1\",\"per_device_train_batch_size\":\"4\",\"preprocessing_num_workers\":\"None\",\"seed\":\"10\",\"train_data_split_seed\":\"0\",\"validation_split_ratio\":\"0.2\"},\"input_config_dir\":\"/opt/ml/input/config\",\"input_data_config\":{\"code\":{\"RecordWrapperType\":\"None\",\"S3DistributionType\":\"FullyReplicated\",\"TrainingInputMode\":\"File\"},\"training\":{\"RecordWrapperType\":\"None\",\"S3DistributionType\":\"FullyReplicated\",\"TrainingInputMode\":\"File\"}},\"input_dir\":\"/opt/ml/input\",\"instance_groups\":[\"homogeneousCluster\"],\"instance_groups_dict\":{\"homogeneousCluster\":{\"hosts\":[\"algo-2\",\"algo-1\"],\"instance_group_name\":\"homogeneousCluster\",\"instance_type\":\"ml.g5.12xlarge\"}},\"is_hetero\":false,\"is_master\":true,\"is_modelparallel_enabled\":null,\"is_smddpmprun_installed\":true,\"job_name\":\"meta-textgeneration-llama-2-7b-2024-01-01-21-05-52-057\",\"log_level\":20,\"master_hostname\":\"algo-1\",\"model_dir\":\"/opt/ml/model\",\"module_dir\":\"/opt/ml/input/data/code/sourcedir.tar.gz\",\"module_name\":\"transfer_learning\",\"network_interface_name\":\"eth0\",\"num_cpus\":48,\"num_gpus\":4,\"num_neurons\":0,\"output_data_dir\":\"/opt/ml/output/data\",\"output_dir\":\"/opt/ml/output\",\"output_intermediate_dir\":\"/opt/ml/output/intermediate\",\"resource_config\":{\"current_group_name\":\"homogeneousCluster\",\"current_host\":\"algo-1\",\"current_instance_type\":\"ml.g5.12xlarge\",\"hosts\":[\"algo-1\",\"algo-2\"],\"instance_groups\":[{\"hosts\":[\"algo-2\",\"algo-1\"],\"instance_group_name\":\"homogeneousCluster\",\"instance_type\":\"ml.g5.12xlarge\"}],\"network_interface_name\":\"eth0\"},\"user_entry_point\":\"transfer_learning.py\"}\u001B[0m\n",
      "\u001B[34mSM_USER_ARGS=[\"--add_input_output_demarcation_key\",\"True\",\"--chat_dataset\",\"False\",\"--enable_fsdp\",\"True\",\"--epoch\",\"5\",\"--instruction_tuned\",\"True\",\"--int8_quantization\",\"False\",\"--learning_rate\",\"0.0001\",\"--lora_alpha\",\"32\",\"--lora_dropout\",\"0.05\",\"--lora_r\",\"8\",\"--max_input_length\",\"1024\",\"--max_train_samples\",\"-1\",\"--max_val_samples\",\"-1\",\"--per_device_eval_batch_size\",\"1\",\"--per_device_train_batch_size\",\"4\",\"--preprocessing_num_workers\",\"None\",\"--seed\",\"10\",\"--train_data_split_seed\",\"0\",\"--validation_split_ratio\",\"0.2\"]\u001B[0m\n",
      "\u001B[34mSM_OUTPUT_INTERMEDIATE_DIR=/opt/ml/output/intermediate\u001B[0m\n",
      "\u001B[34mSM_CHANNEL_CODE=/opt/ml/input/data/code\u001B[0m\n",
      "\u001B[34mSM_CHANNEL_TRAINING=/opt/ml/input/data/training\u001B[0m\n",
      "\u001B[34mSM_HP_ADD_INPUT_OUTPUT_DEMARCATION_KEY=True\u001B[0m\n",
      "\u001B[34mSM_HP_CHAT_DATASET=False\u001B[0m\n",
      "\u001B[34mSM_HP_ENABLE_FSDP=True\u001B[0m\n",
      "\u001B[34mSM_HP_EPOCH=5\u001B[0m\n",
      "\u001B[34mSM_HP_INSTRUCTION_TUNED=True\u001B[0m\n",
      "\u001B[34mSM_HP_INT8_QUANTIZATION=False\u001B[0m\n",
      "\u001B[34mSM_HP_LEARNING_RATE=0.0001\u001B[0m\n",
      "\u001B[34mSM_HP_LORA_ALPHA=32\u001B[0m\n",
      "\u001B[34mSM_HP_LORA_DROPOUT=0.05\u001B[0m\n",
      "\u001B[34mSM_HP_LORA_R=8\u001B[0m\n",
      "\u001B[34mSM_HP_MAX_INPUT_LENGTH=1024\u001B[0m\n",
      "\u001B[34mSM_HP_MAX_TRAIN_SAMPLES=-1\u001B[0m\n",
      "\u001B[34mSM_HP_MAX_VAL_SAMPLES=-1\u001B[0m\n",
      "\u001B[34mSM_HP_PER_DEVICE_EVAL_BATCH_SIZE=1\u001B[0m\n",
      "\u001B[34mSM_HP_PER_DEVICE_TRAIN_BATCH_SIZE=4\u001B[0m\n",
      "\u001B[34mSM_HP_PREPROCESSING_NUM_WORKERS=None\u001B[0m\n",
      "\u001B[34mSM_HP_SEED=10\u001B[0m\n",
      "\u001B[34mSM_HP_TRAIN_DATA_SPLIT_SEED=0\u001B[0m\n",
      "\u001B[34mSM_HP_VALIDATION_SPLIT_RATIO=0.2\u001B[0m\n",
      "\u001B[34mPYTHONPATH=/opt/ml/code:/opt/conda/bin:/opt/conda/lib/python310.zip:/opt/conda/lib/python3.10:/opt/conda/lib/python3.10/lib-dynload:/opt/conda/lib/python3.10/site-packages\u001B[0m\n",
      "\u001B[34mInvoking script with the following command:\u001B[0m\n",
      "\u001B[34m/opt/conda/bin/python3.10 transfer_learning.py --add_input_output_demarcation_key True --chat_dataset False --enable_fsdp True --epoch 5 --instruction_tuned True --int8_quantization False --learning_rate 0.0001 --lora_alpha 32 --lora_dropout 0.05 --lora_r 8 --max_input_length 1024 --max_train_samples -1 --max_val_samples -1 --per_device_eval_batch_size 1 --per_device_train_batch_size 4 --preprocessing_num_workers None --seed 10 --train_data_split_seed 0 --validation_split_ratio 0.2\u001B[0m\n",
      "\u001B[34m2024-01-01 21:19:28,454 sagemaker-training-toolkit INFO     Exceptions not imported for SageMaker TF as Tensorflow is not installed.\u001B[0m\n",
      "\u001B[35mINFO:root:Using pre-trained artifacts in SAGEMAKER_ADDITIONAL_S3_DATA_PATH=/opt/ml/additonals3data\u001B[0m\n",
      "\u001B[35mINFO:root:Identify file serving.properties in the un-tar directory /opt/ml/additonals3data. Copying it over to /opt/ml/model for model deployment after training is finished.\u001B[0m\n",
      "\u001B[35mINFO:root:Invoking the training command ['torchrun', '--nnodes', '1', '--nproc_per_node', '4', 'llama_finetuning.py', '--model_name', '/opt/ml/additonals3data', '--num_gpus', '4', '--pure_bf16', '--dist_checkpoint_root_folder', 'model_checkpoints', '--dist_checkpoint_folder', 'fine-tuned', '--batch_size_training', '4', '--micro_batch_size', '4', '--train_file', '/opt/ml/input/data/training', '--lr', '0.0001', '--do_train', '--output_dir', 'saved_peft_model', '--num_epochs', '5', '--use_peft', '--peft_method', 'lora', '--max_train_samples', '-1', '--max_val_samples', '-1', '--seed', '10', '--per_device_eval_batch_size', '1', '--max_input_length', '1024', '--preprocessing_num_workers', '--None', '--validation_split_ratio', '0.2', '--train_data_split_seed', '0', '--num_workers_dataloader', '0', '--weight_decay', '0.1', '--lora_r', '8', '--lora_alpha', '32', '--lora_dropout', '0.05', '--enable_fsdp', '--add_input_output_demarcation_key', '--instruction_tuned'].\u001B[0m\n",
      "\u001B[34m===================================BUG REPORT===================================\u001B[0m\n",
      "\u001B[34mWelcome to bitsandbytes. For bug reports, please run\u001B[0m\n",
      "\u001B[34mpython -m bitsandbytes\n",
      " and submit this information together with your error trace to: https://github.com/TimDettmers/bitsandbytes/issues\u001B[0m\n",
      "\u001B[34m================================================================================\u001B[0m\n",
      "\u001B[34mbin /opt/conda/lib/python3.10/site-packages/bitsandbytes/libbitsandbytes_cuda118.so\u001B[0m\n",
      "\u001B[34m/opt/conda/lib/python3.10/site-packages/bitsandbytes/cuda_setup/main.py:149: UserWarning: WARNING: The following directories listed in your path were found to be non-existent: {PosixPath('/usr/local/nvidia/lib'), PosixPath('/usr/local/nvidia/lib64')}\n",
      "  warn(msg)\u001B[0m\n",
      "\u001B[34mCUDA SETUP: CUDA runtime path found: /opt/conda/lib/libcudart.so\u001B[0m\n",
      "\u001B[34mCUDA SETUP: Highest compute capability among GPUs detected: 8.6\u001B[0m\n",
      "\u001B[34mCUDA SETUP: Detected CUDA version 118\u001B[0m\n",
      "\u001B[34mCUDA SETUP: Loading binary /opt/conda/lib/python3.10/site-packages/bitsandbytes/libbitsandbytes_cuda118.so...\u001B[0m\n",
      "\u001B[35m[2024-01-01 21:19:30,976] torch.distributed.run: [WARNING] \u001B[0m\n",
      "\u001B[35m[2024-01-01 21:19:30,976] torch.distributed.run: [WARNING] *****************************************\u001B[0m\n",
      "\u001B[35m[2024-01-01 21:19:30,976] torch.distributed.run: [WARNING] Setting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed. \u001B[0m\n",
      "\u001B[35m[2024-01-01 21:19:30,976] torch.distributed.run: [WARNING] *****************************************\u001B[0m\n",
      "\u001B[34mINFO:root:Using pre-trained artifacts in SAGEMAKER_ADDITIONAL_S3_DATA_PATH=/opt/ml/additonals3data\u001B[0m\n",
      "\u001B[34mINFO:root:Identify file serving.properties in the un-tar directory /opt/ml/additonals3data. Copying it over to /opt/ml/model for model deployment after training is finished.\u001B[0m\n",
      "\u001B[34mINFO:root:Invoking the training command ['torchrun', '--nnodes', '1', '--nproc_per_node', '4', 'llama_finetuning.py', '--model_name', '/opt/ml/additonals3data', '--num_gpus', '4', '--pure_bf16', '--dist_checkpoint_root_folder', 'model_checkpoints', '--dist_checkpoint_folder', 'fine-tuned', '--batch_size_training', '4', '--micro_batch_size', '4', '--train_file', '/opt/ml/input/data/training', '--lr', '0.0001', '--do_train', '--output_dir', 'saved_peft_model', '--num_epochs', '5', '--use_peft', '--peft_method', 'lora', '--max_train_samples', '-1', '--max_val_samples', '-1', '--seed', '10', '--per_device_eval_batch_size', '1', '--max_input_length', '1024', '--preprocessing_num_workers', '--None', '--validation_split_ratio', '0.2', '--train_data_split_seed', '0', '--num_workers_dataloader', '0', '--weight_decay', '0.1', '--lora_r', '8', '--lora_alpha', '32', '--lora_dropout', '0.05', '--enable_fsdp', '--add_input_output_demarcation_key', '--instruction_tuned'].\u001B[0m\n",
      "\u001B[35m===================================BUG REPORT===================================\u001B[0m\n",
      "\u001B[35mWelcome to bitsandbytes. For bug reports, please run\u001B[0m\n",
      "\u001B[35mpython -m bitsandbytes\n",
      " and submit this information together with your error trace to: https://github.com/TimDettmers/bitsandbytes/issues\u001B[0m\n",
      "\u001B[35m================================================================================\u001B[0m\n",
      "\u001B[35m===================================BUG REPORT===================================\u001B[0m\n",
      "\u001B[35mWelcome to bitsandbytes. For bug reports, please run\u001B[0m\n",
      "\u001B[35mpython -m bitsandbytes\n",
      " and submit this information together with your error trace to: https://github.com/TimDettmers/bitsandbytes/issues\u001B[0m\n",
      "\u001B[35m================================================================================\u001B[0m\n",
      "\u001B[35mbin /opt/conda/lib/python3.10/site-packages/bitsandbytes/libbitsandbytes_cuda118.so\u001B[0m\n",
      "\u001B[35m/opt/conda/lib/python3.10/site-packages/bitsandbytes/cuda_setup/main.py:149: UserWarning: WARNING: The following directories listed in your path were found to be non-existent: {PosixPath('/usr/local/nvidia/lib64'), PosixPath('/usr/local/nvidia/lib')}\n",
      "  warn(msg)\u001B[0m\n",
      "\u001B[35mCUDA SETUP: CUDA runtime path found: /opt/conda/lib/libcudart.so\u001B[0m\n",
      "\u001B[35mCUDA SETUP: Highest compute capability among GPUs detected: 8.6\u001B[0m\n",
      "\u001B[35mCUDA SETUP: Detected CUDA version 118\u001B[0m\n",
      "\u001B[35mCUDA SETUP: Loading binary /opt/conda/lib/python3.10/site-packages/bitsandbytes/libbitsandbytes_cuda118.so...\u001B[0m\n",
      "\u001B[35mbin /opt/conda/lib/python3.10/site-packages/bitsandbytes/libbitsandbytes_cuda118.so\u001B[0m\n",
      "\u001B[35m/opt/conda/lib/python3.10/site-packages/bitsandbytes/cuda_setup/main.py:149: UserWarning: WARNING: The following directories listed in your path were found to be non-existent: {PosixPath('/usr/local/nvidia/lib'), PosixPath('/usr/local/nvidia/lib64')}\n",
      "  warn(msg)\u001B[0m\n",
      "\u001B[35mCUDA SETUP: CUDA runtime path found: /opt/conda/lib/libcudart.so\u001B[0m\n",
      "\u001B[35mCUDA SETUP: Highest compute capability among GPUs detected: 8.6\u001B[0m\n",
      "\u001B[35mCUDA SETUP: Detected CUDA version 118\u001B[0m\n",
      "\u001B[35mCUDA SETUP: Loading binary /opt/conda/lib/python3.10/site-packages/bitsandbytes/libbitsandbytes_cuda118.so...\u001B[0m\n",
      "\u001B[35m===================================BUG REPORT===================================\u001B[0m\n",
      "\u001B[35mWelcome to bitsandbytes. For bug reports, please run\u001B[0m\n",
      "\u001B[35mpython -m bitsandbytes\n",
      " and submit this information together with your error trace to: https://github.com/TimDettmers/bitsandbytes/issues\u001B[0m\n",
      "\u001B[35m================================================================================\u001B[0m\n",
      "\u001B[34m[2024-01-01 21:19:33,721] torch.distributed.run: [WARNING] \u001B[0m\n",
      "\u001B[34m[2024-01-01 21:19:33,721] torch.distributed.run: [WARNING] *****************************************\u001B[0m\n",
      "\u001B[34m[2024-01-01 21:19:33,721] torch.distributed.run: [WARNING] Setting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed. \u001B[0m\n",
      "\u001B[34m[2024-01-01 21:19:33,721] torch.distributed.run: [WARNING] *****************************************\u001B[0m\n",
      "\u001B[35m===================================BUG REPORT===================================\u001B[0m\n",
      "\u001B[35mWelcome to bitsandbytes. For bug reports, please run\u001B[0m\n",
      "\u001B[35mpython -m bitsandbytes\n",
      " and submit this information together with your error trace to: https://github.com/TimDettmers/bitsandbytes/issues\u001B[0m\n",
      "\u001B[35m================================================================================\u001B[0m\n",
      "\u001B[35mbin /opt/conda/lib/python3.10/site-packages/bitsandbytes/libbitsandbytes_cuda118.so\u001B[0m\n",
      "\u001B[35m/opt/conda/lib/python3.10/site-packages/bitsandbytes/cuda_setup/main.py:149: UserWarning: WARNING: The following directories listed in your path were found to be non-existent: {PosixPath('/usr/local/nvidia/lib64'), PosixPath('/usr/local/nvidia/lib')}\n",
      "  warn(msg)\u001B[0m\n",
      "\u001B[35mCUDA SETUP: CUDA runtime path found: /opt/conda/lib/libcudart.so.11.0\u001B[0m\n",
      "\u001B[35mCUDA SETUP: Highest compute capability among GPUs detected: 8.6\u001B[0m\n",
      "\u001B[35mCUDA SETUP: Detected CUDA version 118\u001B[0m\n",
      "\u001B[35mCUDA SETUP: Loading binary /opt/conda/lib/python3.10/site-packages/bitsandbytes/libbitsandbytes_cuda118.so...\u001B[0m\n",
      "\u001B[35mbin /opt/conda/lib/python3.10/site-packages/bitsandbytes/libbitsandbytes_cuda118.so\u001B[0m\n",
      "\u001B[35m/opt/conda/lib/python3.10/site-packages/bitsandbytes/cuda_setup/main.py:149: UserWarning: WARNING: The following directories listed in your path were found to be non-existent: {PosixPath('/usr/local/nvidia/lib64'), PosixPath('/usr/local/nvidia/lib')}\n",
      "  warn(msg)\u001B[0m\n",
      "\u001B[35mCUDA SETUP: CUDA runtime path found: /opt/conda/lib/libcudart.so\u001B[0m\n",
      "\u001B[35mCUDA SETUP: Highest compute capability among GPUs detected: 8.6\u001B[0m\n",
      "\u001B[35mCUDA SETUP: Detected CUDA version 118\u001B[0m\n",
      "\u001B[35mCUDA SETUP: Loading binary /opt/conda/lib/python3.10/site-packages/bitsandbytes/libbitsandbytes_cuda118.so...\u001B[0m\n",
      "\u001B[34m===================================BUG REPORT===================================\u001B[0m\n",
      "\u001B[34mWelcome to bitsandbytes. For bug reports, please run\u001B[0m\n",
      "\u001B[34mpython -m bitsandbytes\n",
      " and submit this information together with your error trace to: https://github.com/TimDettmers/bitsandbytes/issues\u001B[0m\n",
      "\u001B[34m================================================================================\u001B[0m\n",
      "\u001B[34mbin /opt/conda/lib/python3.10/site-packages/bitsandbytes/libbitsandbytes_cuda118.so\u001B[0m\n",
      "\u001B[34m/opt/conda/lib/python3.10/site-packages/bitsandbytes/cuda_setup/main.py:149: UserWarning: WARNING: The following directories listed in your path were found to be non-existent: {PosixPath('/usr/local/nvidia/lib64'), PosixPath('/usr/local/nvidia/lib')}\n",
      "  warn(msg)\u001B[0m\n",
      "\u001B[34mCUDA SETUP: CUDA runtime path found: /opt/conda/lib/libcudart.so.11.0\u001B[0m\n",
      "\u001B[34mCUDA SETUP: Highest compute capability among GPUs detected: 8.6\u001B[0m\n",
      "\u001B[34mCUDA SETUP: Detected CUDA version 118\u001B[0m\n",
      "\u001B[34mCUDA SETUP: Loading binary /opt/conda/lib/python3.10/site-packages/bitsandbytes/libbitsandbytes_cuda118.so...\u001B[0m\n",
      "\u001B[34m===================================BUG REPORT===================================\u001B[0m\n",
      "\u001B[34mWelcome to bitsandbytes. For bug reports, please run\u001B[0m\n",
      "\u001B[34mpython -m bitsandbytes\n",
      " and submit this information together with your error trace to: https://github.com/TimDettmers/bitsandbytes/issues\u001B[0m\n",
      "\u001B[34m================================================================================\u001B[0m\n",
      "\u001B[34m===================================BUG REPORT===================================\u001B[0m\n",
      "\u001B[34mWelcome to bitsandbytes. For bug reports, please run\u001B[0m\n",
      "\u001B[34mpython -m bitsandbytes\n",
      " and submit this information together with your error trace to: https://github.com/TimDettmers/bitsandbytes/issues\u001B[0m\n",
      "\u001B[34m================================================================================\u001B[0m\n",
      "\u001B[34mbin /opt/conda/lib/python3.10/site-packages/bitsandbytes/libbitsandbytes_cuda118.so\u001B[0m\n",
      "\u001B[34m/opt/conda/lib/python3.10/site-packages/bitsandbytes/cuda_setup/main.py:149: UserWarning: WARNING: The following directories listed in your path were found to be non-existent: {PosixPath('/usr/local/nvidia/lib'), PosixPath('/usr/local/nvidia/lib64')}\n",
      "  warn(msg)\u001B[0m\n",
      "\u001B[34mCUDA SETUP: CUDA runtime path found: /opt/conda/lib/libcudart.so\u001B[0m\n",
      "\u001B[34mCUDA SETUP: Highest compute capability among GPUs detected: 8.6\u001B[0m\n",
      "\u001B[34mCUDA SETUP: Detected CUDA version 118\u001B[0m\n",
      "\u001B[34mCUDA SETUP: Loading binary /opt/conda/lib/python3.10/site-packages/bitsandbytes/libbitsandbytes_cuda118.so...\u001B[0m\n",
      "\u001B[34m===================================BUG REPORT===================================\u001B[0m\n",
      "\u001B[34mWelcome to bitsandbytes. For bug reports, please run\u001B[0m\n",
      "\u001B[34mpython -m bitsandbytes\n",
      " and submit this information together with your error trace to: https://github.com/TimDettmers/bitsandbytes/issues\u001B[0m\n",
      "\u001B[34m================================================================================\u001B[0m\n",
      "\u001B[34mbin /opt/conda/lib/python3.10/site-packages/bitsandbytes/libbitsandbytes_cuda118.so\u001B[0m\n",
      "\u001B[34m/opt/conda/lib/python3.10/site-packages/bitsandbytes/cuda_setup/main.py:149: UserWarning: WARNING: The following directories listed in your path were found to be non-existent: {PosixPath('/usr/local/nvidia/lib64'), PosixPath('/usr/local/nvidia/lib')}\n",
      "  warn(msg)\u001B[0m\n",
      "\u001B[34mCUDA SETUP: CUDA runtime path found: /opt/conda/lib/libcudart.so\u001B[0m\n",
      "\u001B[34mCUDA SETUP: Highest compute capability among GPUs detected: 8.6\u001B[0m\n",
      "\u001B[34mCUDA SETUP: Detected CUDA version 118\u001B[0m\n",
      "\u001B[34mCUDA SETUP: Loading binary /opt/conda/lib/python3.10/site-packages/bitsandbytes/libbitsandbytes_cuda118.so...\u001B[0m\n",
      "\u001B[34mbin /opt/conda/lib/python3.10/site-packages/bitsandbytes/libbitsandbytes_cuda118.so\u001B[0m\n",
      "\u001B[34m/opt/conda/lib/python3.10/site-packages/bitsandbytes/cuda_setup/main.py:149: UserWarning: WARNING: The following directories listed in your path were found to be non-existent: {PosixPath('/usr/local/nvidia/lib'), PosixPath('/usr/local/nvidia/lib64')}\n",
      "  warn(msg)\u001B[0m\n",
      "\u001B[34mCUDA SETUP: CUDA runtime path found: /opt/conda/lib/libcudart.so\u001B[0m\n",
      "\u001B[34mCUDA SETUP: Highest compute capability among GPUs detected: 8.6\u001B[0m\n",
      "\u001B[34mCUDA SETUP: Detected CUDA version 118\u001B[0m\n",
      "\u001B[34mCUDA SETUP: Loading binary /opt/conda/lib/python3.10/site-packages/bitsandbytes/libbitsandbytes_cuda118.so...\u001B[0m\n",
      "\u001B[35mINFO:root:Local rank is 1. Rank is 1\u001B[0m\n",
      "\u001B[35mINFO:root:Setting torch device = 1\u001B[0m\n",
      "\u001B[35mINFO:root:Loading the tokenizer.\u001B[0m\n",
      "\u001B[35mINFO:root:Loading the data.\u001B[0m\n",
      "\u001B[35mINFO:root:Local rank is 0. Rank is 0\u001B[0m\n",
      "\u001B[35mINFO:root:Setting torch device = 0\u001B[0m\n",
      "\u001B[35mINFO:root:Loading the tokenizer.\u001B[0m\n",
      "\u001B[35m--> Running with torch dist debug set to detail\u001B[0m\n",
      "\u001B[35mDownloading data files:   0%|          | 0/1 [00:00<?, ?it/s]\u001B[0m\n",
      "\u001B[35mDownloading data files: 100%|██████████| 1/1 [00:00<00:00, 13934.56it/s]\u001B[0m\n",
      "\u001B[35mExtracting data files:   0%|          | 0/1 [00:00<?, ?it/s]\u001B[0m\n",
      "\u001B[35mExtracting data files: 100%|██████████| 1/1 [00:00<00:00, 2020.38it/s]\u001B[0m\n",
      "\u001B[35mGenerating train split: 0 examples [00:00, ? examples/s]\u001B[0m\n",
      "\u001B[35mINFO:root:Loading the data.\u001B[0m\n",
      "\u001B[35mINFO:root:Local rank is 3. Rank is 3\u001B[0m\n",
      "\u001B[35mINFO:root:Setting torch device = 3\u001B[0m\n",
      "\u001B[35mINFO:root:Loading the tokenizer.\u001B[0m\n",
      "\u001B[35mINFO:root:Loading the data.\u001B[0m\n",
      "\u001B[35mGenerating train split: 1069 examples [00:00, 61128.45 examples/s]\u001B[0m\n",
      "\u001B[35mINFO:root:Local rank is 2. Rank is 2\u001B[0m\n",
      "\u001B[35mINFO:root:Setting torch device = 2\u001B[0m\n",
      "\u001B[35mINFO:root:Loading the tokenizer.\u001B[0m\n",
      "\u001B[35mMap:   0%|          | 0/1069 [00:00<?, ? examples/s]\u001B[0m\n",
      "\u001B[35mMap:   0%|          | 0/1069 [00:00<?, ? examples/s]\u001B[0m\n",
      "\u001B[35mINFO:root:Loading the data.\u001B[0m\n",
      "\u001B[35mMap:   0%|          | 0/1069 [00:00<?, ? examples/s]\u001B[0m\n",
      "\u001B[35mMap:   0%|          | 0/1069 [00:00<?, ? examples/s]\u001B[0m\n",
      "\u001B[35mMap: 100%|██████████| 1069/1069 [00:00<00:00, 17439.02 examples/s]\u001B[0m\n",
      "\u001B[35mMap: 100%|██████████| 1069/1069 [00:00<00:00, 17595.74 examples/s]\u001B[0m\n",
      "\u001B[35mMap:   0%|          | 0/1069 [00:00<?, ? examples/s]\u001B[0m\n",
      "\u001B[35mMap:   0%|          | 0/1069 [00:00<?, ? examples/s]\u001B[0m\n",
      "\u001B[35mMap: 100%|██████████| 1069/1069 [00:00<00:00, 17064.62 examples/s]\u001B[0m\n",
      "\u001B[35mMap:   0%|          | 0/1069 [00:00<?, ? examples/s]\u001B[0m\n",
      "\u001B[35mMap: 100%|██████████| 1069/1069 [00:00<00:00, 16967.56 examples/s]\u001B[0m\n",
      "\u001B[35mMap:   0%|          | 0/1069 [00:00<?, ? examples/s]\u001B[0m\n",
      "\u001B[34mINFO:root:Local rank is 0. Rank is 0\u001B[0m\n",
      "\u001B[34mINFO:root:Setting torch device = 0\u001B[0m\n",
      "\u001B[34mINFO:root:Loading the tokenizer.\u001B[0m\n",
      "\u001B[34m--> Running with torch dist debug set to detail\u001B[0m\n",
      "\u001B[34mINFO:root:Loading the data.\u001B[0m\n",
      "\u001B[34mDownloading data files:   0%|          | 0/1 [00:00<?, ?it/s]\u001B[0m\n",
      "\u001B[34mDownloading data files: 100%|██████████| 1/1 [00:00<00:00, 14716.86it/s]\u001B[0m\n",
      "\u001B[34mExtracting data files:   0%|          | 0/1 [00:00<?, ?it/s]\u001B[0m\n",
      "\u001B[34mExtracting data files: 100%|██████████| 1/1 [00:00<00:00, 1684.46it/s]\u001B[0m\n",
      "\u001B[34mGenerating train split: 0 examples [00:00, ? examples/s]\u001B[0m\n",
      "\u001B[34mGenerating train split: 1069 examples [00:00, 60564.50 examples/s]\u001B[0m\n",
      "\u001B[34mMap:   0%|          | 0/1069 [00:00<?, ? examples/s]\u001B[0m\n",
      "\u001B[34mINFO:root:Local rank is 1. Rank is 1\u001B[0m\n",
      "\u001B[34mINFO:root:Setting torch device = 1\u001B[0m\n",
      "\u001B[34mINFO:root:Loading the tokenizer.\u001B[0m\n",
      "\u001B[34mINFO:root:Local rank is 3. Rank is 3\u001B[0m\n",
      "\u001B[34mINFO:root:Setting torch device = 3\u001B[0m\n",
      "\u001B[34mINFO:root:Loading the tokenizer.\u001B[0m\n",
      "\u001B[34mINFO:root:Loading the data.\u001B[0m\n",
      "\u001B[34mINFO:root:Loading the data.\u001B[0m\n",
      "\u001B[34mMap: 100%|██████████| 1069/1069 [00:00<00:00, 17312.09 examples/s]\u001B[0m\n",
      "\u001B[34mMap:   0%|          | 0/1069 [00:00<?, ? examples/s]\u001B[0m\n",
      "\u001B[34mMap:   0%|          | 0/1069 [00:00<?, ? examples/s]\u001B[0m\n",
      "\u001B[34mINFO:root:Local rank is 2. Rank is 2\u001B[0m\n",
      "\u001B[34mINFO:root:Setting torch device = 2\u001B[0m\n",
      "\u001B[34mINFO:root:Loading the tokenizer.\u001B[0m\n",
      "\u001B[34mMap:   0%|          | 0/1069 [00:00<?, ? examples/s]\u001B[0m\n",
      "\u001B[34mINFO:root:Loading the data.\u001B[0m\n",
      "\u001B[34mMap:   0%|          | 0/1069 [00:00<?, ? examples/s]\u001B[0m\n",
      "\u001B[35mMap:  94%|█████████▎| 1000/1069 [00:02<00:00, 394.93 examples/s]\u001B[0m\n",
      "\u001B[35mMap:  94%|█████████▎| 1000/1069 [00:02<00:00, 389.97 examples/s]\u001B[0m\n",
      "\u001B[35mMap:  94%|█████████▎| 1000/1069 [00:02<00:00, 395.02 examples/s]\u001B[0m\n",
      "\u001B[35mMap:  94%|█████████▎| 1000/1069 [00:02<00:00, 391.88 examples/s]\u001B[0m\n",
      "\u001B[35mMap: 100%|██████████| 1069/1069 [00:02<00:00, 397.60 examples/s]\u001B[0m\n",
      "\u001B[35mMap: 100%|██████████| 1069/1069 [00:02<00:00, 396.64 examples/s]\u001B[0m\n",
      "\u001B[35mMap:   0%|          | 0/1069 [00:00<?, ? examples/s]\u001B[0m\n",
      "\u001B[35mMap: 100%|██████████| 1069/1069 [00:02<00:00, 392.86 examples/s]\u001B[0m\n",
      "\u001B[35mMap: 100%|██████████| 1069/1069 [00:02<00:00, 397.76 examples/s]\u001B[0m\n",
      "\u001B[35mMap: 100%|██████████| 1069/1069 [00:02<00:00, 391.84 examples/s]\u001B[0m\n",
      "\u001B[35mMap: 100%|██████████| 1069/1069 [00:02<00:00, 396.77 examples/s]\u001B[0m\n",
      "\u001B[35mMap:   0%|          | 0/1069 [00:00<?, ? examples/s]\u001B[0m\n",
      "\u001B[35mMap:   0%|          | 0/1069 [00:00<?, ? examples/s]\u001B[0m\n",
      "\u001B[35mMap: 100%|██████████| 1069/1069 [00:02<00:00, 394.65 examples/s]\u001B[0m\n",
      "\u001B[35mMap: 100%|██████████| 1069/1069 [00:02<00:00, 393.67 examples/s]\u001B[0m\n",
      "\u001B[35mMap:   0%|          | 0/1069 [00:00<?, ? examples/s]\u001B[0m\n",
      "\u001B[35mMap:  94%|█████████▎| 1000/1069 [00:00<00:00, 1843.90 examples/s]\u001B[0m\n",
      "\u001B[35mMap:  94%|█████████▎| 1000/1069 [00:00<00:00, 1861.90 examples/s]\u001B[0m\n",
      "\u001B[35mMap: 100%|██████████| 1069/1069 [00:00<00:00, 1835.12 examples/s]\u001B[0m\n",
      "\u001B[35mINFO:root:Test data is not identified. Split the data into train and test data respectively.\u001B[0m\n",
      "\u001B[35mINFO:root:Loading the pre-trained model.\u001B[0m\n",
      "\u001B[35mMap:  94%|█████████▎| 1000/1069 [00:00<00:00, 1789.21 examples/s]\u001B[0m\n",
      "\u001B[35mMap:  94%|█████████▎| 1000/1069 [00:00<00:00, 1787.53 examples/s]\u001B[0m\n",
      "\u001B[35mMap: 100%|██████████| 1069/1069 [00:00<00:00, 1851.77 examples/s]\u001B[0m\n",
      "\u001B[35mINFO:root:Test data is not identified. Split the data into train and test data respectively.\u001B[0m\n",
      "\u001B[35mINFO:root:Loading the pre-trained model.\u001B[0m\n",
      "\u001B[35mMap: 100%|██████████| 1069/1069 [00:00<00:00, 1780.53 examples/s]\u001B[0m\n",
      "\u001B[35mINFO:root:Test data is not identified. Split the data into train and test data respectively.\u001B[0m\n",
      "\u001B[35mMap: 100%|██████████| 1069/1069 [00:00<00:00, 1778.80 examples/s]\u001B[0m\n",
      "\u001B[35mINFO:root:Test data is not identified. Split the data into train and test data respectively.\u001B[0m\n",
      "\u001B[35mINFO:root:Loading the pre-trained model.\u001B[0m\n",
      "\u001B[35mINFO:root:Loading the pre-trained model.\u001B[0m\n",
      "\u001B[34mMap:  94%|█████████▎| 1000/1069 [00:02<00:00, 393.39 examples/s]\u001B[0m\n",
      "\u001B[34mMap:  94%|█████████▎| 1000/1069 [00:02<00:00, 392.66 examples/s]\u001B[0m\n",
      "\u001B[34mMap:  94%|█████████▎| 1000/1069 [00:02<00:00, 391.19 examples/s]\u001B[0m\n",
      "\u001B[34mMap:  94%|█████████▎| 1000/1069 [00:02<00:00, 389.81 examples/s]\u001B[0m\n",
      "\u001B[34mMap: 100%|██████████| 1069/1069 [00:02<00:00, 396.11 examples/s]\u001B[0m\n",
      "\u001B[34mMap: 100%|██████████| 1069/1069 [00:02<00:00, 395.14 examples/s]\u001B[0m\n",
      "\u001B[34mMap:   0%|          | 0/1069 [00:00<?, ? examples/s]\u001B[0m\n",
      "\u001B[34mMap: 100%|██████████| 1069/1069 [00:02<00:00, 395.02 examples/s]\u001B[0m\n",
      "\u001B[34mMap: 100%|██████████| 1069/1069 [00:02<00:00, 394.15 examples/s]\u001B[0m\n",
      "\u001B[34mMap:   0%|          | 0/1069 [00:00<?, ? examples/s]\u001B[0m\n",
      "\u001B[34mMap: 100%|██████████| 1069/1069 [00:02<00:00, 393.85 examples/s]\u001B[0m\n",
      "\u001B[34mMap: 100%|██████████| 1069/1069 [00:02<00:00, 392.90 examples/s]\u001B[0m\n",
      "\u001B[34mMap:   0%|          | 0/1069 [00:00<?, ? examples/s]\u001B[0m\n",
      "\u001B[34mMap: 100%|██████████| 1069/1069 [00:02<00:00, 392.46 examples/s]\u001B[0m\n",
      "\u001B[34mMap: 100%|██████████| 1069/1069 [00:02<00:00, 391.50 examples/s]\u001B[0m\n",
      "\u001B[34mMap:   0%|          | 0/1069 [00:00<?, ? examples/s]\u001B[0m\n",
      "\u001B[34mMap:  94%|█████████▎| 1000/1069 [00:00<00:00, 1893.66 examples/s]\u001B[0m\n",
      "\u001B[34mMap:  94%|█████████▎| 1000/1069 [00:00<00:00, 1807.44 examples/s]\u001B[0m\n",
      "\u001B[34mMap:  94%|█████████▎| 1000/1069 [00:00<00:00, 1834.26 examples/s]\u001B[0m\n",
      "\u001B[34mMap: 100%|██████████| 1069/1069 [00:00<00:00, 1880.40 examples/s]\u001B[0m\n",
      "\u001B[34mINFO:root:Test data is not identified. Split the data into train and test data respectively.\u001B[0m\n",
      "\u001B[34mINFO:root:Loading the pre-trained model.\u001B[0m\n",
      "\u001B[34mMap: 100%|██████████| 1069/1069 [00:00<00:00, 1798.96 examples/s]\u001B[0m\n",
      "\u001B[34mINFO:root:Test data is not identified. Split the data into train and test data respectively.\u001B[0m\n",
      "\u001B[34mINFO:root:Loading the pre-trained model.\u001B[0m\n",
      "\u001B[34mMap: 100%|██████████| 1069/1069 [00:00<00:00, 1823.13 examples/s]\u001B[0m\n",
      "\u001B[34mINFO:root:Test data is not identified. Split the data into train and test data respectively.\u001B[0m\n",
      "\u001B[34mINFO:root:Loading the pre-trained model.\u001B[0m\n",
      "\u001B[34mMap:  94%|█████████▎| 1000/1069 [00:00<00:00, 1734.25 examples/s]\u001B[0m\n",
      "\u001B[34mMap: 100%|██████████| 1069/1069 [00:00<00:00, 1728.56 examples/s]\u001B[0m\n",
      "\u001B[34mINFO:root:Test data is not identified. Split the data into train and test data respectively.\u001B[0m\n",
      "\u001B[34mINFO:root:Loading the pre-trained model.\u001B[0m\n",
      "\u001B[35mLoading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]\u001B[0m\n",
      "\u001B[35mLoading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]\u001B[0m\n",
      "\u001B[35mLoading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]\u001B[0m\n",
      "\u001B[35mLoading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]\u001B[0m\n",
      "\u001B[34mLoading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]\u001B[0m\n",
      "\u001B[34mLoading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]\u001B[0m\n",
      "\u001B[34mLoading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]\u001B[0m\n",
      "\u001B[34mLoading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]\u001B[0m\n",
      "\u001B[35mLoading checkpoint shards:  50%|█████     | 1/2 [00:09<00:09,  9.08s/it]\u001B[0m\n",
      "\u001B[35mLoading checkpoint shards:  50%|█████     | 1/2 [00:09<00:09,  9.52s/it]\u001B[0m\n",
      "\u001B[35mLoading checkpoint shards:  50%|█████     | 1/2 [00:09<00:09,  9.50s/it]\u001B[0m\n",
      "\u001B[35mLoading checkpoint shards:  50%|█████     | 1/2 [00:09<00:09,  9.53s/it]\u001B[0m\n",
      "\u001B[34mLoading checkpoint shards:  50%|█████     | 1/2 [00:09<00:09,  9.15s/it]\u001B[0m\n",
      "\u001B[34mLoading checkpoint shards:  50%|█████     | 1/2 [00:09<00:09,  9.71s/it]\u001B[0m\n",
      "\u001B[34mLoading checkpoint shards:  50%|█████     | 1/2 [00:09<00:09,  9.62s/it]\u001B[0m\n",
      "\u001B[34mLoading checkpoint shards:  50%|█████     | 1/2 [00:09<00:09,  9.70s/it]\u001B[0m\n",
      "\u001B[35mLoading checkpoint shards: 100%|██████████| 2/2 [00:12<00:00,  5.52s/it]\u001B[0m\n",
      "\u001B[35mLoading checkpoint shards: 100%|██████████| 2/2 [00:12<00:00,  6.06s/it]\u001B[0m\n",
      "\u001B[35m--> Model /opt/ml/additonals3data\u001B[0m\n",
      "\u001B[35m--> /opt/ml/additonals3data has 6738.415616 Million params\u001B[0m\n",
      "\u001B[35mLoading checkpoint shards: 100%|██████████| 2/2 [00:12<00:00,  5.62s/it]\u001B[0m\n",
      "\u001B[35mLoading checkpoint shards: 100%|██████████| 2/2 [00:12<00:00,  6.21s/it]\u001B[0m\n",
      "\u001B[35mLoading checkpoint shards: 100%|██████████| 2/2 [00:12<00:00,  5.61s/it]\u001B[0m\n",
      "\u001B[35mLoading checkpoint shards: 100%|██████████| 2/2 [00:12<00:00,  6.20s/it]\u001B[0m\n",
      "\u001B[35mLoading checkpoint shards: 100%|██████████| 2/2 [00:12<00:00,  5.62s/it]\u001B[0m\n",
      "\u001B[35mLoading checkpoint shards: 100%|██████████| 2/2 [00:12<00:00,  6.20s/it]\u001B[0m\n",
      "\u001B[34mLoading checkpoint shards: 100%|██████████| 2/2 [00:11<00:00,  5.35s/it]\u001B[0m\n",
      "\u001B[34mLoading checkpoint shards: 100%|██████████| 2/2 [00:11<00:00,  5.92s/it]\u001B[0m\n",
      "\u001B[34mLoading checkpoint shards: 100%|██████████| 2/2 [00:12<00:00,  5.43s/it]\u001B[0m\n",
      "\u001B[34mLoading checkpoint shards: 100%|██████████| 2/2 [00:12<00:00,  6.07s/it]\u001B[0m\n",
      "\u001B[34mLoading checkpoint shards: 100%|██████████| 2/2 [00:12<00:00,  5.40s/it]\u001B[0m\n",
      "\u001B[34mLoading checkpoint shards: 100%|██████████| 2/2 [00:12<00:00,  6.03s/it]\u001B[0m\n",
      "\u001B[34mLoading checkpoint shards: 100%|██████████| 2/2 [00:12<00:00,  5.43s/it]\u001B[0m\n",
      "\u001B[34mLoading checkpoint shards: 100%|██████████| 2/2 [00:12<00:00,  6.07s/it]\u001B[0m\n",
      "\u001B[34m--> Model /opt/ml/additonals3data\u001B[0m\n",
      "\u001B[34m--> /opt/ml/additonals3data has 6738.415616 Million params\u001B[0m\n",
      "\u001B[35mtrainable params: 4,194,304 || all params: 6,742,609,920 || trainable%: 0.06220594176090199\u001B[0m\n",
      "\u001B[35mbFloat16 enabled for mixed precision - using bfSixteen policy\u001B[0m\n",
      "\u001B[35mtrainable params: 4,194,304 || all params: 6,742,609,920 || trainable%: 0.06220594176090199\u001B[0m\n",
      "\u001B[35mtrainable params: 4,194,304 || all params: 6,742,609,920 || trainable%: 0.06220594176090199\u001B[0m\n",
      "\u001B[34mtrainable params: 4,194,304 || all params: 6,742,609,920 || trainable%: 0.06220594176090199\u001B[0m\n",
      "\u001B[35m--> applying fsdp activation checkpointing...\u001B[0m\n",
      "\u001B[35mINFO:root:--> Training Set Length = 442\u001B[0m\n",
      "\u001B[35mINFO:root:--> Validation Set Length = 111\u001B[0m\n",
      "\u001B[35m--> applying fsdp activation checkpointing...\u001B[0m\n",
      "\u001B[35m/opt/conda/lib/python3.10/site-packages/torch/cuda/memory.py:330: FutureWarning: torch.cuda.reset_max_memory_allocated now calls torch.cuda.reset_peak_memory_stats, which resets /all/ peak memory stats.\n",
      "  warnings.warn(\u001B[0m\n",
      "\u001B[35mTraining Epoch0:   0%|#033[34m          #033[0m| 0/27 [00:00<?, ?it/s]\u001B[0m\n",
      "\u001B[35m--> applying fsdp activation checkpointing...\u001B[0m\n",
      "\u001B[35mNCCL version 2.19.3+cuda11.8\u001B[0m\n",
      "\u001B[35m/opt/conda/lib/python3.10/site-packages/torch/cuda/memory.py:330: FutureWarning: torch.cuda.reset_max_memory_allocated now calls torch.cuda.reset_peak_memory_stats, which resets /all/ peak memory stats.\n",
      "  warnings.warn(\u001B[0m\n",
      "\u001B[35mTraining Epoch0:   0%|#033[34m          #033[0m| 0/27 [00:00<?, ?it/s]\u001B[0m\n",
      "\u001B[35m/opt/conda/lib/python3.10/site-packages/torch/cuda/memory.py:330: FutureWarning: torch.cuda.reset_max_memory_allocated now calls torch.cuda.reset_peak_memory_stats, which resets /all/ peak memory stats.\n",
      "  warnings.warn(\u001B[0m\n",
      "\u001B[35mTraining Epoch0:   0%|#033[34m          #033[0m| 0/27 [00:00<?, ?it/s]\u001B[0m\n",
      "\u001B[34mtrainable params: 4,194,304 || all params: 6,742,609,920 || trainable%: 0.06220594176090199\u001B[0m\n",
      "\u001B[34mtrainable params: 4,194,304 || all params: 6,742,609,920 || trainable%: 0.06220594176090199\u001B[0m\n",
      "\u001B[34mbFloat16 enabled for mixed precision - using bfSixteen policy\u001B[0m\n",
      "\u001B[34mtrainable params: 4,194,304 || all params: 6,742,609,920 || trainable%: 0.06220594176090199\u001B[0m\n",
      "\u001B[35mtrainable params: 4,194,304 || all params: 6,742,609,920 || trainable%: 0.06220594176090199\u001B[0m\n",
      "\u001B[34m--> applying fsdp activation checkpointing...\u001B[0m\n",
      "\u001B[34m--> applying fsdp activation checkpointing...\u001B[0m\n",
      "\u001B[34m--> applying fsdp activation checkpointing...\u001B[0m\n",
      "\u001B[34m/opt/conda/lib/python3.10/site-packages/torch/cuda/memory.py:330: FutureWarning: torch.cuda.reset_max_memory_allocated now calls torch.cuda.reset_peak_memory_stats, which resets /all/ peak memory stats.\n",
      "  warnings.warn(\u001B[0m\n",
      "\u001B[34mTraining Epoch0:   0%|#033[34m          #033[0m| 0/27 [00:00<?, ?it/s]\u001B[0m\n",
      "\u001B[34m--> applying fsdp activation checkpointing...\u001B[0m\n",
      "\u001B[34mINFO:root:--> Training Set Length = 442\u001B[0m\n",
      "\u001B[34mINFO:root:--> Validation Set Length = 111\u001B[0m\n",
      "\u001B[34m/opt/conda/lib/python3.10/site-packages/torch/cuda/memory.py:330: FutureWarning: torch.cuda.reset_max_memory_allocated now calls torch.cuda.reset_peak_memory_stats, which resets /all/ peak memory stats.\n",
      "  warnings.warn(\u001B[0m\n",
      "\u001B[34mTraining Epoch0:   0%|#033[34m          #033[0m| 0/27 [00:00<?, ?it/s]\u001B[0m\n",
      "\u001B[34m/opt/conda/lib/python3.10/site-packages/torch/cuda/memory.py:330: FutureWarning: torch.cuda.reset_max_memory_allocated now calls torch.cuda.reset_peak_memory_stats, which resets /all/ peak memory stats.\n",
      "  warnings.warn(\u001B[0m\n",
      "\u001B[34mTraining Epoch0:   0%|#033[34m          #033[0m| 0/27 [00:00<?, ?it/s]\u001B[0m\n",
      "\u001B[34m/opt/conda/lib/python3.10/site-packages/torch/cuda/memory.py:330: FutureWarning: torch.cuda.reset_max_memory_allocated now calls torch.cuda.reset_peak_memory_stats, which resets /all/ peak memory stats.\n",
      "  warnings.warn(\u001B[0m\n",
      "\u001B[34mTraining Epoch0:   0%|#033[34m          #033[0m| 0/27 [00:00<?, ?it/s]\u001B[0m\n",
      "\u001B[34mNCCL version 2.19.3+cuda11.8\u001B[0m\n",
      "\u001B[35m--> applying fsdp activation checkpointing...\u001B[0m\n",
      "\u001B[35m/opt/conda/lib/python3.10/site-packages/torch/cuda/memory.py:330: FutureWarning: torch.cuda.reset_max_memory_allocated now calls torch.cuda.reset_peak_memory_stats, which resets /all/ peak memory stats.\n",
      "  warnings.warn(\u001B[0m\n",
      "\u001B[35mTraining Epoch0:   0%|#033[34m          #033[0m| 0/27 [00:00<?, ?it/s]\u001B[0m\n",
      "\u001B[34mTraining Epoch0:   4%|#033[34m▎         #033[0m| 1/27 [00:10<04:37, 10.65s/it]\u001B[0m\n",
      "\u001B[34mTraining Epoch0:   4%|#033[34m▎         #033[0m| 1/27 [00:10<04:34, 10.56s/it]\u001B[0m\n",
      "\u001B[34mTraining Epoch0:   4%|#033[34m▎         #033[0m| 1/27 [00:10<04:33, 10.51s/it]\u001B[0m\n",
      "\u001B[34mstep 0 is completed and loss is 1.6134369373321533\u001B[0m\n",
      "\u001B[34mTraining Epoch0:   4%|#033[34m▎         #033[0m| 1/27 [00:10<04:32, 10.47s/it]\u001B[0m\n",
      "\u001B[35mTraining Epoch0:   4%|#033[34m▎         #033[0m| 1/27 [00:10<04:31, 10.44s/it]\u001B[0m\n",
      "\u001B[35mstep 0 is completed and loss is 1.6134369373321533\u001B[0m\n",
      "\u001B[35mTraining Epoch0:   4%|#033[34m▎         #033[0m| 1/27 [00:14<06:22, 14.72s/it]\u001B[0m\n",
      "\u001B[35mTraining Epoch0:   4%|#033[34m▎         #033[0m| 1/27 [00:14<06:17, 14.54s/it]\u001B[0m\n",
      "\u001B[35mTraining Epoch0:   4%|#033[34m▎         #033[0m| 1/27 [00:14<06:19, 14.58s/it]\u001B[0m\n",
      "\u001B[34mTraining Epoch0:   7%|#033[34m▋         #033[0m| 2/27 [00:20<04:13, 10.13s/it]\u001B[0m\n",
      "\u001B[34mTraining Epoch0:   7%|#033[34m▋         #033[0m| 2/27 [00:20<04:12, 10.10s/it]\u001B[0m\n",
      "\u001B[34mTraining Epoch0:   7%|#033[34m▋         #033[0m| 2/27 [00:20<04:14, 10.17s/it]\u001B[0m\n",
      "\u001B[34mstep 1 is completed and loss is 1.3458048105239868\u001B[0m\n",
      "\u001B[34mTraining Epoch0:   7%|#033[34m▋         #033[0m| 2/27 [00:20<04:12, 10.09s/it]\u001B[0m\n",
      "\u001B[35mstep 1 is completed and loss is 1.3458048105239868\u001B[0m\n",
      "\u001B[35mTraining Epoch0:   7%|#033[34m▋         #033[0m| 2/27 [00:24<04:55, 11.83s/it]\u001B[0m\n",
      "\u001B[35mTraining Epoch0:   7%|#033[34m▋         #033[0m| 2/27 [00:24<04:54, 11.77s/it]\u001B[0m\n",
      "\u001B[35mTraining Epoch0:   7%|#033[34m▋         #033[0m| 2/27 [00:20<04:11, 10.07s/it]\u001B[0m\n",
      "\u001B[35mTraining Epoch0:   7%|#033[34m▋         #033[0m| 2/27 [00:24<04:53, 11.76s/it]\u001B[0m\n",
      "\u001B[34mstep 2 is completed and loss is 1.6939013004302979\u001B[0m\n",
      "\u001B[34mTraining Epoch0:  11%|#033[34m█         #033[0m| 3/27 [00:30<03:59,  9.96s/it]\u001B[0m\n",
      "\u001B[34mTraining Epoch0:  11%|#033[34m█         #033[0m| 3/27 [00:30<03:59,  9.98s/it]\u001B[0m\n",
      "\u001B[34mTraining Epoch0:  11%|#033[34m█         #033[0m| 3/27 [00:30<03:59,  9.97s/it]\u001B[0m\n",
      "\u001B[34mTraining Epoch0:  11%|#033[34m█         #033[0m| 3/27 [00:30<04:00, 10.01s/it]\u001B[0m\n",
      "\u001B[35mTraining Epoch0:  11%|#033[34m█         #033[0m| 3/27 [00:30<03:58,  9.94s/it]\u001B[0m\n",
      "\u001B[35mTraining Epoch0:  11%|#033[34m█         #033[0m| 3/27 [00:34<04:20, 10.86s/it]\u001B[0m\n",
      "\u001B[35mstep 2 is completed and loss is 1.6939013004302979\u001B[0m\n",
      "\u001B[35mTraining Epoch0:  11%|#033[34m█         #033[0m| 3/27 [00:34<04:21, 10.90s/it]\u001B[0m\n",
      "\u001B[35mTraining Epoch0:  11%|#033[34m█         #033[0m| 3/27 [00:34<04:20, 10.87s/it]\u001B[0m\n",
      "\u001B[34mTraining Epoch0:  15%|#033[34m█▍        #033[0m| 4/27 [00:40<03:48,  9.93s/it]\u001B[0m\n",
      "\u001B[34mTraining Epoch0:  15%|#033[34m█▍        #033[0m| 4/27 [00:40<03:48,  9.92s/it]\u001B[0m\n",
      "\u001B[34mstep 3 is completed and loss is 1.408427119255066\u001B[0m\n",
      "\u001B[34mTraining Epoch0:  15%|#033[34m█▍        #033[0m| 4/27 [00:39<03:47,  9.91s/it]\u001B[0m\n",
      "\u001B[34mTraining Epoch0:  15%|#033[34m█▍        #033[0m| 4/27 [00:39<03:47,  9.91s/it]\u001B[0m\n",
      "\u001B[35mstep 3 is completed and loss is 1.408427119255066\u001B[0m\n",
      "\u001B[35mTraining Epoch0:  15%|#033[34m█▍        #033[0m| 4/27 [00:44<04:00, 10.47s/it]\u001B[0m\n",
      "\u001B[35mTraining Epoch0:  15%|#033[34m█▍        #033[0m| 4/27 [00:43<04:00, 10.45s/it]\u001B[0m\n",
      "\u001B[35mTraining Epoch0:  15%|#033[34m█▍        #033[0m| 4/27 [00:43<04:00, 10.44s/it]\u001B[0m\n",
      "\u001B[35mTraining Epoch0:  15%|#033[34m█▍        #033[0m| 4/27 [00:39<03:47,  9.89s/it]\u001B[0m\n",
      "\u001B[34mTraining Epoch0:  19%|#033[34m█▊        #033[0m| 5/27 [00:49<03:37,  9.88s/it]\u001B[0m\n",
      "\u001B[34mstep 4 is completed and loss is 1.4413765668869019\u001B[0m\n",
      "\u001B[34mTraining Epoch0:  19%|#033[34m█▊        #033[0m| 5/27 [00:49<03:37,  9.87s/it]\u001B[0m\n",
      "\u001B[34mTraining Epoch0:  19%|#033[34m█▊        #033[0m| 5/27 [00:49<03:37,  9.89s/it]\u001B[0m\n",
      "\u001B[34mTraining Epoch0:  19%|#033[34m█▊        #033[0m| 5/27 [00:49<03:37,  9.87s/it]\u001B[0m\n",
      "\u001B[35mTraining Epoch0:  19%|#033[34m█▊        #033[0m| 5/27 [00:53<03:44, 10.22s/it]\u001B[0m\n",
      "\u001B[35mTraining Epoch0:  19%|#033[34m█▊        #033[0m| 5/27 [00:49<03:36,  9.86s/it]\u001B[0m\n",
      "\u001B[35mstep 4 is completed and loss is 1.4413765668869019\u001B[0m\n",
      "\u001B[35mTraining Epoch0:  19%|#033[34m█▊        #033[0m| 5/27 [00:53<03:44, 10.21s/it]\u001B[0m\n",
      "\u001B[35mTraining Epoch0:  19%|#033[34m█▊        #033[0m| 5/27 [00:53<03:45, 10.23s/it]\u001B[0m\n",
      "\u001B[34mTraining Epoch0:  22%|#033[34m██▏       #033[0m| 6/27 [00:59<03:27,  9.86s/it]\u001B[0m\n",
      "\u001B[34mTraining Epoch0:  22%|#033[34m██▏       #033[0m| 6/27 [00:59<03:26,  9.85s/it]\u001B[0m\n",
      "\u001B[34mstep 5 is completed and loss is 1.4976089000701904\u001B[0m\n",
      "\u001B[34mTraining Epoch0:  22%|#033[34m██▏       #033[0m| 6/27 [00:59<03:26,  9.85s/it]\u001B[0m\n",
      "\u001B[34mTraining Epoch0:  22%|#033[34m██▏       #033[0m| 6/27 [00:59<03:26,  9.85s/it]\u001B[0m\n",
      "\u001B[35mTraining Epoch0:  22%|#033[34m██▏       #033[0m| 6/27 [01:03<03:31, 10.07s/it]\u001B[0m\n",
      "\u001B[35mstep 5 is completed and loss is 1.4976089000701904\u001B[0m\n",
      "\u001B[35mTraining Epoch0:  22%|#033[34m██▏       #033[0m| 6/27 [00:59<03:26,  9.84s/it]\u001B[0m\n",
      "\u001B[35mTraining Epoch0:  22%|#033[34m██▏       #033[0m| 6/27 [01:03<03:31, 10.08s/it]\u001B[0m\n",
      "\u001B[35mTraining Epoch0:  22%|#033[34m██▏       #033[0m| 6/27 [01:03<03:31, 10.07s/it]\u001B[0m\n",
      "\u001B[34mTraining Epoch0:  26%|#033[34m██▌       #033[0m| 7/27 [01:09<03:16,  9.84s/it]\u001B[0m\n",
      "\u001B[34mTraining Epoch0:  26%|#033[34m██▌       #033[0m| 7/27 [01:09<03:16,  9.84s/it]\u001B[0m\n",
      "\u001B[34mstep 6 is completed and loss is 1.7231749296188354\u001B[0m\n",
      "\u001B[34mTraining Epoch0:  26%|#033[34m██▌       #033[0m| 7/27 [01:09<03:16,  9.84s/it]\u001B[0m\n",
      "\u001B[34mTraining Epoch0:  26%|#033[34m██▌       #033[0m| 7/27 [01:09<03:16,  9.84s/it]\u001B[0m\n",
      "\u001B[35mTraining Epoch0:  26%|#033[34m██▌       #033[0m| 7/27 [01:13<03:19,  9.98s/it]\u001B[0m\n",
      "\u001B[35mstep 6 is completed and loss is 1.7231749296188354\u001B[0m\n",
      "\u001B[35mTraining Epoch0:  26%|#033[34m██▌       #033[0m| 7/27 [01:13<03:19,  9.99s/it]\u001B[0m\n",
      "\u001B[35mTraining Epoch0:  26%|#033[34m██▌       #033[0m| 7/27 [01:09<03:16,  9.83s/it]\u001B[0m\n",
      "\u001B[35mTraining Epoch0:  26%|#033[34m██▌       #033[0m| 7/27 [01:13<03:19,  9.99s/it]\u001B[0m\n",
      "\u001B[34mTraining Epoch0:  30%|#033[34m██▉       #033[0m| 8/27 [01:19<03:06,  9.83s/it]\u001B[0m\n",
      "\u001B[34mTraining Epoch0:  30%|#033[34m██▉       #033[0m| 8/27 [01:19<03:06,  9.82s/it]\u001B[0m\n",
      "\u001B[34mTraining Epoch0:  30%|#033[34m██▉       #033[0m| 8/27 [01:19<03:06,  9.83s/it]\u001B[0m\n",
      "\u001B[34mstep 7 is completed and loss is 1.5336047410964966\u001B[0m\n",
      "\u001B[34mTraining Epoch0:  30%|#033[34m██▉       #033[0m| 8/27 [01:19<03:06,  9.82s/it]\u001B[0m\n",
      "\u001B[35mstep 7 is completed and loss is 1.5336047410964966\u001B[0m\n",
      "\u001B[35mTraining Epoch0:  30%|#033[34m██▉       #033[0m| 8/27 [01:23<03:08,  9.93s/it]#015Training Epoch0:  30%|#033[34m██▉       #033[0m| 8/27 [01:23<03:08,  9.93s/it]\u001B[0m\n",
      "\u001B[35mTraining Epoch0:  30%|#033[34m██▉       #033[0m| 8/27 [01:23<03:08,  9.93s/it]\u001B[0m\n",
      "\u001B[35mTraining Epoch0:  30%|#033[34m██▉       #033[0m| 8/27 [01:19<03:06,  9.82s/it]\u001B[0m\n",
      "\u001B[34mTraining Epoch0:  33%|#033[34m███▎      #033[0m| 9/27 [01:29<02:56,  9.82s/it]\u001B[0m\n",
      "\u001B[34mTraining Epoch0:  33%|#033[34m███▎      #033[0m| 9/27 [01:29<02:56,  9.82s/it]\u001B[0m\n",
      "\u001B[34mstep 8 is completed and loss is 1.7194550037384033\u001B[0m\n",
      "\u001B[34mTraining Epoch0:  33%|#033[34m███▎      #033[0m| 9/27 [01:28<02:56,  9.82s/it]\u001B[0m\n",
      "\u001B[34mTraining Epoch0:  33%|#033[34m███▎      #033[0m| 9/27 [01:28<02:56,  9.82s/it]\u001B[0m\n",
      "\u001B[35mstep 8 is completed and loss is 1.7194550037384033\u001B[0m\n",
      "\u001B[35mTraining Epoch0:  33%|#033[34m███▎      #033[0m| 9/27 [01:33<02:58,  9.89s/it]\u001B[0m\n",
      "\u001B[35mTraining Epoch0:  33%|#033[34m███▎      #033[0m| 9/27 [01:32<02:58,  9.89s/it]\u001B[0m\n",
      "\u001B[35mTraining Epoch0:  33%|#033[34m███▎      #033[0m| 9/27 [01:28<02:56,  9.82s/it]\u001B[0m\n",
      "\u001B[35mTraining Epoch0:  33%|#033[34m███▎      #033[0m| 9/27 [01:33<02:58,  9.89s/it]\u001B[0m\n",
      "\u001B[34mTraining Epoch0:  37%|#033[34m███▋      #033[0m| 10/27 [01:38<02:46,  9.82s/it]\u001B[0m\n",
      "\u001B[34mTraining Epoch0:  37%|#033[34m███▋      #033[0m| 10/27 [01:38<02:46,  9.81s/it]\u001B[0m\n",
      "\u001B[34mTraining Epoch0:  37%|#033[34m███▋      #033[0m| 10/27 [01:38<02:46,  9.81s/it]\u001B[0m\n",
      "\u001B[34mstep 9 is completed and loss is 1.3168927431106567\u001B[0m\n",
      "\u001B[34mTraining Epoch0:  37%|#033[34m███▋      #033[0m| 10/27 [01:38<02:46,  9.81s/it]\u001B[0m\n",
      "\u001B[35mTraining Epoch0:  37%|#033[34m███▋      #033[0m| 10/27 [01:42<02:47,  9.86s/it]\u001B[0m\n",
      "\u001B[35mstep 9 is completed and loss is 1.3168927431106567\u001B[0m\n",
      "\u001B[35mTraining Epoch0:  37%|#033[34m███▋      #033[0m| 10/27 [01:42<02:47,  9.87s/it]\u001B[0m\n",
      "\u001B[35mTraining Epoch0:  37%|#033[34m███▋      #033[0m| 10/27 [01:38<02:46,  9.81s/it]\u001B[0m\n",
      "\u001B[35mTraining Epoch0:  37%|#033[34m███▋      #033[0m| 10/27 [01:42<02:47,  9.86s/it]\u001B[0m\n",
      "\u001B[34mTraining Epoch0:  41%|#033[34m████      #033[0m| 11/27 [01:48<02:37,  9.81s/it]\u001B[0m\n",
      "\u001B[34mTraining Epoch0:  41%|#033[34m████      #033[0m| 11/27 [01:48<02:37,  9.81s/it]\u001B[0m\n",
      "\u001B[34mstep 10 is completed and loss is 1.3530369997024536\u001B[0m\n",
      "\u001B[34mTraining Epoch0:  41%|#033[34m████      #033[0m| 11/27 [01:48<02:37,  9.81s/it]\u001B[0m\n",
      "\u001B[34mTraining Epoch0:  41%|#033[34m████      #033[0m| 11/27 [01:48<02:37,  9.81s/it]\u001B[0m\n",
      "\u001B[35mstep 10 is completed and loss is 1.3530369997024536\u001B[0m\n",
      "\u001B[35mTraining Epoch0:  41%|#033[34m████      #033[0m| 11/27 [01:52<02:37,  9.84s/it]\u001B[0m\n",
      "\u001B[35mTraining Epoch0:  41%|#033[34m████      #033[0m| 11/27 [01:52<02:37,  9.84s/it]\u001B[0m\n",
      "\u001B[35mTraining Epoch0:  41%|#033[34m████      #033[0m| 11/27 [01:48<02:36,  9.81s/it]\u001B[0m\n",
      "\u001B[35mTraining Epoch0:  41%|#033[34m████      #033[0m| 11/27 [01:52<02:37,  9.84s/it]\u001B[0m\n",
      "\u001B[34mTraining Epoch0:  44%|#033[34m████▍     #033[0m| 12/27 [01:58<02:27,  9.81s/it]\u001B[0m\n",
      "\u001B[34mstep 11 is completed and loss is 1.3746232986450195\u001B[0m\n",
      "\u001B[34mTraining Epoch0:  44%|#033[34m████▍     #033[0m| 12/27 [01:58<02:27,  9.81s/it]\u001B[0m\n",
      "\u001B[34mTraining Epoch0:  44%|#033[34m████▍     #033[0m| 12/27 [01:58<02:27,  9.81s/it]\u001B[0m\n",
      "\u001B[34mTraining Epoch0:  44%|#033[34m████▍     #033[0m| 12/27 [01:58<02:27,  9.81s/it]\u001B[0m\n",
      "\u001B[35mTraining Epoch0:  44%|#033[34m████▍     #033[0m| 12/27 [01:58<02:27,  9.81s/it]\u001B[0m\n",
      "\u001B[35mstep 11 is completed and loss is 1.3746232986450195\u001B[0m\n",
      "\u001B[35mTraining Epoch0:  44%|#033[34m████▍     #033[0m| 12/27 [02:02<02:27,  9.84s/it]\u001B[0m\n",
      "\u001B[35mTraining Epoch0:  44%|#033[34m████▍     #033[0m| 12/27 [02:02<02:27,  9.84s/it]#015Training Epoch0:  44%|#033[34m████▍     #033[0m| 12/27 [02:02<02:27,  9.84s/it]\u001B[0m\n",
      "\u001B[34mTraining Epoch0:  48%|#033[34m████▊     #033[0m| 13/27 [02:08<02:18,  9.89s/it]\u001B[0m\n",
      "\u001B[34mTraining Epoch0:  48%|#033[34m████▊     #033[0m| 13/27 [02:08<02:18,  9.89s/it]\u001B[0m\n",
      "\u001B[34mstep 12 is completed and loss is 1.447181224822998\u001B[0m\n",
      "\u001B[34mTraining Epoch0:  48%|#033[34m████▊     #033[0m| 13/27 [02:08<02:18,  9.89s/it]\u001B[0m\n",
      "\u001B[34mTraining Epoch0:  48%|#033[34m████▊     #033[0m| 13/27 [02:08<02:18,  9.89s/it]\u001B[0m\n",
      "\u001B[35mTraining Epoch0:  48%|#033[34m████▊     #033[0m| 13/27 [02:12<02:17,  9.83s/it]\u001B[0m\n",
      "\u001B[35mstep 12 is completed and loss is 1.447181224822998\u001B[0m\n",
      "\u001B[35mTraining Epoch0:  48%|#033[34m████▊     #033[0m| 13/27 [02:08<02:17,  9.82s/it]#015Training Epoch0:  48%|#033[34m████▊     #033[0m| 13/27 [02:12<02:17,  9.84s/it]\u001B[0m\n",
      "\u001B[35mTraining Epoch0:  48%|#033[34m████▊     #033[0m| 13/27 [02:12<02:17,  9.84s/it]\u001B[0m\n",
      "\u001B[34mTraining Epoch0:  52%|#033[34m█████▏    #033[0m| 14/27 [02:18<02:08,  9.87s/it]\u001B[0m\n",
      "\u001B[34mstep 13 is completed and loss is 1.1276723146438599\u001B[0m\n",
      "\u001B[34mTraining Epoch0:  52%|#033[34m█████▏    #033[0m| 14/27 [02:18<02:08,  9.87s/it]\u001B[0m\n",
      "\u001B[34mTraining Epoch0:  52%|#033[34m█████▏    #033[0m| 14/27 [02:18<02:08,  9.87s/it]\u001B[0m\n",
      "\u001B[34mTraining Epoch0:  52%|#033[34m█████▏    #033[0m| 14/27 [02:18<02:08,  9.87s/it]\u001B[0m\n",
      "\u001B[35mTraining Epoch0:  52%|#033[34m█████▏    #033[0m| 14/27 [02:22<02:08,  9.86s/it]\u001B[0m\n",
      "\u001B[35mstep 13 is completed and loss is 1.1276723146438599\u001B[0m\n",
      "\u001B[35mTraining Epoch0:  52%|#033[34m█████▏    #033[0m| 14/27 [02:22<02:08,  9.86s/it]#015Training Epoch0:  52%|#033[34m█████▏    #033[0m| 14/27 [02:22<02:08,  9.86s/it]\u001B[0m\n",
      "\u001B[35mTraining Epoch0:  52%|#033[34m█████▏    #033[0m| 14/27 [02:18<02:08,  9.85s/it]\u001B[0m\n",
      "\u001B[34mTraining Epoch0:  56%|#033[34m█████▌    #033[0m| 15/27 [02:28<01:58,  9.85s/it]\u001B[0m\n",
      "\u001B[34mTraining Epoch0:  56%|#033[34m█████▌    #033[0m| 15/27 [02:28<01:58,  9.85s/it]\u001B[0m\n",
      "\u001B[34mTraining Epoch0:  56%|#033[34m█████▌    #033[0m| 15/27 [02:28<01:58,  9.85s/it]\u001B[0m\n",
      "\u001B[34mstep 14 is completed and loss is 1.4604958295822144\u001B[0m\n",
      "\u001B[34mTraining Epoch0:  56%|#033[34m█████▌    #033[0m| 15/27 [02:28<01:58,  9.85s/it]\u001B[0m\n",
      "\u001B[35mstep 14 is completed and loss is 1.4604958295822144\u001B[0m\n",
      "\u001B[35mTraining Epoch0:  56%|#033[34m█████▌    #033[0m| 15/27 [02:32<01:58,  9.84s/it]\u001B[0m\n",
      "\u001B[35mTraining Epoch0:  56%|#033[34m█████▌    #033[0m| 15/27 [02:27<01:57,  9.83s/it]\u001B[0m\n",
      "\u001B[35mTraining Epoch0:  56%|#033[34m█████▌    #033[0m| 15/27 [02:31<01:58,  9.84s/it]\u001B[0m\n",
      "\u001B[35mTraining Epoch0:  56%|#033[34m█████▌    #033[0m| 15/27 [02:31<01:58,  9.84s/it]\u001B[0m\n",
      "\u001B[34mTraining Epoch0:  59%|#033[34m█████▉    #033[0m| 16/27 [02:38<01:48,  9.84s/it]\u001B[0m\n",
      "\u001B[34mstep 15 is completed and loss is 1.5552101135253906\u001B[0m\n",
      "\u001B[34mTraining Epoch0:  59%|#033[34m█████▉    #033[0m| 16/27 [02:37<01:48,  9.84s/it]\u001B[0m\n",
      "\u001B[34mTraining Epoch0:  59%|#033[34m█████▉    #033[0m| 16/27 [02:37<01:48,  9.84s/it]\u001B[0m\n",
      "\u001B[34mTraining Epoch0:  59%|#033[34m█████▉    #033[0m| 16/27 [02:37<01:48,  9.84s/it]\u001B[0m\n",
      "\u001B[35mTraining Epoch0:  59%|#033[34m█████▉    #033[0m| 16/27 [02:41<01:48,  9.83s/it]\u001B[0m\n",
      "\u001B[35mstep 15 is completed and loss is 1.5552101135253906\u001B[0m\n",
      "\u001B[35mTraining Epoch0:  59%|#033[34m█████▉    #033[0m| 16/27 [02:41<01:48,  9.83s/it]\u001B[0m\n",
      "\u001B[35mTraining Epoch0:  59%|#033[34m█████▉    #033[0m| 16/27 [02:37<01:48,  9.83s/it]\u001B[0m\n",
      "\u001B[35mTraining Epoch0:  59%|#033[34m█████▉    #033[0m| 16/27 [02:41<01:48,  9.83s/it]\u001B[0m\n",
      "\u001B[34mTraining Epoch0:  63%|#033[34m██████▎   #033[0m| 17/27 [02:47<01:38,  9.84s/it]\u001B[0m\n",
      "\u001B[34mstep 16 is completed and loss is 1.5662109851837158\u001B[0m\n",
      "\u001B[34mTraining Epoch0:  63%|#033[34m██████▎   #033[0m| 17/27 [02:47<01:38,  9.84s/it]\u001B[0m\n",
      "\u001B[34mTraining Epoch0:  63%|#033[34m██████▎   #033[0m| 17/27 [02:47<01:38,  9.84s/it]\u001B[0m\n",
      "\u001B[34mTraining Epoch0:  63%|#033[34m██████▎   #033[0m| 17/27 [02:47<01:38,  9.84s/it]\u001B[0m\n",
      "\u001B[35mTraining Epoch0:  63%|#033[34m██████▎   #033[0m| 17/27 [02:47<01:38,  9.82s/it]\u001B[0m\n",
      "\u001B[35mTraining Epoch0:  63%|#033[34m██████▎   #033[0m| 17/27 [02:51<01:38,  9.82s/it]\u001B[0m\n",
      "\u001B[35mstep 16 is completed and loss is 1.5662109851837158\u001B[0m\n",
      "\u001B[35mTraining Epoch0:  63%|#033[34m██████▎   #033[0m| 17/27 [02:51<01:38,  9.82s/it]\u001B[0m\n",
      "\u001B[35mTraining Epoch0:  63%|#033[34m██████▎   #033[0m| 17/27 [02:51<01:38,  9.82s/it]\u001B[0m\n",
      "\u001B[34mstep 17 is completed and loss is 1.4702985286712646\u001B[0m\n",
      "\u001B[34mTraining Epoch0:  67%|#033[34m██████▋   #033[0m| 18/27 [02:57<01:28,  9.83s/it]\u001B[0m\n",
      "\u001B[34mTraining Epoch0:  67%|#033[34m██████▋   #033[0m| 18/27 [02:57<01:28,  9.83s/it]\u001B[0m\n",
      "\u001B[34mTraining Epoch0:  67%|#033[34m██████▋   #033[0m| 18/27 [02:57<01:28,  9.83s/it]\u001B[0m\n",
      "\u001B[34mTraining Epoch0:  67%|#033[34m██████▋   #033[0m| 18/27 [02:57<01:28,  9.83s/it]\u001B[0m\n",
      "\u001B[35mstep 17 is completed and loss is 1.4702985286712646\u001B[0m\n",
      "\u001B[35mTraining Epoch0:  67%|#033[34m██████▋   #033[0m| 18/27 [02:57<01:28,  9.81s/it]\u001B[0m\n",
      "\u001B[35mTraining Epoch0:  67%|#033[34m██████▋   #033[0m| 18/27 [03:01<01:28,  9.81s/it]\u001B[0m\n",
      "\u001B[35mTraining Epoch0:  67%|#033[34m██████▋   #033[0m| 18/27 [03:01<01:28,  9.81s/it]\u001B[0m\n",
      "\u001B[35mTraining Epoch0:  67%|#033[34m██████▋   #033[0m| 18/27 [03:01<01:28,  9.81s/it]\u001B[0m\n",
      "\u001B[34mTraining Epoch0:  70%|#033[34m███████   #033[0m| 19/27 [03:07<01:18,  9.82s/it]\u001B[0m\n",
      "\u001B[34mTraining Epoch0:  70%|#033[34m███████   #033[0m| 19/27 [03:07<01:18,  9.82s/it]\u001B[0m\n",
      "\u001B[34mTraining Epoch0:  70%|#033[34m███████   #033[0m| 19/27 [03:07<01:18,  9.82s/it]\u001B[0m\n",
      "\u001B[34mstep 18 is completed and loss is 1.3534547090530396\u001B[0m\n",
      "\u001B[34mTraining Epoch0:  70%|#033[34m███████   #033[0m| 19/27 [03:07<01:18,  9.83s/it]\u001B[0m\n",
      "\u001B[35mstep 18 is completed and loss is 1.3534547090530396\u001B[0m\n",
      "\u001B[35mTraining Epoch0:  70%|#033[34m███████   #033[0m| 19/27 [03:11<01:18,  9.81s/it]\u001B[0m\n",
      "\u001B[35mTraining Epoch0:  70%|#033[34m███████   #033[0m| 19/27 [03:11<01:18,  9.81s/it]\u001B[0m\n",
      "\u001B[35mTraining Epoch0:  70%|#033[34m███████   #033[0m| 19/27 [03:07<01:18,  9.81s/it]\u001B[0m\n",
      "\u001B[35mTraining Epoch0:  70%|#033[34m███████   #033[0m| 19/27 [03:11<01:18,  9.81s/it]\u001B[0m\n",
      "\u001B[34mstep 19 is completed and loss is 1.2277940511703491\u001B[0m\n",
      "\u001B[34mTraining Epoch0:  74%|#033[34m███████▍  #033[0m| 20/27 [03:17<01:08,  9.82s/it]#015Training Epoch0:  74%|#033[34m███████▍  #033[0m| 20/27 [03:17<01:08,  9.82s/it]\u001B[0m\n",
      "\u001B[34mTraining Epoch0:  74%|#033[34m███████▍  #033[0m| 20/27 [03:17<01:08,  9.82s/it]\u001B[0m\n",
      "\u001B[34mTraining Epoch0:  74%|#033[34m███████▍  #033[0m| 20/27 [03:17<01:08,  9.82s/it]\u001B[0m\n",
      "\u001B[35mTraining Epoch0:  74%|#033[34m███████▍  #033[0m| 20/27 [03:20<01:08,  9.81s/it]\u001B[0m\n",
      "\u001B[35mTraining Epoch0:  74%|#033[34m███████▍  #033[0m| 20/27 [03:16<01:08,  9.81s/it]#015Training Epoch0:  74%|#033[34m███████▍  #033[0m| 20/27 [03:20<01:08,  9.81s/it]\u001B[0m\n",
      "\u001B[35mstep 19 is completed and loss is 1.2277940511703491\u001B[0m\n",
      "\u001B[35mTraining Epoch0:  74%|#033[34m███████▍  #033[0m| 20/27 [03:21<01:08,  9.81s/it]\u001B[0m\n",
      "\u001B[34mstep 20 is completed and loss is 1.2822155952453613\u001B[0m\n",
      "\u001B[34mTraining Epoch0:  78%|#033[34m███████▊  #033[0m| 21/27 [03:27<00:58,  9.82s/it]\u001B[0m\n",
      "\u001B[34mTraining Epoch0:  78%|#033[34m███████▊  #033[0m| 21/27 [03:26<00:58,  9.82s/it]\u001B[0m\n",
      "\u001B[34mTraining Epoch0:  78%|#033[34m███████▊  #033[0m| 21/27 [03:27<00:58,  9.82s/it]\u001B[0m\n",
      "\u001B[34mTraining Epoch0:  78%|#033[34m███████▊  #033[0m| 21/27 [03:26<00:58,  9.82s/it]\u001B[0m\n",
      "\u001B[35mTraining Epoch0:  78%|#033[34m███████▊  #033[0m| 21/27 [03:30<00:58,  9.81s/it]\u001B[0m\n",
      "\u001B[35mstep 20 is completed and loss is 1.2822155952453613\u001B[0m\n",
      "\u001B[35mTraining Epoch0:  78%|#033[34m███████▊  #033[0m| 21/27 [03:30<00:58,  9.81s/it]\u001B[0m\n",
      "\u001B[35mTraining Epoch0:  78%|#033[34m███████▊  #033[0m| 21/27 [03:30<00:58,  9.81s/it]\u001B[0m\n",
      "\u001B[35mTraining Epoch0:  78%|#033[34m███████▊  #033[0m| 21/27 [03:26<00:58,  9.81s/it]\u001B[0m\n",
      "\u001B[34mstep 21 is completed and loss is 1.332402229309082\u001B[0m\n",
      "\u001B[34mTraining Epoch0:  81%|#033[34m████████▏ #033[0m| 22/27 [03:36<00:49,  9.82s/it]\u001B[0m\n",
      "\u001B[34mTraining Epoch0:  81%|#033[34m████████▏ #033[0m| 22/27 [03:36<00:49,  9.82s/it]\u001B[0m\n",
      "\u001B[34mTraining Epoch0:  81%|#033[34m████████▏ #033[0m| 22/27 [03:36<00:49,  9.82s/it]\u001B[0m\n",
      "\u001B[34mTraining Epoch0:  81%|#033[34m████████▏ #033[0m| 22/27 [03:36<00:49,  9.82s/it]\u001B[0m\n",
      "\u001B[35mTraining Epoch0:  81%|#033[34m████████▏ #033[0m| 22/27 [03:36<00:49,  9.80s/it]\u001B[0m\n",
      "\u001B[35mTraining Epoch0:  81%|#033[34m████████▏ #033[0m| 22/27 [03:40<00:49,  9.81s/it]\u001B[0m\n",
      "\u001B[35mTraining Epoch0:  81%|#033[34m████████▏ #033[0m| 22/27 [03:40<00:49,  9.81s/it]\u001B[0m\n",
      "\u001B[35mstep 21 is completed and loss is 1.332402229309082\u001B[0m\n",
      "\u001B[35mTraining Epoch0:  81%|#033[34m████████▏ #033[0m| 22/27 [03:40<00:49,  9.81s/it]\u001B[0m\n",
      "\u001B[34mTraining Epoch0:  85%|#033[34m████████▌ #033[0m| 23/27 [03:46<00:39,  9.81s/it]\u001B[0m\n",
      "\u001B[34mstep 22 is completed and loss is 1.442696213722229\u001B[0m\n",
      "\u001B[34mTraining Epoch0:  85%|#033[34m████████▌ #033[0m| 23/27 [03:46<00:39,  9.81s/it]\u001B[0m\n",
      "\u001B[34mTraining Epoch0:  85%|#033[34m████████▌ #033[0m| 23/27 [03:46<00:39,  9.81s/it]\u001B[0m\n",
      "\u001B[34mTraining Epoch0:  85%|#033[34m████████▌ #033[0m| 23/27 [03:46<00:39,  9.81s/it]\u001B[0m\n",
      "\u001B[35mstep 22 is completed and loss is 1.442696213722229\u001B[0m\n",
      "\u001B[35mTraining Epoch0:  85%|#033[34m████████▌ #033[0m| 23/27 [03:50<00:39,  9.80s/it]#015Training Epoch0:  85%|#033[34m████████▌ #033[0m| 23/27 [03:46<00:39,  9.80s/it]\u001B[0m\n",
      "\u001B[35mTraining Epoch0:  85%|#033[34m████████▌ #033[0m| 23/27 [03:50<00:39,  9.81s/it]\u001B[0m\n",
      "\u001B[35mTraining Epoch0:  85%|#033[34m████████▌ #033[0m| 23/27 [03:50<00:39,  9.81s/it]\u001B[0m\n",
      "\u001B[34mTraining Epoch0:  89%|#033[34m████████▉ #033[0m| 24/27 [03:56<00:29,  9.81s/it]\u001B[0m\n",
      "\u001B[34mTraining Epoch0:  89%|#033[34m████████▉ #033[0m| 24/27 [03:56<00:29,  9.81s/it]\u001B[0m\n",
      "\u001B[34mTraining Epoch0:  89%|#033[34m████████▉ #033[0m| 24/27 [03:56<00:29,  9.81s/it]\u001B[0m\n",
      "\u001B[34mstep 23 is completed and loss is 1.3019806146621704\u001B[0m\n",
      "\u001B[34mTraining Epoch0:  89%|#033[34m████████▉ #033[0m| 24/27 [03:56<00:29,  9.81s/it]\u001B[0m\n",
      "\u001B[35mTraining Epoch0:  89%|#033[34m████████▉ #033[0m| 24/27 [03:56<00:29,  9.80s/it]\u001B[0m\n",
      "\u001B[35mTraining Epoch0:  89%|#033[34m████████▉ #033[0m| 24/27 [04:00<00:29,  9.80s/it]\u001B[0m\n",
      "\u001B[35mstep 23 is completed and loss is 1.3019806146621704\u001B[0m\n",
      "\u001B[35mTraining Epoch0:  89%|#033[34m████████▉ #033[0m| 24/27 [04:00<00:29,  9.80s/it]\u001B[0m\n",
      "\u001B[35mTraining Epoch0:  89%|#033[34m████████▉ #033[0m| 24/27 [04:00<00:29,  9.80s/it]\u001B[0m\n",
      "\u001B[34mstep 24 is completed and loss is 1.5274810791015625\u001B[0m\n",
      "\u001B[34mTraining Epoch0:  93%|#033[34m█████████▎#033[0m| 25/27 [04:06<00:19,  9.82s/it]\u001B[0m\n",
      "\u001B[34mTraining Epoch0:  93%|#033[34m█████████▎#033[0m| 25/27 [04:06<00:19,  9.82s/it]\u001B[0m\n",
      "\u001B[34mTraining Epoch0:  93%|#033[34m█████████▎#033[0m| 25/27 [04:06<00:19,  9.82s/it]\u001B[0m\n",
      "\u001B[34mTraining Epoch0:  93%|#033[34m█████████▎#033[0m| 25/27 [04:06<00:19,  9.82s/it]\u001B[0m\n",
      "\u001B[35mTraining Epoch0:  93%|#033[34m█████████▎#033[0m| 25/27 [04:09<00:19,  9.81s/it]\u001B[0m\n",
      "\u001B[35mTraining Epoch0:  93%|#033[34m█████████▎#033[0m| 25/27 [04:05<00:19,  9.81s/it]\u001B[0m\n",
      "\u001B[35mstep 24 is completed and loss is 1.5274810791015625\u001B[0m\n",
      "\u001B[35mTraining Epoch0:  93%|#033[34m█████████▎#033[0m| 25/27 [04:10<00:19,  9.81s/it]\u001B[0m\n",
      "\u001B[35mTraining Epoch0:  93%|#033[34m█████████▎#033[0m| 25/27 [04:10<00:19,  9.81s/it]\u001B[0m\n",
      "\u001B[34mstep 25 is completed and loss is 1.184082269668579\u001B[0m\n",
      "\u001B[34mTraining Epoch0:  96%|#033[34m█████████▋#033[0m| 26/27 [04:16<00:09,  9.85s/it]\u001B[0m\n",
      "\u001B[34mTraining Epoch0:  96%|#033[34m█████████▋#033[0m| 26/27 [04:16<00:09,  9.85s/it]\u001B[0m\n",
      "\u001B[34mTraining Epoch0:  96%|#033[34m█████████▋#033[0m| 26/27 [04:16<00:09,  9.85s/it]\u001B[0m\n",
      "\u001B[34mTraining Epoch0:  96%|#033[34m█████████▋#033[0m| 26/27 [04:16<00:09,  9.85s/it]\u001B[0m\n",
      "\u001B[35mTraining Epoch0:  96%|#033[34m█████████▋#033[0m| 26/27 [04:15<00:09,  9.81s/it]\u001B[0m\n",
      "\u001B[35mstep 25 is completed and loss is 1.184082269668579\u001B[0m\n",
      "\u001B[35mTraining Epoch0:  96%|#033[34m█████████▋#033[0m| 26/27 [04:19<00:09,  9.81s/it]\u001B[0m\n",
      "\u001B[35mTraining Epoch0:  96%|#033[34m█████████▋#033[0m| 26/27 [04:19<00:09,  9.81s/it]\u001B[0m\n",
      "\u001B[35mTraining Epoch0:  96%|#033[34m█████████▋#033[0m| 26/27 [04:19<00:09,  9.81s/it]\u001B[0m\n",
      "\u001B[34mTraining Epoch0: 100%|#033[34m██████████#033[0m| 27/27 [04:26<00:00,  9.88s/it]\u001B[0m\n",
      "\u001B[34mTraining Epoch0: 100%|#033[34m██████████#033[0m| 27/27 [04:26<00:00,  9.86s/it]\u001B[0m\n",
      "\u001B[34mTraining Epoch0: 100%|#033[34m██████████#033[0m| 27/27 [04:26<00:00,  9.88s/it]\u001B[0m\n",
      "\u001B[34mstep 26 is completed and loss is 1.118351697921753\u001B[0m\n",
      "\u001B[34mTraining Epoch0: 100%|#033[34m██████████#033[0m| 27/27 [04:26<00:00,  9.86s/it]\u001B[0m\n",
      "\u001B[34m#015Training Epoch0: 100%|#033[34m██████████#033[0m| 27/27 [04:26<00:00,  9.88s/it]\u001B[0m\n",
      "\u001B[34mTraining Epoch0: 100%|#033[34m██████████#033[0m| 27/27 [04:26<00:00,  9.88s/it]\u001B[0m\n",
      "\u001B[34mTraining Epoch0: 100%|#033[34m██████████#033[0m| 27/27 [04:26<00:00,  9.86s/it]\u001B[0m\n",
      "\u001B[34mTraining Epoch0: 100%|#033[34m██████████#033[0m| 27/27 [04:26<00:00,  9.85s/it]\u001B[0m\n",
      "\u001B[34mMax CUDA memory allocated was 8 GB\u001B[0m\n",
      "\u001B[34mMax CUDA memory reserved was 9 GB\u001B[0m\n",
      "\u001B[34mPeak active CUDA memory was 8 GB\u001B[0m\n",
      "\u001B[34mCuda Malloc retires : 0\u001B[0m\n",
      "\u001B[34mCPU Total Peak Memory consumed during the train (max): 1 GB\u001B[0m\n",
      "\u001B[34mevaluating Epoch:   0%|#033[32m          #033[0m| 0/28 [00:00<?, ?it/s]\u001B[0m\n",
      "\u001B[34mevaluating Epoch:   0%|#033[32m          #033[0m| 0/28 [00:00<?, ?it/s]\u001B[0m\n",
      "\u001B[34mevaluating Epoch:   0%|#033[32m          #033[0m| 0/28 [00:00<?, ?it/s]\u001B[0m\n",
      "\u001B[34mevaluating Epoch:   0%|#033[32m          #033[0m| 0/28 [00:00<?, ?it/s]\u001B[0m\n",
      "\u001B[34mevaluating Epoch:   4%|#033[32m▎         #033[0m| 1/28 [00:03<01:37,  3.60s/it]\u001B[0m\n",
      "\u001B[34mevaluating Epoch:   4%|#033[32m▎         #033[0m| 1/28 [00:03<01:37,  3.60s/it]\u001B[0m\n",
      "\u001B[34mevaluating Epoch:   4%|#033[32m▎         #033[0m| 1/28 [00:03<01:36,  3.59s/it]\u001B[0m\n",
      "\u001B[34mevaluating Epoch:   4%|#033[32m▎         #033[0m| 1/28 [00:03<01:37,  3.60s/it]\u001B[0m\n",
      "\u001B[35mTraining Epoch0: 100%|#033[34m██████████#033[0m| 27/27 [04:29<00:00,  9.88s/it]#015Training Epoch0: 100%|#033[34m██████████#033[0m| 27/27 [04:29<00:00,  9.88s/it]#015Training Epoch0: 100%|#033[34m██████████#033[0m| 27/27 [04:25<00:00,  9.88s/it]\u001B[0m\n",
      "\u001B[35mTraining Epoch0: 100%|#033[34m██████████#033[0m| 27/27 [04:29<00:00,  9.99s/it]\u001B[0m\n",
      "\u001B[35m#015Training Epoch0: 100%|#033[34m██████████#033[0m| 27/27 [04:25<00:00,  9.84s/it]\u001B[0m\n",
      "\u001B[35mTraining Epoch0: 100%|#033[34m██████████#033[0m| 27/27 [04:29<00:00, 10.00s/it]\u001B[0m\n",
      "\u001B[35mstep 26 is completed and loss is 1.118351697921753\u001B[0m\n",
      "\u001B[35mTraining Epoch0: 100%|#033[34m██████████#033[0m| 27/27 [04:30<00:00,  9.88s/it]\u001B[0m\n",
      "\u001B[35mTraining Epoch0: 100%|#033[34m██████████#033[0m| 27/27 [04:30<00:00, 10.00s/it]\u001B[0m\n",
      "\u001B[35mMax CUDA memory allocated was 8 GB\u001B[0m\n",
      "\u001B[35mMax CUDA memory reserved was 9 GB\u001B[0m\n",
      "\u001B[35mPeak active CUDA memory was 8 GB\u001B[0m\n",
      "\u001B[35mCuda Malloc retires : 0\u001B[0m\n",
      "\u001B[35mCPU Total Peak Memory consumed during the train (max): 1 GB\u001B[0m\n",
      "\u001B[35mevaluating Epoch:   0%|#033[32m          #033[0m| 0/28 [00:00<?, ?it/s]\u001B[0m\n",
      "\u001B[35mevaluating Epoch:   0%|#033[32m          #033[0m| 0/28 [00:00<?, ?it/s]\u001B[0m\n",
      "\u001B[35mevaluating Epoch:   0%|#033[32m          #033[0m| 0/28 [00:00<?, ?it/s]\u001B[0m\n",
      "\u001B[35mevaluating Epoch:   0%|#033[32m          #033[0m| 0/28 [00:00<?, ?it/s]\u001B[0m\n",
      "\u001B[35mevaluating Epoch:   4%|#033[32m▎         #033[0m| 1/28 [00:03<01:36,  3.58s/it]\u001B[0m\n",
      "\u001B[35mevaluating Epoch:   4%|#033[32m▎         #033[0m| 1/28 [00:03<01:36,  3.59s/it]\u001B[0m\n",
      "\u001B[35mevaluating Epoch:   4%|#033[32m▎         #033[0m| 1/28 [00:03<01:36,  3.58s/it]\u001B[0m\n",
      "\u001B[35mevaluating Epoch:   4%|#033[32m▎         #033[0m| 1/28 [00:03<01:36,  3.59s/it]\u001B[0m\n",
      "\u001B[34mevaluating Epoch:   7%|#033[32m▋         #033[0m| 2/28 [00:07<01:31,  3.53s/it]\u001B[0m\n",
      "\u001B[34mevaluating Epoch:   7%|#033[32m▋         #033[0m| 2/28 [00:07<01:31,  3.53s/it]\u001B[0m\n",
      "\u001B[34mevaluating Epoch:   7%|#033[32m▋         #033[0m| 2/28 [00:07<01:31,  3.53s/it]\u001B[0m\n",
      "\u001B[34mevaluating Epoch:   7%|#033[32m▋         #033[0m| 2/28 [00:07<01:31,  3.53s/it]\u001B[0m\n",
      "\u001B[35mevaluating Epoch:   7%|#033[32m▋         #033[0m| 2/28 [00:07<01:31,  3.51s/it]#015evaluating Epoch:   7%|#033[32m▋         #033[0m| 2/28 [00:07<01:31,  3.51s/it]\u001B[0m\n",
      "\u001B[35mevaluating Epoch:   7%|#033[32m▋         #033[0m| 2/28 [00:07<01:31,  3.52s/it]\u001B[0m\n",
      "\u001B[35mevaluating Epoch:   7%|#033[32m▋         #033[0m| 2/28 [00:07<01:31,  3.52s/it]\u001B[0m\n",
      "\u001B[34mevaluating Epoch:  11%|#033[32m█         #033[0m| 3/28 [00:10<01:27,  3.50s/it]#015evaluating Epoch:  11%|#033[32m█         #033[0m| 3/28 [00:10<01:27,  3.50s/it]\u001B[0m\n",
      "\u001B[34mevaluating Epoch:  11%|#033[32m█         #033[0m| 3/28 [00:10<01:27,  3.50s/it]\u001B[0m\n",
      "\u001B[34mevaluating Epoch:  11%|#033[32m█         #033[0m| 3/28 [00:10<01:27,  3.50s/it]\u001B[0m\n",
      "\u001B[35mevaluating Epoch:  11%|#033[32m█         #033[0m| 3/28 [00:10<01:27,  3.48s/it]#015evaluating Epoch:  11%|#033[32m█         #033[0m| 3/28 [00:10<01:27,  3.48s/it]#015evaluating Epoch:  11%|#033[32m█         #033[0m| 3/28 [00:10<01:27,  3.48s/it]\u001B[0m\n",
      "\u001B[35mevaluating Epoch:  11%|#033[32m█         #033[0m| 3/28 [00:10<01:27,  3.48s/it]\u001B[0m\n",
      "\u001B[34mevaluating Epoch:  14%|#033[32m█▍        #033[0m| 4/28 [00:14<01:23,  3.49s/it]#015evaluating Epoch:  14%|#033[32m█▍        #033[0m| 4/28 [00:14<01:23,  3.49s/it]\u001B[0m\n",
      "\u001B[34mevaluating Epoch:  14%|#033[32m█▍        #033[0m| 4/28 [00:14<01:23,  3.49s/it]\u001B[0m\n",
      "\u001B[34mevaluating Epoch:  14%|#033[32m█▍        #033[0m| 4/28 [00:14<01:23,  3.49s/it]\u001B[0m\n",
      "\u001B[35mevaluating Epoch:  14%|#033[32m█▍        #033[0m| 4/28 [00:13<01:23,  3.47s/it]\u001B[0m\n",
      "\u001B[35mevaluating Epoch:  14%|#033[32m█▍        #033[0m| 4/28 [00:13<01:23,  3.47s/it]\u001B[0m\n",
      "\u001B[35mevaluating Epoch:  14%|#033[32m█▍        #033[0m| 4/28 [00:13<01:23,  3.47s/it]#015evaluating Epoch:  14%|#033[32m█▍        #033[0m| 4/28 [00:13<01:23,  3.47s/it]\u001B[0m\n",
      "\u001B[34mevaluating Epoch:  18%|#033[32m█▊        #033[0m| 5/28 [00:17<01:20,  3.48s/it]\u001B[0m\n",
      "\u001B[34mevaluating Epoch:  18%|#033[32m█▊        #033[0m| 5/28 [00:17<01:20,  3.48s/it]\u001B[0m\n",
      "\u001B[34mevaluating Epoch:  18%|#033[32m█▊        #033[0m| 5/28 [00:17<01:20,  3.48s/it]\u001B[0m\n",
      "\u001B[34mevaluating Epoch:  18%|#033[32m█▊        #033[0m| 5/28 [00:17<01:20,  3.48s/it]\u001B[0m\n",
      "\u001B[35mevaluating Epoch:  18%|#033[32m█▊        #033[0m| 5/28 [00:17<01:19,  3.47s/it]#015evaluating Epoch:  18%|#033[32m█▊        #033[0m| 5/28 [00:17<01:19,  3.47s/it]#015evaluating Epoch:  18%|#033[32m█▊        #033[0m| 5/28 [00:17<01:19,  3.47s/it]\u001B[0m\n",
      "\u001B[35mevaluating Epoch:  18%|#033[32m█▊        #033[0m| 5/28 [00:17<01:19,  3.47s/it]\u001B[0m\n",
      "\u001B[34mevaluating Epoch:  21%|#033[32m██▏       #033[0m| 6/28 [00:20<01:16,  3.48s/it]#015evaluating Epoch:  21%|#033[32m██▏       #033[0m| 6/28 [00:20<01:16,  3.48s/it]\u001B[0m\n",
      "\u001B[34mevaluating Epoch:  21%|#033[32m██▏       #033[0m| 6/28 [00:20<01:16,  3.48s/it]\u001B[0m\n",
      "\u001B[34mevaluating Epoch:  21%|#033[32m██▏       #033[0m| 6/28 [00:20<01:16,  3.48s/it]\u001B[0m\n",
      "\u001B[35mevaluating Epoch:  21%|#033[32m██▏       #033[0m| 6/28 [00:20<01:16,  3.47s/it]\u001B[0m\n",
      "\u001B[35mevaluating Epoch:  21%|#033[32m██▏       #033[0m| 6/28 [00:20<01:16,  3.47s/it]\u001B[0m\n",
      "\u001B[35mevaluating Epoch:  21%|#033[32m██▏       #033[0m| 6/28 [00:20<01:16,  3.47s/it]\u001B[0m\n",
      "\u001B[35mevaluating Epoch:  21%|#033[32m██▏       #033[0m| 6/28 [00:20<01:16,  3.47s/it]\u001B[0m\n",
      "\u001B[34mevaluating Epoch:  25%|#033[32m██▌       #033[0m| 7/28 [00:24<01:12,  3.47s/it]\u001B[0m\n",
      "\u001B[34mevaluating Epoch:  25%|#033[32m██▌       #033[0m| 7/28 [00:24<01:12,  3.47s/it]\u001B[0m\n",
      "\u001B[34mevaluating Epoch:  25%|#033[32m██▌       #033[0m| 7/28 [00:24<01:12,  3.47s/it]\u001B[0m\n",
      "\u001B[34mevaluating Epoch:  25%|#033[32m██▌       #033[0m| 7/28 [00:24<01:12,  3.47s/it]\u001B[0m\n",
      "\u001B[35mevaluating Epoch:  25%|#033[32m██▌       #033[0m| 7/28 [00:24<01:12,  3.46s/it]\u001B[0m\n",
      "\u001B[35mevaluating Epoch:  25%|#033[32m██▌       #033[0m| 7/28 [00:24<01:12,  3.46s/it]\u001B[0m\n",
      "\u001B[35mevaluating Epoch:  25%|#033[32m██▌       #033[0m| 7/28 [00:24<01:12,  3.46s/it]\u001B[0m\n",
      "\u001B[35mevaluating Epoch:  25%|#033[32m██▌       #033[0m| 7/28 [00:24<01:12,  3.46s/it]\u001B[0m\n",
      "\u001B[34mevaluating Epoch:  29%|#033[32m██▊       #033[0m| 8/28 [00:27<01:09,  3.47s/it]#015evaluating Epoch:  29%|#033[32m██▊       #033[0m| 8/28 [00:27<01:09,  3.47s/it]\u001B[0m\n",
      "\u001B[34mevaluating Epoch:  29%|#033[32m██▊       #033[0m| 8/28 [00:27<01:09,  3.47s/it]\u001B[0m\n",
      "\u001B[34mevaluating Epoch:  29%|#033[32m██▊       #033[0m| 8/28 [00:27<01:09,  3.47s/it]\u001B[0m\n",
      "\u001B[35mevaluating Epoch:  29%|#033[32m██▊       #033[0m| 8/28 [00:27<01:09,  3.46s/it]#015evaluating Epoch:  29%|#033[32m██▊       #033[0m| 8/28 [00:27<01:09,  3.46s/it]\u001B[0m\n",
      "\u001B[35mevaluating Epoch:  29%|#033[32m██▊       #033[0m| 8/28 [00:27<01:09,  3.46s/it]\u001B[0m\n",
      "\u001B[35mevaluating Epoch:  29%|#033[32m██▊       #033[0m| 8/28 [00:27<01:09,  3.46s/it]\u001B[0m\n",
      "\u001B[34mevaluating Epoch:  32%|#033[32m███▏      #033[0m| 9/28 [00:31<01:05,  3.47s/it]\u001B[0m\n",
      "\u001B[34mevaluating Epoch:  32%|#033[32m███▏      #033[0m| 9/28 [00:31<01:05,  3.47s/it]\u001B[0m\n",
      "\u001B[34mevaluating Epoch:  32%|#033[32m███▏      #033[0m| 9/28 [00:31<01:05,  3.47s/it]\u001B[0m\n",
      "\u001B[34mevaluating Epoch:  32%|#033[32m███▏      #033[0m| 9/28 [00:31<01:05,  3.47s/it]\u001B[0m\n",
      "\u001B[35mevaluating Epoch:  32%|#033[32m███▏      #033[0m| 9/28 [00:31<01:05,  3.46s/it]#015evaluating Epoch:  32%|#033[32m███▏      #033[0m| 9/28 [00:31<01:05,  3.46s/it]\u001B[0m\n",
      "\u001B[35mevaluating Epoch:  32%|#033[32m███▏      #033[0m| 9/28 [00:31<01:05,  3.46s/it]\u001B[0m\n",
      "\u001B[35mevaluating Epoch:  32%|#033[32m███▏      #033[0m| 9/28 [00:31<01:05,  3.46s/it]\u001B[0m\n",
      "\u001B[34mevaluating Epoch:  36%|#033[32m███▌      #033[0m| 10/28 [00:34<01:02,  3.48s/it]#015evaluating Epoch:  36%|#033[32m███▌      #033[0m| 10/28 [00:34<01:02,  3.48s/it]\u001B[0m\n",
      "\u001B[34mevaluating Epoch:  36%|#033[32m███▌      #033[0m| 10/28 [00:34<01:02,  3.48s/it]\u001B[0m\n",
      "\u001B[34mevaluating Epoch:  36%|#033[32m███▌      #033[0m| 10/28 [00:34<01:02,  3.48s/it]\u001B[0m\n",
      "\u001B[35mevaluating Epoch:  36%|#033[32m███▌      #033[0m| 10/28 [00:34<01:02,  3.46s/it]\u001B[0m\n",
      "\u001B[35mevaluating Epoch:  36%|#033[32m███▌      #033[0m| 10/28 [00:34<01:02,  3.46s/it]\u001B[0m\n",
      "\u001B[35mevaluating Epoch:  36%|#033[32m███▌      #033[0m| 10/28 [00:34<01:02,  3.46s/it]\u001B[0m\n",
      "\u001B[35mevaluating Epoch:  36%|#033[32m███▌      #033[0m| 10/28 [00:34<01:02,  3.46s/it]\u001B[0m\n",
      "\u001B[34mevaluating Epoch:  39%|#033[32m███▉      #033[0m| 11/28 [00:38<00:59,  3.47s/it]#015evaluating Epoch:  39%|#033[32m███▉      #033[0m| 11/28 [00:38<00:59,  3.47s/it]\u001B[0m\n",
      "\u001B[34mevaluating Epoch:  39%|#033[32m███▉      #033[0m| 11/28 [00:38<00:59,  3.47s/it]\u001B[0m\n",
      "\u001B[34mevaluating Epoch:  39%|#033[32m███▉      #033[0m| 11/28 [00:38<00:59,  3.47s/it]\u001B[0m\n",
      "\u001B[35mevaluating Epoch:  39%|#033[32m███▉      #033[0m| 11/28 [00:38<00:58,  3.46s/it]#015evaluating Epoch:  39%|#033[32m███▉      #033[0m| 11/28 [00:38<00:58,  3.46s/it]\u001B[0m\n",
      "\u001B[35mevaluating Epoch:  39%|#033[32m███▉      #033[0m| 11/28 [00:38<00:58,  3.46s/it]\u001B[0m\n",
      "\u001B[35mevaluating Epoch:  39%|#033[32m███▉      #033[0m| 11/28 [00:38<00:58,  3.46s/it]\u001B[0m\n",
      "\u001B[34mevaluating Epoch:  43%|#033[32m████▎     #033[0m| 12/28 [00:41<00:55,  3.47s/it]\u001B[0m\n",
      "\u001B[34mevaluating Epoch:  43%|#033[32m████▎     #033[0m| 12/28 [00:41<00:55,  3.47s/it]\u001B[0m\n",
      "\u001B[34mevaluating Epoch:  43%|#033[32m████▎     #033[0m| 12/28 [00:41<00:55,  3.47s/it]\u001B[0m\n",
      "\u001B[34mevaluating Epoch:  43%|#033[32m████▎     #033[0m| 12/28 [00:41<00:55,  3.47s/it]\u001B[0m\n",
      "\u001B[35mevaluating Epoch:  43%|#033[32m████▎     #033[0m| 12/28 [00:41<00:55,  3.46s/it]#015evaluating Epoch:  43%|#033[32m████▎     #033[0m| 12/28 [00:41<00:55,  3.46s/it]\u001B[0m\n",
      "\u001B[35mevaluating Epoch:  43%|#033[32m████▎     #033[0m| 12/28 [00:41<00:55,  3.46s/it]\u001B[0m\n",
      "\u001B[35mevaluating Epoch:  43%|#033[32m████▎     #033[0m| 12/28 [00:41<00:55,  3.46s/it]\u001B[0m\n",
      "\u001B[34mevaluating Epoch:  46%|#033[32m████▋     #033[0m| 13/28 [00:45<00:52,  3.47s/it]\u001B[0m\n",
      "\u001B[34mevaluating Epoch:  46%|#033[32m████▋     #033[0m| 13/28 [00:45<00:52,  3.47s/it]\u001B[0m\n",
      "\u001B[34mevaluating Epoch:  46%|#033[32m████▋     #033[0m| 13/28 [00:45<00:52,  3.47s/it]\u001B[0m\n",
      "\u001B[34mevaluating Epoch:  46%|#033[32m████▋     #033[0m| 13/28 [00:45<00:52,  3.47s/it]\u001B[0m\n",
      "\u001B[35mevaluating Epoch:  46%|#033[32m████▋     #033[0m| 13/28 [00:45<00:51,  3.46s/it]\u001B[0m\n",
      "\u001B[35mevaluating Epoch:  46%|#033[32m████▋     #033[0m| 13/28 [00:45<00:51,  3.46s/it]#015evaluating Epoch:  46%|#033[32m████▋     #033[0m| 13/28 [00:45<00:51,  3.46s/it]\u001B[0m\n",
      "\u001B[35mevaluating Epoch:  46%|#033[32m████▋     #033[0m| 13/28 [00:45<00:51,  3.46s/it]\u001B[0m\n",
      "\u001B[34mevaluating Epoch:  50%|#033[32m█████     #033[0m| 14/28 [00:48<00:48,  3.47s/it]#015evaluating Epoch:  50%|#033[32m█████     #033[0m| 14/28 [00:48<00:48,  3.47s/it]\u001B[0m\n",
      "\u001B[34mevaluating Epoch:  50%|#033[32m█████     #033[0m| 14/28 [00:48<00:48,  3.47s/it]\u001B[0m\n",
      "\u001B[34mevaluating Epoch:  50%|#033[32m█████     #033[0m| 14/28 [00:48<00:48,  3.47s/it]\u001B[0m\n",
      "\u001B[35mevaluating Epoch:  50%|#033[32m█████     #033[0m| 14/28 [00:48<00:48,  3.46s/it]\u001B[0m\n",
      "\u001B[35mevaluating Epoch:  50%|#033[32m█████     #033[0m| 14/28 [00:48<00:48,  3.46s/it]#015evaluating Epoch:  50%|#033[32m█████     #033[0m| 14/28 [00:48<00:48,  3.46s/it]\u001B[0m\n",
      "\u001B[35mevaluating Epoch:  50%|#033[32m█████     #033[0m| 14/28 [00:48<00:48,  3.46s/it]\u001B[0m\n",
      "\u001B[34mevaluating Epoch:  54%|#033[32m█████▎    #033[0m| 15/28 [00:52<00:45,  3.47s/it]\u001B[0m\n",
      "\u001B[34mevaluating Epoch:  54%|#033[32m█████▎    #033[0m| 15/28 [00:52<00:45,  3.47s/it]\u001B[0m\n",
      "\u001B[34mevaluating Epoch:  54%|#033[32m█████▎    #033[0m| 15/28 [00:52<00:45,  3.47s/it]\u001B[0m\n",
      "\u001B[34mevaluating Epoch:  54%|#033[32m█████▎    #033[0m| 15/28 [00:52<00:45,  3.47s/it]\u001B[0m\n",
      "\u001B[35mevaluating Epoch:  54%|#033[32m█████▎    #033[0m| 15/28 [00:52<00:44,  3.46s/it]\u001B[0m\n",
      "\u001B[35mevaluating Epoch:  54%|#033[32m█████▎    #033[0m| 15/28 [00:52<00:44,  3.46s/it]#015evaluating Epoch:  54%|#033[32m█████▎    #033[0m| 15/28 [00:52<00:44,  3.46s/it]\u001B[0m\n",
      "\u001B[35mevaluating Epoch:  54%|#033[32m█████▎    #033[0m| 15/28 [00:52<00:44,  3.46s/it]\u001B[0m\n",
      "\u001B[34mevaluating Epoch:  57%|#033[32m█████▋    #033[0m| 16/28 [00:55<00:41,  3.47s/it]#015evaluating Epoch:  57%|#033[32m█████▋    #033[0m| 16/28 [00:55<00:41,  3.47s/it]\u001B[0m\n",
      "\u001B[34mevaluating Epoch:  57%|#033[32m█████▋    #033[0m| 16/28 [00:55<00:41,  3.47s/it]\u001B[0m\n",
      "\u001B[34mevaluating Epoch:  57%|#033[32m█████▋    #033[0m| 16/28 [00:55<00:41,  3.47s/it]\u001B[0m\n",
      "\u001B[35mevaluating Epoch:  57%|#033[32m█████▋    #033[0m| 16/28 [00:55<00:41,  3.46s/it]\u001B[0m\n",
      "\u001B[35mevaluating Epoch:  57%|#033[32m█████▋    #033[0m| 16/28 [00:55<00:41,  3.46s/it]\u001B[0m\n",
      "\u001B[35mevaluating Epoch:  57%|#033[32m█████▋    #033[0m| 16/28 [00:55<00:41,  3.46s/it]\u001B[0m\n",
      "\u001B[35mevaluating Epoch:  57%|#033[32m█████▋    #033[0m| 16/28 [00:55<00:41,  3.46s/it]\u001B[0m\n",
      "\u001B[34mevaluating Epoch:  61%|#033[32m██████    #033[0m| 17/28 [00:59<00:38,  3.47s/it]#015evaluating Epoch:  61%|#033[32m██████    #033[0m| 17/28 [00:59<00:38,  3.47s/it]\u001B[0m\n",
      "\u001B[34mevaluating Epoch:  61%|#033[32m██████    #033[0m| 17/28 [00:59<00:38,  3.47s/it]\u001B[0m\n",
      "\u001B[34mevaluating Epoch:  61%|#033[32m██████    #033[0m| 17/28 [00:59<00:38,  3.47s/it]\u001B[0m\n",
      "\u001B[35mevaluating Epoch:  61%|#033[32m██████    #033[0m| 17/28 [00:58<00:38,  3.46s/it]#015evaluating Epoch:  61%|#033[32m██████    #033[0m| 17/28 [00:58<00:38,  3.46s/it]\u001B[0m\n",
      "\u001B[35mevaluating Epoch:  61%|#033[32m██████    #033[0m| 17/28 [00:58<00:38,  3.46s/it]\u001B[0m\n",
      "\u001B[35mevaluating Epoch:  61%|#033[32m██████    #033[0m| 17/28 [00:58<00:38,  3.46s/it]\u001B[0m\n",
      "\u001B[34mevaluating Epoch:  64%|#033[32m██████▍   #033[0m| 18/28 [01:02<00:34,  3.47s/it]\u001B[0m\n",
      "\u001B[34mevaluating Epoch:  64%|#033[32m██████▍   #033[0m| 18/28 [01:02<00:34,  3.47s/it]#015evaluating Epoch:  64%|#033[32m██████▍   #033[0m| 18/28 [01:02<00:34,  3.47s/it]\u001B[0m\n",
      "\u001B[34mevaluating Epoch:  64%|#033[32m██████▍   #033[0m| 18/28 [01:02<00:34,  3.47s/it]\u001B[0m\n",
      "\u001B[35mevaluating Epoch:  64%|#033[32m██████▍   #033[0m| 18/28 [01:02<00:34,  3.46s/it]\u001B[0m\n",
      "\u001B[35mevaluating Epoch:  64%|#033[32m██████▍   #033[0m| 18/28 [01:02<00:34,  3.46s/it]\u001B[0m\n",
      "\u001B[35mevaluating Epoch:  64%|#033[32m██████▍   #033[0m| 18/28 [01:02<00:34,  3.46s/it]\u001B[0m\n",
      "\u001B[35mevaluating Epoch:  64%|#033[32m██████▍   #033[0m| 18/28 [01:02<00:34,  3.46s/it]\u001B[0m\n",
      "\u001B[34mevaluating Epoch:  68%|#033[32m██████▊   #033[0m| 19/28 [01:06<00:31,  3.47s/it]\u001B[0m\n",
      "\u001B[34mevaluating Epoch:  68%|#033[32m██████▊   #033[0m| 19/28 [01:06<00:31,  3.47s/it]\u001B[0m\n",
      "\u001B[34mevaluating Epoch:  68%|#033[32m██████▊   #033[0m| 19/28 [01:06<00:31,  3.47s/it]\u001B[0m\n",
      "\u001B[34mevaluating Epoch:  68%|#033[32m██████▊   #033[0m| 19/28 [01:06<00:31,  3.47s/it]\u001B[0m\n",
      "\u001B[35mevaluating Epoch:  68%|#033[32m██████▊   #033[0m| 19/28 [01:05<00:31,  3.46s/it]#015evaluating Epoch:  68%|#033[32m██████▊   #033[0m| 19/28 [01:05<00:31,  3.46s/it]\u001B[0m\n",
      "\u001B[35mevaluating Epoch:  68%|#033[32m██████▊   #033[0m| 19/28 [01:05<00:31,  3.46s/it]\u001B[0m\n",
      "\u001B[35mevaluating Epoch:  68%|#033[32m██████▊   #033[0m| 19/28 [01:05<00:31,  3.46s/it]\u001B[0m\n",
      "\u001B[34mevaluating Epoch:  71%|#033[32m███████▏  #033[0m| 20/28 [01:09<00:27,  3.47s/it]#015evaluating Epoch:  71%|#033[32m███████▏  #033[0m| 20/28 [01:09<00:27,  3.47s/it]\u001B[0m\n",
      "\u001B[34mevaluating Epoch:  71%|#033[32m███████▏  #033[0m| 20/28 [01:09<00:27,  3.47s/it]\u001B[0m\n",
      "\u001B[34mevaluating Epoch:  71%|#033[32m███████▏  #033[0m| 20/28 [01:09<00:27,  3.47s/it]\u001B[0m\n",
      "\u001B[35mevaluating Epoch:  71%|#033[32m███████▏  #033[0m| 20/28 [01:09<00:27,  3.46s/it]\u001B[0m\n",
      "\u001B[35mevaluating Epoch:  71%|#033[32m███████▏  #033[0m| 20/28 [01:09<00:27,  3.46s/it]#015evaluating Epoch:  71%|#033[32m███████▏  #033[0m| 20/28 [01:09<00:27,  3.46s/it]#015evaluating Epoch:  71%|#033[32m███████▏  #033[0m| 20/28 [01:09<00:27,  3.46s/it]\u001B[0m\n",
      "\u001B[34mevaluating Epoch:  75%|#033[32m███████▌  #033[0m| 21/28 [01:12<00:24,  3.47s/it]\u001B[0m\n",
      "\u001B[34mevaluating Epoch:  75%|#033[32m███████▌  #033[0m| 21/28 [01:13<00:24,  3.47s/it]\u001B[0m\n",
      "\u001B[34mevaluating Epoch:  75%|#033[32m███████▌  #033[0m| 21/28 [01:13<00:24,  3.47s/it]\u001B[0m\n",
      "\u001B[34mevaluating Epoch:  75%|#033[32m███████▌  #033[0m| 21/28 [01:13<00:24,  3.47s/it]\u001B[0m\n",
      "\u001B[35mevaluating Epoch:  75%|#033[32m███████▌  #033[0m| 21/28 [01:12<00:24,  3.46s/it]#015evaluating Epoch:  75%|#033[32m███████▌  #033[0m| 21/28 [01:12<00:24,  3.46s/it]#015evaluating Epoch:  75%|#033[32m███████▌  #033[0m| 21/28 [01:12<00:24,  3.46s/it]\u001B[0m\n",
      "\u001B[35mevaluating Epoch:  75%|#033[32m███████▌  #033[0m| 21/28 [01:12<00:24,  3.46s/it]\u001B[0m\n",
      "\u001B[34mevaluating Epoch:  79%|#033[32m███████▊  #033[0m| 22/28 [01:16<00:20,  3.47s/it]\u001B[0m\n",
      "\u001B[34mevaluating Epoch:  79%|#033[32m███████▊  #033[0m| 22/28 [01:16<00:20,  3.47s/it]\u001B[0m\n",
      "\u001B[34mevaluating Epoch:  79%|#033[32m███████▊  #033[0m| 22/28 [01:16<00:20,  3.47s/it]#015evaluating Epoch:  79%|#033[32m███████▊  #033[0m| 22/28 [01:16<00:20,  3.47s/it]\u001B[0m\n",
      "\u001B[35mevaluating Epoch:  79%|#033[32m███████▊  #033[0m| 22/28 [01:16<00:20,  3.46s/it]\u001B[0m\n",
      "\u001B[35mevaluating Epoch:  79%|#033[32m███████▊  #033[0m| 22/28 [01:16<00:20,  3.46s/it]#015evaluating Epoch:  79%|#033[32m███████▊  #033[0m| 22/28 [01:16<00:20,  3.46s/it]\u001B[0m\n",
      "\u001B[35mevaluating Epoch:  79%|#033[32m███████▊  #033[0m| 22/28 [01:16<00:20,  3.46s/it]\u001B[0m\n",
      "\u001B[34mevaluating Epoch:  82%|#033[32m████████▏ #033[0m| 23/28 [01:19<00:17,  3.47s/it]\u001B[0m\n",
      "\u001B[34mevaluating Epoch:  82%|#033[32m████████▏ #033[0m| 23/28 [01:19<00:17,  3.47s/it]\u001B[0m\n",
      "\u001B[34mevaluating Epoch:  82%|#033[32m████████▏ #033[0m| 23/28 [01:19<00:17,  3.47s/it]\u001B[0m\n",
      "\u001B[34mevaluating Epoch:  82%|#033[32m████████▏ #033[0m| 23/28 [01:19<00:17,  3.47s/it]\u001B[0m\n",
      "\u001B[35mevaluating Epoch:  82%|#033[32m████████▏ #033[0m| 23/28 [01:19<00:17,  3.46s/it]\u001B[0m\n",
      "\u001B[35mevaluating Epoch:  82%|#033[32m████████▏ #033[0m| 23/28 [01:19<00:17,  3.46s/it]\u001B[0m\n",
      "\u001B[35mevaluating Epoch:  82%|#033[32m████████▏ #033[0m| 23/28 [01:19<00:17,  3.46s/it]\u001B[0m\n",
      "\u001B[35mevaluating Epoch:  82%|#033[32m████████▏ #033[0m| 23/28 [01:19<00:17,  3.46s/it]\u001B[0m\n",
      "\u001B[34mevaluating Epoch:  86%|#033[32m████████▌ #033[0m| 24/28 [01:23<00:13,  3.47s/it]\u001B[0m\n",
      "\u001B[34mevaluating Epoch:  86%|#033[32m████████▌ #033[0m| 24/28 [01:23<00:13,  3.47s/it]\u001B[0m\n",
      "\u001B[34mevaluating Epoch:  86%|#033[32m████████▌ #033[0m| 24/28 [01:23<00:13,  3.47s/it]\u001B[0m\n",
      "\u001B[34mevaluating Epoch:  86%|#033[32m████████▌ #033[0m| 24/28 [01:23<00:13,  3.47s/it]\u001B[0m\n",
      "\u001B[35mevaluating Epoch:  86%|#033[32m████████▌ #033[0m| 24/28 [01:23<00:13,  3.46s/it]#015evaluating Epoch:  86%|#033[32m████████▌ #033[0m| 24/28 [01:23<00:13,  3.46s/it]\u001B[0m\n",
      "\u001B[35mevaluating Epoch:  86%|#033[32m████████▌ #033[0m| 24/28 [01:23<00:13,  3.46s/it]\u001B[0m\n",
      "\u001B[35mevaluating Epoch:  86%|#033[32m████████▌ #033[0m| 24/28 [01:23<00:13,  3.46s/it]\u001B[0m\n",
      "\u001B[34mevaluating Epoch:  89%|#033[32m████████▉ #033[0m| 25/28 [01:26<00:10,  3.47s/it]\u001B[0m\n",
      "\u001B[34mevaluating Epoch:  89%|#033[32m████████▉ #033[0m| 25/28 [01:26<00:10,  3.47s/it]\u001B[0m\n",
      "\u001B[34mevaluating Epoch:  89%|#033[32m████████▉ #033[0m| 25/28 [01:26<00:10,  3.47s/it]\u001B[0m\n",
      "\u001B[34mevaluating Epoch:  89%|#033[32m████████▉ #033[0m| 25/28 [01:26<00:10,  3.47s/it]\u001B[0m\n",
      "\u001B[35mevaluating Epoch:  89%|#033[32m████████▉ #033[0m| 25/28 [01:26<00:10,  3.46s/it]\u001B[0m\n",
      "\u001B[35mevaluating Epoch:  89%|#033[32m████████▉ #033[0m| 25/28 [01:26<00:10,  3.46s/it]\u001B[0m\n",
      "\u001B[35mevaluating Epoch:  89%|#033[32m████████▉ #033[0m| 25/28 [01:26<00:10,  3.46s/it]\u001B[0m\n",
      "\u001B[35mevaluating Epoch:  89%|#033[32m████████▉ #033[0m| 25/28 [01:26<00:10,  3.46s/it]\u001B[0m\n",
      "\u001B[34mevaluating Epoch:  93%|#033[32m█████████▎#033[0m| 26/28 [01:30<00:06,  3.47s/it]#015evaluating Epoch:  93%|#033[32m█████████▎#033[0m| 26/28 [01:30<00:06,  3.47s/it]#015evaluating Epoch:  93%|#033[32m█████████▎#033[0m| 26/28 [01:30<00:06,  3.47s/it]\u001B[0m\n",
      "\u001B[34mevaluating Epoch:  93%|#033[32m█████████▎#033[0m| 26/28 [01:30<00:06,  3.47s/it]\u001B[0m\n",
      "\u001B[35mevaluating Epoch:  93%|#033[32m█████████▎#033[0m| 26/28 [01:30<00:06,  3.46s/it]#015evaluating Epoch:  93%|#033[32m█████████▎#033[0m| 26/28 [01:30<00:06,  3.46s/it]\u001B[0m\n",
      "\u001B[35mevaluating Epoch:  93%|#033[32m█████████▎#033[0m| 26/28 [01:30<00:06,  3.46s/it]\u001B[0m\n",
      "\u001B[35mevaluating Epoch:  93%|#033[32m█████████▎#033[0m| 26/28 [01:30<00:06,  3.46s/it]\u001B[0m\n",
      "\u001B[34mevaluating Epoch:  96%|#033[32m█████████▋#033[0m| 27/28 [01:33<00:03,  3.47s/it]#015evaluating Epoch:  96%|#033[32m█████████▋#033[0m| 27/28 [01:33<00:03,  3.47s/it]\u001B[0m\n",
      "\u001B[34mevaluating Epoch:  96%|#033[32m█████████▋#033[0m| 27/28 [01:33<00:03,  3.47s/it]\u001B[0m\n",
      "\u001B[34mevaluating Epoch:  96%|#033[32m█████████▋#033[0m| 27/28 [01:33<00:03,  3.47s/it]\u001B[0m\n",
      "\u001B[35mevaluating Epoch:  96%|#033[32m█████████▋#033[0m| 27/28 [01:33<00:03,  3.46s/it]\u001B[0m\n",
      "\u001B[35mevaluating Epoch:  96%|#033[32m█████████▋#033[0m| 27/28 [01:33<00:03,  3.46s/it]\u001B[0m\n",
      "\u001B[35mevaluating Epoch:  96%|#033[32m█████████▋#033[0m| 27/28 [01:33<00:03,  3.46s/it]\u001B[0m\n",
      "\u001B[35mevaluating Epoch:  96%|#033[32m█████████▋#033[0m| 27/28 [01:33<00:03,  3.46s/it]\u001B[0m\n",
      "\u001B[34mevaluating Epoch: 100%|#033[32m██████████#033[0m| 28/28 [01:37<00:00,  3.47s/it]\u001B[0m\n",
      "\u001B[34mevaluating Epoch: 100%|#033[32m██████████#033[0m| 28/28 [01:37<00:00,  3.47s/it]#015evaluating Epoch: 100%|#033[32m██████████#033[0m| 28/28 [01:37<00:00,  3.47s/it]#015evaluating Epoch: 100%|#033[32m██████████#033[0m| 28/28 [01:37<00:00,  3.48s/it]\u001B[0m\n",
      "\u001B[34mevaluating Epoch: 100%|#033[32m██████████#033[0m| 28/28 [01:37<00:00,  3.47s/it]\u001B[0m\n",
      "\u001B[34mevaluating Epoch: 100%|#033[32m██████████#033[0m| 28/28 [01:37<00:00,  3.48s/it]\u001B[0m\n",
      "\u001B[34mevaluating Epoch: 100%|#033[32m██████████#033[0m| 28/28 [01:37<00:00,  3.48s/it]\u001B[0m\n",
      "\u001B[34mevaluating Epoch: 100%|#033[32m██████████#033[0m| 28/28 [01:37<00:00,  3.48s/it]\u001B[0m\n",
      "\u001B[34meval_ppl=tensor(3.7397, device='cuda:0') eval_epoch_loss=tensor(1.3190, device='cuda:0')\u001B[0m\n",
      "\u001B[34mwe are about to save the PEFT modules\u001B[0m\n",
      "\u001B[35mevaluating Epoch: 100%|#033[32m██████████#033[0m| 28/28 [01:37<00:00,  3.47s/it]\u001B[0m\n",
      "\u001B[35mevaluating Epoch: 100%|#033[32m██████████#033[0m| 28/28 [01:37<00:00,  3.46s/it]\u001B[0m\n",
      "\u001B[35mevaluating Epoch: 100%|#033[32m██████████#033[0m| 28/28 [01:37<00:00,  3.47s/it]\u001B[0m\n",
      "\u001B[35mevaluating Epoch: 100%|#033[32m██████████#033[0m| 28/28 [01:37<00:00,  3.46s/it]\u001B[0m\n",
      "\u001B[35m#015evaluating Epoch: 100%|#033[32m██████████#033[0m| 28/28 [01:37<00:00,  3.47s/it]\u001B[0m\n",
      "\u001B[35mevaluating Epoch: 100%|#033[32m██████████#033[0m| 28/28 [01:37<00:00,  3.46s/it]\u001B[0m\n",
      "\u001B[35mevaluating Epoch: 100%|#033[32m██████████#033[0m| 28/28 [01:37<00:00,  3.47s/it]\u001B[0m\n",
      "\u001B[35mevaluating Epoch: 100%|#033[32m██████████#033[0m| 28/28 [01:37<00:00,  3.46s/it]\u001B[0m\n",
      "\u001B[35meval_ppl=tensor(3.7397, device='cuda:0') eval_epoch_loss=tensor(1.3190, device='cuda:0')\u001B[0m\n",
      "\u001B[35mwe are about to save the PEFT modules\u001B[0m\n",
      "\u001B[34mPEFT modules are saved in saved_peft_model directory\u001B[0m\n",
      "\u001B[34mbest eval loss on epoch 0 is 1.319017767906189\u001B[0m\n",
      "\u001B[34mEpoch 1: train_perplexity=4.1531, train_epoch_loss=1.4238, epcoh time 266.3525361190001s\u001B[0m\n",
      "\u001B[34mTraining Epoch1:   0%|#033[34m          #033[0m| 0/27 [00:00<?, ?it/s]\u001B[0m\n",
      "\u001B[34mTraining Epoch1:   0%|#033[34m          #033[0m| 0/27 [00:00<?, ?it/s]\u001B[0m\n",
      "\u001B[34mTraining Epoch1:   0%|#033[34m          #033[0m| 0/27 [00:00<?, ?it/s]#015Training Epoch1:   0%|#033[34m          #033[0m| 0/27 [00:00<?, ?it/s]\u001B[0m\n",
      "\u001B[35mPEFT modules are saved in saved_peft_model directory\u001B[0m\n",
      "\u001B[35mbest eval loss on epoch 0 is 1.319017767906189\u001B[0m\n",
      "\u001B[35mEpoch 1: train_perplexity=4.1531, train_epoch_loss=1.4238, epcoh time 270.29995143099995s\u001B[0m\n",
      "\u001B[35mTraining Epoch1:   0%|#033[34m          #033[0m| 0/27 [00:00<?, ?it/s]\u001B[0m\n",
      "\u001B[35mTraining Epoch1:   0%|#033[34m          #033[0m| 0/27 [00:00<?, ?it/s]\u001B[0m\n",
      "\u001B[35mTraining Epoch1:   0%|#033[34m          #033[0m| 0/27 [00:00<?, ?it/s]\u001B[0m\n",
      "\u001B[35mTraining Epoch1:   0%|#033[34m          #033[0m| 0/27 [00:00<?, ?it/s]\u001B[0m\n",
      "\u001B[34mTraining Epoch1:   4%|#033[34m▎         #033[0m| 1/27 [00:09<04:13,  9.75s/it]\u001B[0m\n",
      "\u001B[34mTraining Epoch1:   4%|#033[34m▎         #033[0m| 1/27 [00:09<04:13,  9.75s/it]\u001B[0m\n",
      "\u001B[34mTraining Epoch1:   4%|#033[34m▎         #033[0m| 1/27 [00:09<04:13,  9.75s/it]\u001B[0m\n",
      "\u001B[34mstep 0 is completed and loss is 1.4256399869918823\u001B[0m\n",
      "\u001B[34mTraining Epoch1:   4%|#033[34m▎         #033[0m| 1/27 [00:09<04:13,  9.75s/it]\u001B[0m\n",
      "\u001B[35mTraining Epoch1:   4%|#033[34m▎         #033[0m| 1/27 [00:09<04:13,  9.76s/it]\u001B[0m\n",
      "\u001B[35mstep 0 is completed and loss is 1.4256399869918823\u001B[0m\n",
      "\u001B[35mTraining Epoch1:   4%|#033[34m▎         #033[0m| 1/27 [00:09<04:13,  9.77s/it]\u001B[0m\n",
      "\u001B[35mTraining Epoch1:   4%|#033[34m▎         #033[0m| 1/27 [00:09<04:13,  9.77s/it]\u001B[0m\n",
      "\u001B[35mTraining Epoch1:   4%|#033[34m▎         #033[0m| 1/27 [00:09<04:13,  9.77s/it]\u001B[0m\n",
      "\u001B[34mTraining Epoch1:   7%|#033[34m▋         #033[0m| 2/27 [00:19<04:04,  9.78s/it]\u001B[0m\n",
      "\u001B[34mTraining Epoch1:   7%|#033[34m▋         #033[0m| 2/27 [00:19<04:04,  9.78s/it]\u001B[0m\n",
      "\u001B[34mstep 1 is completed and loss is 1.0932499170303345\u001B[0m\n",
      "\u001B[34mTraining Epoch1:   7%|#033[34m▋         #033[0m| 2/27 [00:19<04:04,  9.78s/it]\u001B[0m\n",
      "\u001B[34mTraining Epoch1:   7%|#033[34m▋         #033[0m| 2/27 [00:19<04:04,  9.78s/it]\u001B[0m\n",
      "\u001B[35mTraining Epoch1:   7%|#033[34m▋         #033[0m| 2/27 [00:19<04:04,  9.79s/it]\u001B[0m\n",
      "\u001B[35mstep 1 is completed and loss is 1.0932499170303345\u001B[0m\n",
      "\u001B[35mTraining Epoch1:   7%|#033[34m▋         #033[0m| 2/27 [00:19<04:04,  9.79s/it]\u001B[0m\n",
      "\u001B[35mTraining Epoch1:   7%|#033[34m▋         #033[0m| 2/27 [00:19<04:04,  9.80s/it]\u001B[0m\n",
      "\u001B[35mTraining Epoch1:   7%|#033[34m▋         #033[0m| 2/27 [00:19<04:04,  9.80s/it]\u001B[0m\n",
      "\u001B[34mTraining Epoch1:  11%|#033[34m█         #033[0m| 3/27 [00:29<03:55,  9.80s/it]\u001B[0m\n",
      "\u001B[34mstep 2 is completed and loss is 1.4904955625534058\u001B[0m\n",
      "\u001B[34mTraining Epoch1:  11%|#033[34m█         #033[0m| 3/27 [00:29<03:55,  9.80s/it]\u001B[0m\n",
      "\u001B[34mTraining Epoch1:  11%|#033[34m█         #033[0m| 3/27 [00:29<03:55,  9.80s/it]\u001B[0m\n",
      "\u001B[34mTraining Epoch1:  11%|#033[34m█         #033[0m| 3/27 [00:29<03:55,  9.80s/it]\u001B[0m\n",
      "\u001B[35mTraining Epoch1:  11%|#033[34m█         #033[0m| 3/27 [00:29<03:55,  9.80s/it]\u001B[0m\n",
      "\u001B[35mTraining Epoch1:  11%|#033[34m█         #033[0m| 3/27 [00:29<03:55,  9.80s/it]\u001B[0m\n",
      "\u001B[35mTraining Epoch1:  11%|#033[34m█         #033[0m| 3/27 [00:29<03:55,  9.80s/it]\u001B[0m\n",
      "\u001B[35mstep 2 is completed and loss is 1.4904955625534058\u001B[0m\n",
      "\u001B[35mTraining Epoch1:  11%|#033[34m█         #033[0m| 3/27 [00:29<03:55,  9.80s/it]\u001B[0m\n",
      "\u001B[34mTraining Epoch1:  15%|#033[34m█▍        #033[0m| 4/27 [00:39<03:45,  9.80s/it]\u001B[0m\n",
      "\u001B[34mstep 3 is completed and loss is 1.213043212890625\u001B[0m\n",
      "\u001B[34mTraining Epoch1:  15%|#033[34m█▍        #033[0m| 4/27 [00:39<03:45,  9.80s/it]\u001B[0m\n",
      "\u001B[34mTraining Epoch1:  15%|#033[34m█▍        #033[0m| 4/27 [00:39<03:45,  9.80s/it]\u001B[0m\n",
      "\u001B[34mTraining Epoch1:  15%|#033[34m█▍        #033[0m| 4/27 [00:39<03:45,  9.80s/it]\u001B[0m\n",
      "\u001B[35mTraining Epoch1:  15%|#033[34m█▍        #033[0m| 4/27 [00:39<03:45,  9.80s/it]\u001B[0m\n",
      "\u001B[35mTraining Epoch1:  15%|#033[34m█▍        #033[0m| 4/27 [00:39<03:45,  9.80s/it]\u001B[0m\n",
      "\u001B[35mTraining Epoch1:  15%|#033[34m█▍        #033[0m| 4/27 [00:39<03:45,  9.80s/it]\u001B[0m\n",
      "\u001B[35mstep 3 is completed and loss is 1.213043212890625\u001B[0m\n",
      "\u001B[35mTraining Epoch1:  15%|#033[34m█▍        #033[0m| 4/27 [00:39<03:45,  9.80s/it]\u001B[0m\n",
      "\u001B[34mstep 4 is completed and loss is 1.214312195777893\u001B[0m\n",
      "\u001B[34mTraining Epoch1:  19%|#033[34m█▊        #033[0m| 5/27 [00:48<03:35,  9.80s/it]\u001B[0m\n",
      "\u001B[34mTraining Epoch1:  19%|#033[34m█▊        #033[0m| 5/27 [00:48<03:35,  9.80s/it]\u001B[0m\n",
      "\u001B[34mTraining Epoch1:  19%|#033[34m█▊        #033[0m| 5/27 [00:48<03:35,  9.81s/it]\u001B[0m\n",
      "\u001B[34mTraining Epoch1:  19%|#033[34m█▊        #033[0m| 5/27 [00:48<03:35,  9.81s/it]\u001B[0m\n",
      "\u001B[35mTraining Epoch1:  19%|#033[34m█▊        #033[0m| 5/27 [00:48<03:35,  9.80s/it]\u001B[0m\n",
      "\u001B[35mstep 4 is completed and loss is 1.214312195777893\u001B[0m\n",
      "\u001B[35mTraining Epoch1:  19%|#033[34m█▊        #033[0m| 5/27 [00:48<03:35,  9.80s/it]\u001B[0m\n",
      "\u001B[35mTraining Epoch1:  19%|#033[34m█▊        #033[0m| 5/27 [00:48<03:35,  9.80s/it]\u001B[0m\n",
      "\u001B[35mTraining Epoch1:  19%|#033[34m█▊        #033[0m| 5/27 [00:48<03:35,  9.80s/it]\u001B[0m\n",
      "\u001B[34mstep 5 is completed and loss is 1.299070954322815\u001B[0m\n",
      "\u001B[34mTraining Epoch1:  22%|#033[34m██▏       #033[0m| 6/27 [00:58<03:25,  9.81s/it]\u001B[0m\n",
      "\u001B[34mTraining Epoch1:  22%|#033[34m██▏       #033[0m| 6/27 [00:58<03:25,  9.81s/it]\u001B[0m\n",
      "\u001B[34mTraining Epoch1:  22%|#033[34m██▏       #033[0m| 6/27 [00:58<03:25,  9.81s/it]\u001B[0m\n",
      "\u001B[34mTraining Epoch1:  22%|#033[34m██▏       #033[0m| 6/27 [00:58<03:25,  9.81s/it]\u001B[0m\n",
      "\u001B[35mTraining Epoch1:  22%|#033[34m██▏       #033[0m| 6/27 [00:58<03:25,  9.80s/it]\u001B[0m\n",
      "\u001B[35mstep 5 is completed and loss is 1.299070954322815\u001B[0m\n",
      "\u001B[35mTraining Epoch1:  22%|#033[34m██▏       #033[0m| 6/27 [00:58<03:25,  9.80s/it]\u001B[0m\n",
      "\u001B[35mTraining Epoch1:  22%|#033[34m██▏       #033[0m| 6/27 [00:58<03:25,  9.80s/it]\u001B[0m\n",
      "\u001B[35mTraining Epoch1:  22%|#033[34m██▏       #033[0m| 6/27 [00:58<03:25,  9.80s/it]\u001B[0m\n",
      "\u001B[34mTraining Epoch1:  26%|#033[34m██▌       #033[0m| 7/27 [01:08<03:16,  9.81s/it]#015Training Epoch1:  26%|#033[34m██▌       #033[0m| 7/27 [01:08<03:16,  9.81s/it]\u001B[0m\n",
      "\u001B[34mTraining Epoch1:  26%|#033[34m██▌       #033[0m| 7/27 [01:08<03:16,  9.81s/it]\u001B[0m\n",
      "\u001B[34mstep 6 is completed and loss is 1.5416914224624634\u001B[0m\n",
      "\u001B[34mTraining Epoch1:  26%|#033[34m██▌       #033[0m| 7/27 [01:08<03:16,  9.81s/it]\u001B[0m\n",
      "\u001B[35mTraining Epoch1:  26%|#033[34m██▌       #033[0m| 7/27 [01:08<03:16,  9.80s/it]\u001B[0m\n",
      "\u001B[35mTraining Epoch1:  26%|#033[34m██▌       #033[0m| 7/27 [01:08<03:16,  9.80s/it]\u001B[0m\n",
      "\u001B[35mstep 6 is completed and loss is 1.5416914224624634\u001B[0m\n",
      "\u001B[35mTraining Epoch1:  26%|#033[34m██▌       #033[0m| 7/27 [01:08<03:16,  9.80s/it]\u001B[0m\n",
      "\u001B[35mTraining Epoch1:  26%|#033[34m██▌       #033[0m| 7/27 [01:08<03:16,  9.80s/it]\u001B[0m\n",
      "\u001B[34mTraining Epoch1:  30%|#033[34m██▉       #033[0m| 8/27 [01:18<03:06,  9.81s/it]#015Training Epoch1:  30%|#033[34m██▉       #033[0m| 8/27 [01:18<03:06,  9.81s/it]\u001B[0m\n",
      "\u001B[34mTraining Epoch1:  30%|#033[34m██▉       #033[0m| 8/27 [01:18<03:06,  9.81s/it]\u001B[0m\n",
      "\u001B[34mstep 7 is completed and loss is 1.395768404006958\u001B[0m\n",
      "\u001B[34mTraining Epoch1:  30%|#033[34m██▉       #033[0m| 8/27 [01:18<03:06,  9.81s/it]\u001B[0m\n",
      "\u001B[35mstep 7 is completed and loss is 1.395768404006958\u001B[0m\n",
      "\u001B[35mTraining Epoch1:  30%|#033[34m██▉       #033[0m| 8/27 [01:18<03:06,  9.81s/it]\u001B[0m\n",
      "\u001B[35mTraining Epoch1:  30%|#033[34m██▉       #033[0m| 8/27 [01:18<03:06,  9.81s/it]\u001B[0m\n",
      "\u001B[35mTraining Epoch1:  30%|#033[34m██▉       #033[0m| 8/27 [01:18<03:06,  9.81s/it]\u001B[0m\n",
      "\u001B[35mTraining Epoch1:  30%|#033[34m██▉       #033[0m| 8/27 [01:18<03:06,  9.81s/it]\u001B[0m\n",
      "\u001B[34mTraining Epoch1:  33%|#033[34m███▎      #033[0m| 9/27 [01:28<02:56,  9.81s/it]#015Training Epoch1:  33%|#033[34m███▎      #033[0m| 9/27 [01:28<02:56,  9.81s/it]\u001B[0m\n",
      "\u001B[34mTraining Epoch1:  33%|#033[34m███▎      #033[0m| 9/27 [01:28<02:56,  9.81s/it]\u001B[0m\n",
      "\u001B[34mstep 8 is completed and loss is 1.5869824886322021\u001B[0m\n",
      "\u001B[34mTraining Epoch1:  33%|#033[34m███▎      #033[0m| 9/27 [01:28<02:56,  9.81s/it]\u001B[0m\n",
      "\u001B[35mstep 8 is completed and loss is 1.5869824886322021\u001B[0m\n",
      "\u001B[35mTraining Epoch1:  33%|#033[34m███▎      #033[0m| 9/27 [01:28<02:56,  9.81s/it]\u001B[0m\n",
      "\u001B[35mTraining Epoch1:  33%|#033[34m███▎      #033[0m| 9/27 [01:28<02:56,  9.81s/it]\u001B[0m\n",
      "\u001B[35mTraining Epoch1:  33%|#033[34m███▎      #033[0m| 9/27 [01:28<02:56,  9.81s/it]\u001B[0m\n",
      "\u001B[35mTraining Epoch1:  33%|#033[34m███▎      #033[0m| 9/27 [01:28<02:56,  9.81s/it]\u001B[0m\n",
      "\u001B[34mTraining Epoch1:  37%|#033[34m███▋      #033[0m| 10/27 [01:38<02:46,  9.81s/it]\u001B[0m\n",
      "\u001B[34mstep 9 is completed and loss is 1.1647844314575195\u001B[0m\n",
      "\u001B[34mTraining Epoch1:  37%|#033[34m███▋      #033[0m| 10/27 [01:38<02:46,  9.81s/it]\u001B[0m\n",
      "\u001B[34mTraining Epoch1:  37%|#033[34m███▋      #033[0m| 10/27 [01:38<02:46,  9.81s/it]\u001B[0m\n",
      "\u001B[34mTraining Epoch1:  37%|#033[34m███▋      #033[0m| 10/27 [01:38<02:46,  9.81s/it]\u001B[0m\n",
      "\u001B[35mTraining Epoch1:  37%|#033[34m███▋      #033[0m| 10/27 [01:38<02:46,  9.81s/it]\u001B[0m\n",
      "\u001B[35mTraining Epoch1:  37%|#033[34m███▋      #033[0m| 10/27 [01:38<02:46,  9.81s/it]\u001B[0m\n",
      "\u001B[35mTraining Epoch1:  37%|#033[34m███▋      #033[0m| 10/27 [01:38<02:46,  9.81s/it]\u001B[0m\n",
      "\u001B[35mstep 9 is completed and loss is 1.1647844314575195\u001B[0m\n",
      "\u001B[35mTraining Epoch1:  37%|#033[34m███▋      #033[0m| 10/27 [01:38<02:46,  9.81s/it]\u001B[0m\n",
      "\u001B[34mTraining Epoch1:  41%|#033[34m████      #033[0m| 11/27 [01:47<02:36,  9.81s/it]\u001B[0m\n",
      "\u001B[34mTraining Epoch1:  41%|#033[34m████      #033[0m| 11/27 [01:47<02:36,  9.81s/it]\u001B[0m\n",
      "\u001B[34mstep 10 is completed and loss is 1.2049232721328735\u001B[0m\n",
      "\u001B[34mTraining Epoch1:  41%|#033[34m████      #033[0m| 11/27 [01:47<02:36,  9.81s/it]#015Training Epoch1:  41%|#033[34m████      #033[0m| 11/27 [01:47<02:36,  9.81s/it]\u001B[0m\n",
      "\u001B[35mTraining Epoch1:  41%|#033[34m████      #033[0m| 11/27 [01:47<02:36,  9.81s/it]\u001B[0m\n",
      "\u001B[35mstep 10 is completed and loss is 1.2049232721328735\u001B[0m\n",
      "\u001B[35mTraining Epoch1:  41%|#033[34m████      #033[0m| 11/27 [01:47<02:36,  9.81s/it]\u001B[0m\n",
      "\u001B[35mTraining Epoch1:  41%|#033[34m████      #033[0m| 11/27 [01:47<02:36,  9.81s/it]\u001B[0m\n",
      "\u001B[35mTraining Epoch1:  41%|#033[34m████      #033[0m| 11/27 [01:47<02:36,  9.81s/it]\u001B[0m\n",
      "\u001B[34mstep 11 is completed and loss is 1.2345153093338013\u001B[0m\n",
      "\u001B[34mTraining Epoch1:  44%|#033[34m████▍     #033[0m| 12/27 [01:57<02:27,  9.81s/it]\u001B[0m\n",
      "\u001B[34mTraining Epoch1:  44%|#033[34m████▍     #033[0m| 12/27 [01:57<02:27,  9.81s/it]\u001B[0m\n",
      "\u001B[34mTraining Epoch1:  44%|#033[34m████▍     #033[0m| 12/27 [01:57<02:27,  9.81s/it]\u001B[0m\n",
      "\u001B[34mTraining Epoch1:  44%|#033[34m████▍     #033[0m| 12/27 [01:57<02:27,  9.81s/it]\u001B[0m\n",
      "\u001B[35mTraining Epoch1:  44%|#033[34m████▍     #033[0m| 12/27 [01:57<02:27,  9.82s/it]\u001B[0m\n",
      "\u001B[35mTraining Epoch1:  44%|#033[34m████▍     #033[0m| 12/27 [01:57<02:27,  9.82s/it]\u001B[0m\n",
      "\u001B[35mTraining Epoch1:  44%|#033[34m████▍     #033[0m| 12/27 [01:57<02:27,  9.82s/it]\u001B[0m\n",
      "\u001B[35mstep 11 is completed and loss is 1.2345153093338013\u001B[0m\n",
      "\u001B[35mTraining Epoch1:  44%|#033[34m████▍     #033[0m| 12/27 [01:57<02:27,  9.82s/it]\u001B[0m\n",
      "\u001B[34mTraining Epoch1:  48%|#033[34m████▊     #033[0m| 13/27 [02:07<02:17,  9.85s/it]\u001B[0m\n",
      "\u001B[34mstep 12 is completed and loss is 1.32742178440094\u001B[0m\n",
      "\u001B[34mTraining Epoch1:  48%|#033[34m████▊     #033[0m| 13/27 [02:07<02:17,  9.85s/it]\u001B[0m\n",
      "\u001B[34mTraining Epoch1:  48%|#033[34m████▊     #033[0m| 13/27 [02:07<02:17,  9.85s/it]\u001B[0m\n",
      "\u001B[34mTraining Epoch1:  48%|#033[34m████▊     #033[0m| 13/27 [02:07<02:17,  9.85s/it]\u001B[0m\n",
      "\u001B[35mTraining Epoch1:  48%|#033[34m████▊     #033[0m| 13/27 [02:07<02:18,  9.89s/it]\u001B[0m\n",
      "\u001B[35mstep 12 is completed and loss is 1.32742178440094\u001B[0m\n",
      "\u001B[35mTraining Epoch1:  48%|#033[34m████▊     #033[0m| 13/27 [02:07<02:18,  9.89s/it]\u001B[0m\n",
      "\u001B[35mTraining Epoch1:  48%|#033[34m████▊     #033[0m| 13/27 [02:07<02:18,  9.89s/it]\u001B[0m\n",
      "\u001B[35mTraining Epoch1:  48%|#033[34m████▊     #033[0m| 13/27 [02:07<02:18,  9.89s/it]\u001B[0m\n",
      "\u001B[34mTraining Epoch1:  52%|#033[34m█████▏    #033[0m| 14/27 [02:17<02:08,  9.87s/it]\u001B[0m\n",
      "\u001B[34mstep 13 is completed and loss is 1.0015766620635986\u001B[0m\n",
      "\u001B[34mTraining Epoch1:  52%|#033[34m█████▏    #033[0m| 14/27 [02:17<02:08,  9.87s/it]\u001B[0m\n",
      "\u001B[34mTraining Epoch1:  52%|#033[34m█████▏    #033[0m| 14/27 [02:17<02:08,  9.87s/it]\u001B[0m\n",
      "\u001B[34mTraining Epoch1:  52%|#033[34m█████▏    #033[0m| 14/27 [02:17<02:08,  9.87s/it]\u001B[0m\n",
      "\u001B[35mstep 13 is completed and loss is 1.0015766620635986\u001B[0m\n",
      "\u001B[35mTraining Epoch1:  52%|#033[34m█████▏    #033[0m| 14/27 [02:17<02:08,  9.86s/it]\u001B[0m\n",
      "\u001B[35mTraining Epoch1:  52%|#033[34m█████▏    #033[0m| 14/27 [02:17<02:08,  9.86s/it]\u001B[0m\n",
      "\u001B[35mTraining Epoch1:  52%|#033[34m█████▏    #033[0m| 14/27 [02:17<02:08,  9.86s/it]\u001B[0m\n",
      "\u001B[35mTraining Epoch1:  52%|#033[34m█████▏    #033[0m| 14/27 [02:17<02:08,  9.86s/it]\u001B[0m\n",
      "\u001B[34mTraining Epoch1:  56%|#033[34m█████▌    #033[0m| 15/27 [02:27<01:58,  9.85s/it]\u001B[0m\n",
      "\u001B[34mTraining Epoch1:  56%|#033[34m█████▌    #033[0m| 15/27 [02:27<01:58,  9.85s/it]#015Training Epoch1:  56%|#033[34m█████▌    #033[0m| 15/27 [02:27<01:58,  9.85s/it]\u001B[0m\n",
      "\u001B[34mstep 14 is completed and loss is 1.364158034324646\u001B[0m\n",
      "\u001B[34mTraining Epoch1:  56%|#033[34m█████▌    #033[0m| 15/27 [02:27<01:58,  9.85s/it]\u001B[0m\n",
      "\u001B[35mTraining Epoch1:  56%|#033[34m█████▌    #033[0m| 15/27 [02:27<01:58,  9.84s/it]\u001B[0m\n",
      "\u001B[35mTraining Epoch1:  56%|#033[34m█████▌    #033[0m| 15/27 [02:27<01:58,  9.84s/it]\u001B[0m\n",
      "\u001B[35mstep 14 is completed and loss is 1.364158034324646\u001B[0m\n",
      "\u001B[35mTraining Epoch1:  56%|#033[34m█████▌    #033[0m| 15/27 [02:27<01:58,  9.84s/it]\u001B[0m\n",
      "\u001B[35mTraining Epoch1:  56%|#033[34m█████▌    #033[0m| 15/27 [02:27<01:58,  9.85s/it]\u001B[0m\n",
      "\u001B[34mstep 15 is completed and loss is 1.473850131034851\u001B[0m\n",
      "\u001B[34mTraining Epoch1:  59%|#033[34m█████▉    #033[0m| 16/27 [02:37<01:48,  9.84s/it]\u001B[0m\n",
      "\u001B[34mTraining Epoch1:  59%|#033[34m█████▉    #033[0m| 16/27 [02:37<01:48,  9.84s/it]\u001B[0m\n",
      "\u001B[34mTraining Epoch1:  59%|#033[34m█████▉    #033[0m| 16/27 [02:37<01:48,  9.84s/it]\u001B[0m\n",
      "\u001B[34mTraining Epoch1:  59%|#033[34m█████▉    #033[0m| 16/27 [02:37<01:48,  9.84s/it]\u001B[0m\n",
      "\u001B[35mTraining Epoch1:  59%|#033[34m█████▉    #033[0m| 16/27 [02:37<01:48,  9.83s/it]\u001B[0m\n",
      "\u001B[35mTraining Epoch1:  59%|#033[34m█████▉    #033[0m| 16/27 [02:37<01:48,  9.83s/it]\u001B[0m\n",
      "\u001B[35mTraining Epoch1:  59%|#033[34m█████▉    #033[0m| 16/27 [02:37<01:48,  9.83s/it]\u001B[0m\n",
      "\u001B[35mstep 15 is completed and loss is 1.473850131034851\u001B[0m\n",
      "\u001B[35mTraining Epoch1:  59%|#033[34m█████▉    #033[0m| 16/27 [02:37<01:48,  9.83s/it]\u001B[0m\n",
      "\u001B[34mstep 16 is completed and loss is 1.478633165359497\u001B[0m\n",
      "\u001B[34mTraining Epoch1:  63%|#033[34m██████▎   #033[0m| 17/27 [02:46<01:38,  9.83s/it]\u001B[0m\n",
      "\u001B[34mTraining Epoch1:  63%|#033[34m██████▎   #033[0m| 17/27 [02:46<01:38,  9.83s/it]\u001B[0m\n",
      "\u001B[34mTraining Epoch1:  63%|#033[34m██████▎   #033[0m| 17/27 [02:46<01:38,  9.83s/it]\u001B[0m\n",
      "\u001B[34mTraining Epoch1:  63%|#033[34m██████▎   #033[0m| 17/27 [02:46<01:38,  9.83s/it]\u001B[0m\n",
      "\u001B[35mTraining Epoch1:  63%|#033[34m██████▎   #033[0m| 17/27 [02:46<01:38,  9.82s/it]#015Training Epoch1:  63%|#033[34m██████▎   #033[0m| 17/27 [02:46<01:38,  9.82s/it]\u001B[0m\n",
      "\u001B[35mTraining Epoch1:  63%|#033[34m██████▎   #033[0m| 17/27 [02:46<01:38,  9.82s/it]\u001B[0m\n",
      "\u001B[35mstep 16 is completed and loss is 1.478633165359497\u001B[0m\n",
      "\u001B[35mTraining Epoch1:  63%|#033[34m██████▎   #033[0m| 17/27 [02:46<01:38,  9.82s/it]\u001B[0m\n",
      "\u001B[34mTraining Epoch1:  67%|#033[34m██████▋   #033[0m| 18/27 [02:56<01:28,  9.83s/it]#015Training Epoch1:  67%|#033[34m██████▋   #033[0m| 18/27 [02:56<01:28,  9.83s/it]\u001B[0m\n",
      "\u001B[34mstep 17 is completed and loss is 1.3875631093978882\u001B[0m\n",
      "\u001B[34mTraining Epoch1:  67%|#033[34m██████▋   #033[0m| 18/27 [02:56<01:28,  9.83s/it]#015Training Epoch1:  67%|#033[34m██████▋   #033[0m| 18/27 [02:56<01:28,  9.83s/it]\u001B[0m\n",
      "\u001B[35mTraining Epoch1:  67%|#033[34m██████▋   #033[0m| 18/27 [02:56<01:28,  9.82s/it]\u001B[0m\n",
      "\u001B[35mTraining Epoch1:  67%|#033[34m██████▋   #033[0m| 18/27 [02:56<01:28,  9.82s/it]\u001B[0m\n",
      "\u001B[35mstep 17 is completed and loss is 1.3875631093978882\u001B[0m\n",
      "\u001B[35mTraining Epoch1:  67%|#033[34m██████▋   #033[0m| 18/27 [02:56<01:28,  9.82s/it]\u001B[0m\n",
      "\u001B[35mTraining Epoch1:  67%|#033[34m██████▋   #033[0m| 18/27 [02:56<01:28,  9.82s/it]\u001B[0m\n",
      "\u001B[34mTraining Epoch1:  70%|#033[34m███████   #033[0m| 19/27 [03:06<01:18,  9.82s/it]\u001B[0m\n",
      "\u001B[34mTraining Epoch1:  70%|#033[34m███████   #033[0m| 19/27 [03:06<01:18,  9.82s/it]\u001B[0m\n",
      "\u001B[34mTraining Epoch1:  70%|#033[34m███████   #033[0m| 19/27 [03:06<01:18,  9.82s/it]\u001B[0m\n",
      "\u001B[34mstep 18 is completed and loss is 1.2659332752227783\u001B[0m\n",
      "\u001B[34mTraining Epoch1:  70%|#033[34m███████   #033[0m| 19/27 [03:06<01:18,  9.82s/it]\u001B[0m\n",
      "\u001B[35mTraining Epoch1:  70%|#033[34m███████   #033[0m| 19/27 [03:06<01:18,  9.82s/it]\u001B[0m\n",
      "\u001B[35mTraining Epoch1:  70%|#033[34m███████   #033[0m| 19/27 [03:06<01:18,  9.82s/it]\u001B[0m\n",
      "\u001B[35mstep 18 is completed and loss is 1.2659332752227783\u001B[0m\n",
      "\u001B[35mTraining Epoch1:  70%|#033[34m███████   #033[0m| 19/27 [03:06<01:18,  9.82s/it]\u001B[0m\n",
      "\u001B[35mTraining Epoch1:  70%|#033[34m███████   #033[0m| 19/27 [03:06<01:18,  9.82s/it]\u001B[0m\n",
      "\u001B[34mTraining Epoch1:  74%|#033[34m███████▍  #033[0m| 20/27 [03:16<01:08,  9.82s/it]\u001B[0m\n",
      "\u001B[34mTraining Epoch1:  74%|#033[34m███████▍  #033[0m| 20/27 [03:16<01:08,  9.82s/it]\u001B[0m\n",
      "\u001B[34mstep 19 is completed and loss is 1.1619799137115479\u001B[0m\n",
      "\u001B[34mTraining Epoch1:  74%|#033[34m███████▍  #033[0m| 20/27 [03:16<01:08,  9.82s/it]\u001B[0m\n",
      "\u001B[34mTraining Epoch1:  74%|#033[34m███████▍  #033[0m| 20/27 [03:16<01:08,  9.82s/it]\u001B[0m\n",
      "\u001B[35mTraining Epoch1:  74%|#033[34m███████▍  #033[0m| 20/27 [03:16<01:08,  9.82s/it]\u001B[0m\n",
      "\u001B[35mTraining Epoch1:  74%|#033[34m███████▍  #033[0m| 20/27 [03:16<01:08,  9.82s/it]\u001B[0m\n",
      "\u001B[35mstep 19 is completed and loss is 1.1619799137115479\u001B[0m\n",
      "\u001B[35mTraining Epoch1:  74%|#033[34m███████▍  #033[0m| 20/27 [03:16<01:08,  9.82s/it]#015Training Epoch1:  74%|#033[34m███████▍  #033[0m| 20/27 [03:16<01:08,  9.82s/it]\u001B[0m\n",
      "\u001B[34mTraining Epoch1:  78%|#033[34m███████▊  #033[0m| 21/27 [03:26<00:58,  9.82s/it]\u001B[0m\n",
      "\u001B[34mTraining Epoch1:  78%|#033[34m███████▊  #033[0m| 21/27 [03:26<00:58,  9.82s/it]\u001B[0m\n",
      "\u001B[34mstep 20 is completed and loss is 1.213156819343567\u001B[0m\n",
      "\u001B[34mTraining Epoch1:  78%|#033[34m███████▊  #033[0m| 21/27 [03:26<00:58,  9.82s/it]\u001B[0m\n",
      "\u001B[34mTraining Epoch1:  78%|#033[34m███████▊  #033[0m| 21/27 [03:26<00:58,  9.82s/it]\u001B[0m\n",
      "\u001B[35mTraining Epoch1:  78%|#033[34m███████▊  #033[0m| 21/27 [03:26<00:58,  9.82s/it]\u001B[0m\n",
      "\u001B[35mstep 20 is completed and loss is 1.213156819343567\u001B[0m\n",
      "\u001B[35mTraining Epoch1:  78%|#033[34m███████▊  #033[0m| 21/27 [03:26<00:58,  9.82s/it]#015Training Epoch1:  78%|#033[34m███████▊  #033[0m| 21/27 [03:26<00:58,  9.82s/it]\u001B[0m\n",
      "\u001B[35mTraining Epoch1:  78%|#033[34m███████▊  #033[0m| 21/27 [03:26<00:58,  9.82s/it]\u001B[0m\n",
      "\u001B[34mTraining Epoch1:  81%|#033[34m████████▏ #033[0m| 22/27 [03:36<00:49,  9.81s/it]\u001B[0m\n",
      "\u001B[34mTraining Epoch1:  81%|#033[34m████████▏ #033[0m| 22/27 [03:36<00:49,  9.81s/it]\u001B[0m\n",
      "\u001B[34mTraining Epoch1:  81%|#033[34m████████▏ #033[0m| 22/27 [03:36<00:49,  9.81s/it]\u001B[0m\n",
      "\u001B[34mstep 21 is completed and loss is 1.2617470026016235\u001B[0m\n",
      "\u001B[34mTraining Epoch1:  81%|#033[34m████████▏ #033[0m| 22/27 [03:36<00:49,  9.81s/it]\u001B[0m\n",
      "\u001B[35mTraining Epoch1:  81%|#033[34m████████▏ #033[0m| 22/27 [03:36<00:49,  9.82s/it]\u001B[0m\n",
      "\u001B[35mTraining Epoch1:  81%|#033[34m████████▏ #033[0m| 22/27 [03:36<00:49,  9.82s/it]#015Training Epoch1:  81%|#033[34m████████▏ #033[0m| 22/27 [03:36<00:49,  9.82s/it]\u001B[0m\n",
      "\u001B[35mstep 21 is completed and loss is 1.2617470026016235\u001B[0m\n",
      "\u001B[35mTraining Epoch1:  81%|#033[34m████████▏ #033[0m| 22/27 [03:36<00:49,  9.82s/it]\u001B[0m\n",
      "\u001B[34mTraining Epoch1:  85%|#033[34m████████▌ #033[0m| 23/27 [03:45<00:39,  9.81s/it]\u001B[0m\n",
      "\u001B[34mTraining Epoch1:  85%|#033[34m████████▌ #033[0m| 23/27 [03:45<00:39,  9.81s/it]\u001B[0m\n",
      "\u001B[34mTraining Epoch1:  85%|#033[34m████████▌ #033[0m| 23/27 [03:45<00:39,  9.81s/it]\u001B[0m\n",
      "\u001B[34mstep 22 is completed and loss is 1.3959481716156006\u001B[0m\n",
      "\u001B[34mTraining Epoch1:  85%|#033[34m████████▌ #033[0m| 23/27 [03:45<00:39,  9.81s/it]\u001B[0m\n",
      "\u001B[35mTraining Epoch1:  85%|#033[34m████████▌ #033[0m| 23/27 [03:45<00:39,  9.81s/it]\u001B[0m\n",
      "\u001B[35mTraining Epoch1:  85%|#033[34m████████▌ #033[0m| 23/27 [03:45<00:39,  9.81s/it]\u001B[0m\n",
      "\u001B[35mstep 22 is completed and loss is 1.3959481716156006\u001B[0m\n",
      "\u001B[35mTraining Epoch1:  85%|#033[34m████████▌ #033[0m| 23/27 [03:45<00:39,  9.81s/it]\u001B[0m\n",
      "\u001B[35mTraining Epoch1:  85%|#033[34m████████▌ #033[0m| 23/27 [03:45<00:39,  9.81s/it]\u001B[0m\n",
      "\u001B[34mTraining Epoch1:  89%|#033[34m████████▉ #033[0m| 24/27 [03:55<00:29,  9.81s/it]\u001B[0m\n",
      "\u001B[34mTraining Epoch1:  89%|#033[34m████████▉ #033[0m| 24/27 [03:55<00:29,  9.81s/it]\u001B[0m\n",
      "\u001B[34mTraining Epoch1:  89%|#033[34m████████▉ #033[0m| 24/27 [03:55<00:29,  9.81s/it]\u001B[0m\n",
      "\u001B[34mstep 23 is completed and loss is 1.2452287673950195\u001B[0m\n",
      "\u001B[34mTraining Epoch1:  89%|#033[34m████████▉ #033[0m| 24/27 [03:55<00:29,  9.81s/it]\u001B[0m\n",
      "\u001B[35mTraining Epoch1:  89%|#033[34m████████▉ #033[0m| 24/27 [03:55<00:29,  9.82s/it]\u001B[0m\n",
      "\u001B[35mstep 23 is completed and loss is 1.2452287673950195\u001B[0m\n",
      "\u001B[35mTraining Epoch1:  89%|#033[34m████████▉ #033[0m| 24/27 [03:55<00:29,  9.82s/it]\u001B[0m\n",
      "\u001B[35mTraining Epoch1:  89%|#033[34m████████▉ #033[0m| 24/27 [03:55<00:29,  9.82s/it]\u001B[0m\n",
      "\u001B[35mTraining Epoch1:  89%|#033[34m████████▉ #033[0m| 24/27 [03:55<00:29,  9.82s/it]\u001B[0m\n",
      "\u001B[34mTraining Epoch1:  93%|#033[34m█████████▎#033[0m| 25/27 [04:05<00:19,  9.83s/it]\u001B[0m\n",
      "\u001B[34mTraining Epoch1:  93%|#033[34m█████████▎#033[0m| 25/27 [04:05<00:19,  9.83s/it]\u001B[0m\n",
      "\u001B[34mstep 24 is completed and loss is 1.477048397064209\u001B[0m\n",
      "\u001B[34mTraining Epoch1:  93%|#033[34m█████████▎#033[0m| 25/27 [04:05<00:19,  9.83s/it]#015Training Epoch1:  93%|#033[34m█████████▎#033[0m| 25/27 [04:05<00:19,  9.83s/it]\u001B[0m\n",
      "\u001B[35mTraining Epoch1:  93%|#033[34m█████████▎#033[0m| 25/27 [04:05<00:19,  9.82s/it]\u001B[0m\n",
      "\u001B[35mTraining Epoch1:  93%|#033[34m█████████▎#033[0m| 25/27 [04:05<00:19,  9.82s/it]\u001B[0m\n",
      "\u001B[35mstep 24 is completed and loss is 1.477048397064209\u001B[0m\n",
      "\u001B[35mTraining Epoch1:  93%|#033[34m█████████▎#033[0m| 25/27 [04:05<00:19,  9.82s/it]\u001B[0m\n",
      "\u001B[35mTraining Epoch1:  93%|#033[34m█████████▎#033[0m| 25/27 [04:05<00:19,  9.82s/it]\u001B[0m\n",
      "\u001B[34mstep 25 is completed and loss is 1.1289381980895996\u001B[0m\n",
      "\u001B[34mTraining Epoch1:  96%|#033[34m█████████▋#033[0m| 26/27 [04:15<00:09,  9.88s/it]\u001B[0m\n",
      "\u001B[34mTraining Epoch1:  96%|#033[34m█████████▋#033[0m| 26/27 [04:15<00:09,  9.88s/it]\u001B[0m\n",
      "\u001B[34mTraining Epoch1:  96%|#033[34m█████████▋#033[0m| 26/27 [04:15<00:09,  9.88s/it]\u001B[0m\n",
      "\u001B[34mTraining Epoch1:  96%|#033[34m█████████▋#033[0m| 26/27 [04:15<00:09,  9.88s/it]\u001B[0m\n",
      "\u001B[35mstep 25 is completed and loss is 1.1289381980895996\u001B[0m\n",
      "\u001B[35mTraining Epoch1:  96%|#033[34m█████████▋#033[0m| 26/27 [04:15<00:09,  9.88s/it]\u001B[0m\n",
      "\u001B[35mTraining Epoch1:  96%|#033[34m█████████▋#033[0m| 26/27 [04:15<00:09,  9.88s/it]\u001B[0m\n",
      "\u001B[35mTraining Epoch1:  96%|#033[34m█████████▋#033[0m| 26/27 [04:15<00:09,  9.88s/it]\u001B[0m\n",
      "\u001B[35mTraining Epoch1:  96%|#033[34m█████████▋#033[0m| 26/27 [04:15<00:09,  9.88s/it]\u001B[0m\n",
      "\u001B[34mstep 26 is completed and loss is 1.0644201040267944\u001B[0m\n",
      "\u001B[34mTraining Epoch1: 100%|#033[34m██████████#033[0m| 27/27 [04:25<00:00,  9.86s/it]\u001B[0m\n",
      "\u001B[34mTraining Epoch1: 100%|#033[34m██████████#033[0m| 27/27 [04:25<00:00,  9.83s/it]\u001B[0m\n",
      "\u001B[34mTraining Epoch1: 100%|#033[34m██████████#033[0m| 27/27 [04:25<00:00,  9.86s/it]\u001B[0m\n",
      "\u001B[34mTraining Epoch1: 100%|#033[34m██████████#033[0m| 27/27 [04:25<00:00,  9.83s/it]#015Training Epoch1: 100%|#033[34m██████████#033[0m| 27/27 [04:25<00:00,  9.86s/it]\u001B[0m\n",
      "\u001B[34mTraining Epoch1: 100%|#033[34m██████████#033[0m| 27/27 [04:25<00:00,  9.83s/it]\u001B[0m\n",
      "\u001B[34mTraining Epoch1: 100%|#033[34m██████████#033[0m| 27/27 [04:25<00:00,  9.86s/it]\u001B[0m\n",
      "\u001B[34mTraining Epoch1: 100%|#033[34m██████████#033[0m| 27/27 [04:25<00:00,  9.83s/it]\u001B[0m\n",
      "\u001B[34mMax CUDA memory allocated was 8 GB\u001B[0m\n",
      "\u001B[34mMax CUDA memory reserved was 9 GB\u001B[0m\n",
      "\u001B[34mPeak active CUDA memory was 8 GB\u001B[0m\n",
      "\u001B[34mCuda Malloc retires : 0\u001B[0m\n",
      "\u001B[34mCPU Total Peak Memory consumed during the train (max): 2 GB\u001B[0m\n",
      "\u001B[34mevaluating Epoch:   0%|#033[32m          #033[0m| 0/28 [00:00<?, ?it/s]\u001B[0m\n",
      "\u001B[34mevaluating Epoch:   0%|#033[32m          #033[0m| 0/28 [00:00<?, ?it/s]\u001B[0m\n",
      "\u001B[34mevaluating Epoch:   0%|#033[32m          #033[0m| 0/28 [00:00<?, ?it/s]\u001B[0m\n",
      "\u001B[34mevaluating Epoch:   0%|#033[32m          #033[0m| 0/28 [00:00<?, ?it/s]\u001B[0m\n",
      "\u001B[35mstep 26 is completed and loss is 1.0644201040267944\u001B[0m\n",
      "\u001B[35mTraining Epoch1: 100%|#033[34m██████████#033[0m| 27/27 [04:25<00:00,  9.89s/it]\u001B[0m\n",
      "\u001B[35mTraining Epoch1: 100%|#033[34m██████████#033[0m| 27/27 [04:25<00:00,  9.89s/it]\u001B[0m\n",
      "\u001B[35mTraining Epoch1: 100%|#033[34m██████████#033[0m| 27/27 [04:25<00:00,  9.83s/it]#015Training Epoch1: 100%|#033[34m██████████#033[0m| 27/27 [04:25<00:00,  9.83s/it]\u001B[0m\n",
      "\u001B[35mTraining Epoch1: 100%|#033[34m██████████#033[0m| 27/27 [04:25<00:00,  9.89s/it]\u001B[0m\n",
      "\u001B[35mTraining Epoch1: 100%|#033[34m██████████#033[0m| 27/27 [04:25<00:00,  9.83s/it]\u001B[0m\n",
      "\u001B[35mTraining Epoch1: 100%|#033[34m██████████#033[0m| 27/27 [04:25<00:00,  9.89s/it]\u001B[0m\n",
      "\u001B[35mTraining Epoch1: 100%|#033[34m██████████#033[0m| 27/27 [04:25<00:00,  9.83s/it]\u001B[0m\n",
      "\u001B[35mMax CUDA memory allocated was 8 GB\u001B[0m\n",
      "\u001B[35mMax CUDA memory reserved was 9 GB\u001B[0m\n",
      "\u001B[35mPeak active CUDA memory was 8 GB\u001B[0m\n",
      "\u001B[35mCuda Malloc retires : 0\u001B[0m\n",
      "\u001B[35mCPU Total Peak Memory consumed during the train (max): 2 GB\u001B[0m\n",
      "\u001B[35mevaluating Epoch:   0%|#033[32m          #033[0m| 0/28 [00:00<?, ?it/s]\u001B[0m\n",
      "\u001B[35mevaluating Epoch:   0%|#033[32m          #033[0m| 0/28 [00:00<?, ?it/s]\u001B[0m\n",
      "\u001B[35mevaluating Epoch:   0%|#033[32m          #033[0m| 0/28 [00:00<?, ?it/s]\u001B[0m\n",
      "\u001B[35mevaluating Epoch:   0%|#033[32m          #033[0m| 0/28 [00:00<?, ?it/s]\u001B[0m\n",
      "\u001B[34mevaluating Epoch:   4%|#033[32m▎         #033[0m| 1/28 [00:03<01:36,  3.59s/it]#015evaluating Epoch:   4%|#033[32m▎         #033[0m| 1/28 [00:03<01:36,  3.58s/it]\u001B[0m\n",
      "\u001B[34mevaluating Epoch:   4%|#033[32m▎         #033[0m| 1/28 [00:03<01:36,  3.59s/it]\u001B[0m\n",
      "\u001B[34mevaluating Epoch:   4%|#033[32m▎         #033[0m| 1/28 [00:03<01:36,  3.59s/it]\u001B[0m\n",
      "\u001B[35mevaluating Epoch:   4%|#033[32m▎         #033[0m| 1/28 [00:03<01:36,  3.59s/it]#015evaluating Epoch:   4%|#033[32m▎         #033[0m| 1/28 [00:03<01:36,  3.59s/it]\u001B[0m\n",
      "\u001B[35mevaluating Epoch:   4%|#033[32m▎         #033[0m| 1/28 [00:03<01:36,  3.59s/it]\u001B[0m\n",
      "\u001B[35mevaluating Epoch:   4%|#033[32m▎         #033[0m| 1/28 [00:03<01:36,  3.59s/it]\u001B[0m\n",
      "\u001B[34mevaluating Epoch:   7%|#033[32m▋         #033[0m| 2/28 [00:07<01:31,  3.52s/it]\u001B[0m\n",
      "\u001B[34mevaluating Epoch:   7%|#033[32m▋         #033[0m| 2/28 [00:07<01:31,  3.52s/it]\u001B[0m\n",
      "\u001B[34mevaluating Epoch:   7%|#033[32m▋         #033[0m| 2/28 [00:07<01:31,  3.52s/it]\u001B[0m\n",
      "\u001B[34mevaluating Epoch:   7%|#033[32m▋         #033[0m| 2/28 [00:07<01:31,  3.52s/it]\u001B[0m\n",
      "\u001B[35mevaluating Epoch:   7%|#033[32m▋         #033[0m| 2/28 [00:07<01:31,  3.51s/it]\u001B[0m\n",
      "\u001B[35mevaluating Epoch:   7%|#033[32m▋         #033[0m| 2/28 [00:07<01:31,  3.51s/it]\u001B[0m\n",
      "\u001B[35mevaluating Epoch:   7%|#033[32m▋         #033[0m| 2/28 [00:07<01:31,  3.51s/it]\u001B[0m\n",
      "\u001B[35mevaluating Epoch:   7%|#033[32m▋         #033[0m| 2/28 [00:07<01:31,  3.51s/it]\u001B[0m\n",
      "\u001B[34mevaluating Epoch:  11%|#033[32m█         #033[0m| 3/28 [00:10<01:27,  3.49s/it]\u001B[0m\n",
      "\u001B[34mevaluating Epoch:  11%|#033[32m█         #033[0m| 3/28 [00:10<01:27,  3.49s/it]\u001B[0m\n",
      "\u001B[34mevaluating Epoch:  11%|#033[32m█         #033[0m| 3/28 [00:10<01:27,  3.49s/it]#015evaluating Epoch:  11%|#033[32m█         #033[0m| 3/28 [00:10<01:27,  3.49s/it]\u001B[0m\n",
      "\u001B[35mevaluating Epoch:  11%|#033[32m█         #033[0m| 3/28 [00:10<01:27,  3.48s/it]\u001B[0m\n",
      "\u001B[35mevaluating Epoch:  11%|#033[32m█         #033[0m| 3/28 [00:10<01:27,  3.48s/it]\u001B[0m\n",
      "\u001B[35mevaluating Epoch:  11%|#033[32m█         #033[0m| 3/28 [00:10<01:27,  3.48s/it]\u001B[0m\n",
      "\u001B[35mevaluating Epoch:  11%|#033[32m█         #033[0m| 3/28 [00:10<01:27,  3.48s/it]\u001B[0m\n",
      "\u001B[34mevaluating Epoch:  14%|#033[32m█▍        #033[0m| 4/28 [00:13<01:23,  3.48s/it]#015evaluating Epoch:  14%|#033[32m█▍        #033[0m| 4/28 [00:13<01:23,  3.48s/it]\u001B[0m\n",
      "\u001B[34mevaluating Epoch:  14%|#033[32m█▍        #033[0m| 4/28 [00:13<01:23,  3.48s/it]#015evaluating Epoch:  14%|#033[32m█▍        #033[0m| 4/28 [00:13<01:23,  3.48s/it]\u001B[0m\n",
      "\u001B[35mevaluating Epoch:  14%|#033[32m█▍        #033[0m| 4/28 [00:13<01:23,  3.47s/it]\u001B[0m\n",
      "\u001B[35mevaluating Epoch:  14%|#033[32m█▍        #033[0m| 4/28 [00:13<01:23,  3.47s/it]\u001B[0m\n",
      "\u001B[35mevaluating Epoch:  14%|#033[32m█▍        #033[0m| 4/28 [00:13<01:23,  3.47s/it]\u001B[0m\n",
      "\u001B[35mevaluating Epoch:  14%|#033[32m█▍        #033[0m| 4/28 [00:13<01:23,  3.48s/it]\u001B[0m\n",
      "\u001B[34mevaluating Epoch:  18%|#033[32m█▊        #033[0m| 5/28 [00:17<01:20,  3.48s/it]\u001B[0m\n",
      "\u001B[34mevaluating Epoch:  18%|#033[32m█▊        #033[0m| 5/28 [00:17<01:20,  3.48s/it]\u001B[0m\n",
      "\u001B[34mevaluating Epoch:  18%|#033[32m█▊        #033[0m| 5/28 [00:17<01:20,  3.48s/it]\u001B[0m\n",
      "\u001B[34mevaluating Epoch:  18%|#033[32m█▊        #033[0m| 5/28 [00:17<01:20,  3.48s/it]\u001B[0m\n",
      "\u001B[35mevaluating Epoch:  18%|#033[32m█▊        #033[0m| 5/28 [00:17<01:19,  3.47s/it]\u001B[0m\n",
      "\u001B[35mevaluating Epoch:  18%|#033[32m█▊        #033[0m| 5/28 [00:17<01:19,  3.47s/it]#015evaluating Epoch:  18%|#033[32m█▊        #033[0m| 5/28 [00:17<01:19,  3.47s/it]\u001B[0m\n",
      "\u001B[35mevaluating Epoch:  18%|#033[32m█▊        #033[0m| 5/28 [00:17<01:19,  3.47s/it]\u001B[0m\n",
      "\u001B[34mevaluating Epoch:  21%|#033[32m██▏       #033[0m| 6/28 [00:20<01:16,  3.48s/it]\u001B[0m\n",
      "\u001B[34mevaluating Epoch:  21%|#033[32m██▏       #033[0m| 6/28 [00:20<01:16,  3.48s/it]\u001B[0m\n",
      "\u001B[34mevaluating Epoch:  21%|#033[32m██▏       #033[0m| 6/28 [00:20<01:16,  3.48s/it]\u001B[0m\n",
      "\u001B[34mevaluating Epoch:  21%|#033[32m██▏       #033[0m| 6/28 [00:20<01:16,  3.48s/it]\u001B[0m\n",
      "\u001B[34mevaluating Epoch:  25%|#033[32m██▌       #033[0m| 7/28 [00:24<01:12,  3.47s/it]\u001B[0m\n",
      "\u001B[34mevaluating Epoch:  25%|#033[32m██▌       #033[0m| 7/28 [00:24<01:12,  3.47s/it]#015evaluating Epoch:  25%|#033[32m██▌       #033[0m| 7/28 [00:24<01:12,  3.47s/it]\u001B[0m\n",
      "\u001B[34mevaluating Epoch:  25%|#033[32m██▌       #033[0m| 7/28 [00:24<01:12,  3.47s/it]\u001B[0m\n",
      "\u001B[35mevaluating Epoch:  21%|#033[32m██▏       #033[0m| 6/28 [00:20<01:16,  3.47s/it]\u001B[0m\n",
      "\u001B[35mevaluating Epoch:  21%|#033[32m██▏       #033[0m| 6/28 [00:20<01:16,  3.47s/it]#015evaluating Epoch:  21%|#033[32m██▏       #033[0m| 6/28 [00:20<01:16,  3.47s/it]\u001B[0m\n",
      "\u001B[35mevaluating Epoch:  21%|#033[32m██▏       #033[0m| 6/28 [00:20<01:16,  3.47s/it]\u001B[0m\n",
      "\u001B[35mevaluating Epoch:  25%|#033[32m██▌       #033[0m| 7/28 [00:24<01:12,  3.47s/it]\u001B[0m\n",
      "\u001B[35mevaluating Epoch:  25%|#033[32m██▌       #033[0m| 7/28 [00:24<01:12,  3.47s/it]\u001B[0m\n",
      "\u001B[35mevaluating Epoch:  25%|#033[32m██▌       #033[0m| 7/28 [00:24<01:12,  3.47s/it]\u001B[0m\n",
      "\u001B[35mevaluating Epoch:  25%|#033[32m██▌       #033[0m| 7/28 [00:24<01:12,  3.47s/it]\u001B[0m\n",
      "\u001B[34mevaluating Epoch:  29%|#033[32m██▊       #033[0m| 8/28 [00:27<01:09,  3.47s/it]#015evaluating Epoch:  29%|#033[32m██▊       #033[0m| 8/28 [00:27<01:09,  3.47s/it]#015evaluating Epoch:  29%|#033[32m██▊       #033[0m| 8/28 [00:27<01:09,  3.47s/it]\u001B[0m\n",
      "\u001B[34mevaluating Epoch:  29%|#033[32m██▊       #033[0m| 8/28 [00:27<01:09,  3.47s/it]\u001B[0m\n",
      "\u001B[35mevaluating Epoch:  29%|#033[32m██▊       #033[0m| 8/28 [00:27<01:09,  3.47s/it]#015evaluating Epoch:  29%|#033[32m██▊       #033[0m| 8/28 [00:27<01:09,  3.47s/it]\u001B[0m\n",
      "\u001B[35mevaluating Epoch:  29%|#033[32m██▊       #033[0m| 8/28 [00:27<01:09,  3.47s/it]\u001B[0m\n",
      "\u001B[35mevaluating Epoch:  29%|#033[32m██▊       #033[0m| 8/28 [00:27<01:09,  3.47s/it]\u001B[0m\n",
      "\u001B[34mevaluating Epoch:  32%|#033[32m███▏      #033[0m| 9/28 [00:31<01:06,  3.47s/it]\u001B[0m\n",
      "\u001B[34mevaluating Epoch:  32%|#033[32m███▏      #033[0m| 9/28 [00:31<01:06,  3.47s/it]\u001B[0m\n",
      "\u001B[34mevaluating Epoch:  32%|#033[32m███▏      #033[0m| 9/28 [00:31<01:06,  3.47s/it]\u001B[0m\n",
      "\u001B[34mevaluating Epoch:  32%|#033[32m███▏      #033[0m| 9/28 [00:31<01:06,  3.47s/it]\u001B[0m\n",
      "\u001B[35mevaluating Epoch:  32%|#033[32m███▏      #033[0m| 9/28 [00:31<01:05,  3.47s/it]\u001B[0m\n",
      "\u001B[35mevaluating Epoch:  32%|#033[32m███▏      #033[0m| 9/28 [00:31<01:05,  3.47s/it]\u001B[0m\n",
      "\u001B[35mevaluating Epoch:  32%|#033[32m███▏      #033[0m| 9/28 [00:31<01:05,  3.47s/it]\u001B[0m\n",
      "\u001B[35mevaluating Epoch:  32%|#033[32m███▏      #033[0m| 9/28 [00:31<01:05,  3.47s/it]\u001B[0m\n",
      "\u001B[34mevaluating Epoch:  36%|#033[32m███▌      #033[0m| 10/28 [00:34<01:02,  3.47s/it]\u001B[0m\n",
      "\u001B[34mevaluating Epoch:  36%|#033[32m███▌      #033[0m| 10/28 [00:34<01:02,  3.47s/it]#015evaluating Epoch:  36%|#033[32m███▌      #033[0m| 10/28 [00:34<01:02,  3.47s/it]\u001B[0m\n",
      "\u001B[34mevaluating Epoch:  36%|#033[32m███▌      #033[0m| 10/28 [00:34<01:02,  3.47s/it]\u001B[0m\n",
      "\u001B[35mevaluating Epoch:  36%|#033[32m███▌      #033[0m| 10/28 [00:34<01:02,  3.47s/it]#015evaluating Epoch:  36%|#033[32m███▌      #033[0m| 10/28 [00:34<01:02,  3.47s/it]\u001B[0m\n",
      "\u001B[35mevaluating Epoch:  36%|#033[32m███▌      #033[0m| 10/28 [00:34<01:02,  3.47s/it]\u001B[0m\n",
      "\u001B[35mevaluating Epoch:  36%|#033[32m███▌      #033[0m| 10/28 [00:34<01:02,  3.47s/it]\u001B[0m\n",
      "\u001B[34mevaluating Epoch:  39%|#033[32m███▉      #033[0m| 11/28 [00:38<00:59,  3.47s/it]\u001B[0m\n",
      "\u001B[34mevaluating Epoch:  39%|#033[32m███▉      #033[0m| 11/28 [00:38<00:59,  3.47s/it]\u001B[0m\n",
      "\u001B[34mevaluating Epoch:  39%|#033[32m███▉      #033[0m| 11/28 [00:38<00:59,  3.47s/it]\u001B[0m\n",
      "\u001B[34mevaluating Epoch:  39%|#033[32m███▉      #033[0m| 11/28 [00:38<00:59,  3.47s/it]\u001B[0m\n",
      "\u001B[35mevaluating Epoch:  39%|#033[32m███▉      #033[0m| 11/28 [00:38<00:58,  3.47s/it]\u001B[0m\n",
      "\u001B[35mevaluating Epoch:  39%|#033[32m███▉      #033[0m| 11/28 [00:38<00:58,  3.47s/it]\u001B[0m\n",
      "\u001B[35mevaluating Epoch:  39%|#033[32m███▉      #033[0m| 11/28 [00:38<00:58,  3.47s/it]#015evaluating Epoch:  39%|#033[32m███▉      #033[0m| 11/28 [00:38<00:58,  3.47s/it]\u001B[0m\n",
      "\u001B[34mevaluating Epoch:  43%|#033[32m████▎     #033[0m| 12/28 [00:41<00:55,  3.47s/it]#015evaluating Epoch:  43%|#033[32m████▎     #033[0m| 12/28 [00:41<00:55,  3.47s/it]\u001B[0m\n",
      "\u001B[34mevaluating Epoch:  43%|#033[32m████▎     #033[0m| 12/28 [00:41<00:55,  3.47s/it]\u001B[0m\n",
      "\u001B[34mevaluating Epoch:  43%|#033[32m████▎     #033[0m| 12/28 [00:41<00:55,  3.47s/it]\u001B[0m\n",
      "\u001B[35mevaluating Epoch:  43%|#033[32m████▎     #033[0m| 12/28 [00:41<00:55,  3.47s/it]\u001B[0m\n",
      "\u001B[35mevaluating Epoch:  43%|#033[32m████▎     #033[0m| 12/28 [00:41<00:55,  3.47s/it]\u001B[0m\n",
      "\u001B[35mevaluating Epoch:  43%|#033[32m████▎     #033[0m| 12/28 [00:41<00:55,  3.47s/it]\u001B[0m\n",
      "\u001B[35mevaluating Epoch:  43%|#033[32m████▎     #033[0m| 12/28 [00:41<00:55,  3.47s/it]\u001B[0m\n",
      "\u001B[34mevaluating Epoch:  46%|#033[32m████▋     #033[0m| 13/28 [00:45<00:52,  3.47s/it]\u001B[0m\n",
      "\u001B[34mevaluating Epoch:  46%|#033[32m████▋     #033[0m| 13/28 [00:45<00:52,  3.47s/it]\u001B[0m\n",
      "\u001B[34mevaluating Epoch:  46%|#033[32m████▋     #033[0m| 13/28 [00:45<00:52,  3.47s/it]\u001B[0m\n",
      "\u001B[34mevaluating Epoch:  46%|#033[32m████▋     #033[0m| 13/28 [00:45<00:52,  3.47s/it]\u001B[0m\n",
      "\u001B[35mevaluating Epoch:  46%|#033[32m████▋     #033[0m| 13/28 [00:45<00:52,  3.47s/it]#015evaluating Epoch:  46%|#033[32m████▋     #033[0m| 13/28 [00:45<00:52,  3.47s/it]\u001B[0m\n",
      "\u001B[35mevaluating Epoch:  46%|#033[32m████▋     #033[0m| 13/28 [00:45<00:52,  3.47s/it]\u001B[0m\n",
      "\u001B[35mevaluating Epoch:  46%|#033[32m████▋     #033[0m| 13/28 [00:45<00:52,  3.47s/it]\u001B[0m\n",
      "\u001B[34mevaluating Epoch:  50%|#033[32m█████     #033[0m| 14/28 [00:48<00:48,  3.47s/it]#015evaluating Epoch:  50%|#033[32m█████     #033[0m| 14/28 [00:48<00:48,  3.47s/it]\u001B[0m\n",
      "\u001B[34mevaluating Epoch:  50%|#033[32m█████     #033[0m| 14/28 [00:48<00:48,  3.47s/it]\u001B[0m\n",
      "\u001B[34mevaluating Epoch:  50%|#033[32m█████     #033[0m| 14/28 [00:48<00:48,  3.47s/it]\u001B[0m\n",
      "\u001B[35mevaluating Epoch:  50%|#033[32m█████     #033[0m| 14/28 [00:48<00:48,  3.47s/it]\u001B[0m\n",
      "\u001B[35mevaluating Epoch:  50%|#033[32m█████     #033[0m| 14/28 [00:48<00:48,  3.47s/it]\u001B[0m\n",
      "\u001B[35mevaluating Epoch:  50%|#033[32m█████     #033[0m| 14/28 [00:48<00:48,  3.47s/it]\u001B[0m\n",
      "\u001B[35mevaluating Epoch:  50%|#033[32m█████     #033[0m| 14/28 [00:48<00:48,  3.47s/it]\u001B[0m\n",
      "\u001B[34mevaluating Epoch:  54%|#033[32m█████▎    #033[0m| 15/28 [00:52<00:45,  3.47s/it]#015evaluating Epoch:  54%|#033[32m█████▎    #033[0m| 15/28 [00:52<00:45,  3.47s/it]\u001B[0m\n",
      "\u001B[34mevaluating Epoch:  54%|#033[32m█████▎    #033[0m| 15/28 [00:52<00:45,  3.47s/it]\u001B[0m\n",
      "\u001B[34mevaluating Epoch:  54%|#033[32m█████▎    #033[0m| 15/28 [00:52<00:45,  3.47s/it]\u001B[0m\n",
      "\u001B[35mevaluating Epoch:  54%|#033[32m█████▎    #033[0m| 15/28 [00:52<00:45,  3.47s/it]#015evaluating Epoch:  54%|#033[32m█████▎    #033[0m| 15/28 [00:52<00:45,  3.47s/it]\u001B[0m\n",
      "\u001B[35mevaluating Epoch:  54%|#033[32m█████▎    #033[0m| 15/28 [00:52<00:45,  3.47s/it]\u001B[0m\n",
      "\u001B[35mevaluating Epoch:  54%|#033[32m█████▎    #033[0m| 15/28 [00:52<00:45,  3.47s/it]\u001B[0m\n",
      "\u001B[34mevaluating Epoch:  57%|#033[32m█████▋    #033[0m| 16/28 [00:55<00:41,  3.47s/it]\u001B[0m\n",
      "\u001B[34mevaluating Epoch:  57%|#033[32m█████▋    #033[0m| 16/28 [00:55<00:41,  3.47s/it]\u001B[0m\n",
      "\u001B[34mevaluating Epoch:  57%|#033[32m█████▋    #033[0m| 16/28 [00:55<00:41,  3.47s/it]\u001B[0m\n",
      "\u001B[34mevaluating Epoch:  57%|#033[32m█████▋    #033[0m| 16/28 [00:55<00:41,  3.47s/it]\u001B[0m\n",
      "\u001B[35mevaluating Epoch:  57%|#033[32m█████▋    #033[0m| 16/28 [00:55<00:41,  3.46s/it]\u001B[0m\n",
      "\u001B[35mevaluating Epoch:  57%|#033[32m█████▋    #033[0m| 16/28 [00:55<00:41,  3.46s/it]\u001B[0m\n",
      "\u001B[35mevaluating Epoch:  57%|#033[32m█████▋    #033[0m| 16/28 [00:55<00:41,  3.46s/it]\u001B[0m\n",
      "\u001B[35mevaluating Epoch:  57%|#033[32m█████▋    #033[0m| 16/28 [00:55<00:41,  3.46s/it]\u001B[0m\n",
      "\u001B[34mevaluating Epoch:  61%|#033[32m██████    #033[0m| 17/28 [00:59<00:38,  3.47s/it]#015evaluating Epoch:  61%|#033[32m██████    #033[0m| 17/28 [00:59<00:38,  3.47s/it]\u001B[0m\n",
      "\u001B[34mevaluating Epoch:  61%|#033[32m██████    #033[0m| 17/28 [00:59<00:38,  3.47s/it]\u001B[0m\n",
      "\u001B[34mevaluating Epoch:  61%|#033[32m██████    #033[0m| 17/28 [00:59<00:38,  3.47s/it]\u001B[0m\n",
      "\u001B[35mevaluating Epoch:  61%|#033[32m██████    #033[0m| 17/28 [00:58<00:38,  3.46s/it]\u001B[0m\n",
      "\u001B[35mevaluating Epoch:  61%|#033[32m██████    #033[0m| 17/28 [00:59<00:38,  3.46s/it]\u001B[0m\n",
      "\u001B[35mevaluating Epoch:  61%|#033[32m██████    #033[0m| 17/28 [00:58<00:38,  3.46s/it]\u001B[0m\n",
      "\u001B[35mevaluating Epoch:  61%|#033[32m██████    #033[0m| 17/28 [00:58<00:38,  3.46s/it]\u001B[0m\n",
      "\u001B[34mevaluating Epoch:  64%|#033[32m██████▍   #033[0m| 18/28 [01:02<00:34,  3.47s/it]\u001B[0m\n",
      "\u001B[34mevaluating Epoch:  64%|#033[32m██████▍   #033[0m| 18/28 [01:02<00:34,  3.47s/it]\u001B[0m\n",
      "\u001B[34mevaluating Epoch:  64%|#033[32m██████▍   #033[0m| 18/28 [01:02<00:34,  3.47s/it]#015evaluating Epoch:  64%|#033[32m██████▍   #033[0m| 18/28 [01:02<00:34,  3.47s/it]\u001B[0m\n",
      "\u001B[35mevaluating Epoch:  64%|#033[32m██████▍   #033[0m| 18/28 [01:02<00:34,  3.46s/it]#015evaluating Epoch:  64%|#033[32m██████▍   #033[0m| 18/28 [01:02<00:34,  3.46s/it]\u001B[0m\n",
      "\u001B[35mevaluating Epoch:  64%|#033[32m██████▍   #033[0m| 18/28 [01:02<00:34,  3.46s/it]#015evaluating Epoch:  64%|#033[32m██████▍   #033[0m| 18/28 [01:02<00:34,  3.46s/it]\u001B[0m\n",
      "\u001B[34mevaluating Epoch:  68%|#033[32m██████▊   #033[0m| 19/28 [01:06<00:31,  3.47s/it]\u001B[0m\n",
      "\u001B[34mevaluating Epoch:  68%|#033[32m██████▊   #033[0m| 19/28 [01:06<00:31,  3.47s/it]\u001B[0m\n",
      "\u001B[34mevaluating Epoch:  68%|#033[32m██████▊   #033[0m| 19/28 [01:06<00:31,  3.47s/it]\u001B[0m\n",
      "\u001B[34mevaluating Epoch:  68%|#033[32m██████▊   #033[0m| 19/28 [01:06<00:31,  3.47s/it]\u001B[0m\n",
      "\u001B[35mevaluating Epoch:  68%|#033[32m██████▊   #033[0m| 19/28 [01:05<00:31,  3.46s/it]#015evaluating Epoch:  68%|#033[32m██████▊   #033[0m| 19/28 [01:05<00:31,  3.46s/it]\u001B[0m\n",
      "\u001B[35mevaluating Epoch:  68%|#033[32m██████▊   #033[0m| 19/28 [01:05<00:31,  3.46s/it]#015evaluating Epoch:  68%|#033[32m██████▊   #033[0m| 19/28 [01:05<00:31,  3.46s/it]\u001B[0m\n",
      "\u001B[34mevaluating Epoch:  71%|#033[32m███████▏  #033[0m| 20/28 [01:09<00:27,  3.47s/it]#015evaluating Epoch:  71%|#033[32m███████▏  #033[0m| 20/28 [01:09<00:27,  3.47s/it]\u001B[0m\n",
      "\u001B[34mevaluating Epoch:  71%|#033[32m███████▏  #033[0m| 20/28 [01:09<00:27,  3.47s/it]\u001B[0m\n",
      "\u001B[34mevaluating Epoch:  71%|#033[32m███████▏  #033[0m| 20/28 [01:09<00:27,  3.47s/it]\u001B[0m\n",
      "\u001B[35mevaluating Epoch:  71%|#033[32m███████▏  #033[0m| 20/28 [01:09<00:27,  3.46s/it]\u001B[0m\n",
      "\u001B[35mevaluating Epoch:  71%|#033[32m███████▏  #033[0m| 20/28 [01:09<00:27,  3.46s/it]\u001B[0m\n",
      "\u001B[35mevaluating Epoch:  71%|#033[32m███████▏  #033[0m| 20/28 [01:09<00:27,  3.46s/it]#015evaluating Epoch:  71%|#033[32m███████▏  #033[0m| 20/28 [01:09<00:27,  3.46s/it]\u001B[0m\n",
      "\u001B[34mevaluating Epoch:  75%|#033[32m███████▌  #033[0m| 21/28 [01:13<00:24,  3.47s/it]#015evaluating Epoch:  75%|#033[32m███████▌  #033[0m| 21/28 [01:13<00:24,  3.47s/it]\u001B[0m\n",
      "\u001B[34mevaluating Epoch:  75%|#033[32m███████▌  #033[0m| 21/28 [01:13<00:24,  3.47s/it]\u001B[0m\n",
      "\u001B[34mevaluating Epoch:  75%|#033[32m███████▌  #033[0m| 21/28 [01:13<00:24,  3.47s/it]\u001B[0m\n",
      "\u001B[35mevaluating Epoch:  75%|#033[32m███████▌  #033[0m| 21/28 [01:12<00:24,  3.46s/it]\u001B[0m\n",
      "\u001B[35mevaluating Epoch:  75%|#033[32m███████▌  #033[0m| 21/28 [01:12<00:24,  3.46s/it]\u001B[0m\n",
      "\u001B[35mevaluating Epoch:  75%|#033[32m███████▌  #033[0m| 21/28 [01:12<00:24,  3.46s/it]\u001B[0m\n",
      "\u001B[35mevaluating Epoch:  75%|#033[32m███████▌  #033[0m| 21/28 [01:12<00:24,  3.46s/it]\u001B[0m\n",
      "\u001B[34mevaluating Epoch:  79%|#033[32m███████▊  #033[0m| 22/28 [01:16<00:20,  3.47s/it]#015evaluating Epoch:  79%|#033[32m███████▊  #033[0m| 22/28 [01:16<00:20,  3.47s/it]#015evaluating Epoch:  79%|#033[32m███████▊  #033[0m| 22/28 [01:16<00:20,  3.47s/it]\u001B[0m\n",
      "\u001B[34mevaluating Epoch:  79%|#033[32m███████▊  #033[0m| 22/28 [01:16<00:20,  3.47s/it]\u001B[0m\n",
      "\u001B[35mevaluating Epoch:  79%|#033[32m███████▊  #033[0m| 22/28 [01:16<00:20,  3.46s/it]#015evaluating Epoch:  79%|#033[32m███████▊  #033[0m| 22/28 [01:16<00:20,  3.46s/it]\u001B[0m\n",
      "\u001B[35mevaluating Epoch:  79%|#033[32m███████▊  #033[0m| 22/28 [01:16<00:20,  3.46s/it]\u001B[0m\n",
      "\u001B[35mevaluating Epoch:  79%|#033[32m███████▊  #033[0m| 22/28 [01:16<00:20,  3.46s/it]\u001B[0m\n",
      "\u001B[34mevaluating Epoch:  82%|#033[32m████████▏ #033[0m| 23/28 [01:19<00:17,  3.47s/it]#015evaluating Epoch:  82%|#033[32m████████▏ #033[0m| 23/28 [01:19<00:17,  3.47s/it]\u001B[0m\n",
      "\u001B[34mevaluating Epoch:  82%|#033[32m████████▏ #033[0m| 23/28 [01:19<00:17,  3.47s/it]#015evaluating Epoch:  82%|#033[32m████████▏ #033[0m| 23/28 [01:19<00:17,  3.47s/it]\u001B[0m\n",
      "\u001B[35mevaluating Epoch:  82%|#033[32m████████▏ #033[0m| 23/28 [01:19<00:17,  3.46s/it]\u001B[0m\n",
      "\u001B[35mevaluating Epoch:  82%|#033[32m████████▏ #033[0m| 23/28 [01:19<00:17,  3.46s/it]\u001B[0m\n",
      "\u001B[35mevaluating Epoch:  82%|#033[32m████████▏ #033[0m| 23/28 [01:19<00:17,  3.46s/it]#015evaluating Epoch:  82%|#033[32m████████▏ #033[0m| 23/28 [01:19<00:17,  3.46s/it]\u001B[0m\n",
      "\u001B[34mevaluating Epoch:  86%|#033[32m████████▌ #033[0m| 24/28 [01:23<00:13,  3.47s/it]#015evaluating Epoch:  86%|#033[32m████████▌ #033[0m| 24/28 [01:23<00:13,  3.47s/it]#015evaluating Epoch:  86%|#033[32m████████▌ #033[0m| 24/28 [01:23<00:13,  3.47s/it]\u001B[0m\n",
      "\u001B[34mevaluating Epoch:  86%|#033[32m████████▌ #033[0m| 24/28 [01:23<00:13,  3.47s/it]\u001B[0m\n",
      "\u001B[35mevaluating Epoch:  86%|#033[32m████████▌ #033[0m| 24/28 [01:23<00:13,  3.46s/it]\u001B[0m\n",
      "\u001B[35mevaluating Epoch:  86%|#033[32m████████▌ #033[0m| 24/28 [01:23<00:13,  3.46s/it]#015evaluating Epoch:  86%|#033[32m████████▌ #033[0m| 24/28 [01:23<00:13,  3.46s/it]\u001B[0m\n",
      "\u001B[35mevaluating Epoch:  86%|#033[32m████████▌ #033[0m| 24/28 [01:23<00:13,  3.46s/it]\u001B[0m\n",
      "\u001B[34mevaluating Epoch:  89%|#033[32m████████▉ #033[0m| 25/28 [01:26<00:10,  3.47s/it]#015evaluating Epoch:  89%|#033[32m████████▉ #033[0m| 25/28 [01:26<00:10,  3.47s/it]\u001B[0m\n",
      "\u001B[34mevaluating Epoch:  89%|#033[32m████████▉ #033[0m| 25/28 [01:26<00:10,  3.47s/it]\u001B[0m\n",
      "\u001B[34mevaluating Epoch:  89%|#033[32m████████▉ #033[0m| 25/28 [01:26<00:10,  3.47s/it]\u001B[0m\n",
      "\u001B[35mevaluating Epoch:  89%|#033[32m████████▉ #033[0m| 25/28 [01:26<00:10,  3.46s/it]#015evaluating Epoch:  89%|#033[32m████████▉ #033[0m| 25/28 [01:26<00:10,  3.46s/it]\u001B[0m\n",
      "\u001B[35mevaluating Epoch:  89%|#033[32m████████▉ #033[0m| 25/28 [01:26<00:10,  3.46s/it]\u001B[0m\n",
      "\u001B[35mevaluating Epoch:  89%|#033[32m████████▉ #033[0m| 25/28 [01:26<00:10,  3.46s/it]\u001B[0m\n",
      "\u001B[34mevaluating Epoch:  93%|#033[32m█████████▎#033[0m| 26/28 [01:30<00:06,  3.47s/it]\u001B[0m\n",
      "\u001B[34mevaluating Epoch:  93%|#033[32m█████████▎#033[0m| 26/28 [01:30<00:06,  3.47s/it]#015evaluating Epoch:  93%|#033[32m█████████▎#033[0m| 26/28 [01:30<00:06,  3.47s/it]\u001B[0m\n",
      "\u001B[34mevaluating Epoch:  93%|#033[32m█████████▎#033[0m| 26/28 [01:30<00:06,  3.47s/it]\u001B[0m\n",
      "\u001B[35mevaluating Epoch:  93%|#033[32m█████████▎#033[0m| 26/28 [01:30<00:06,  3.46s/it]\u001B[0m\n",
      "\u001B[35mevaluating Epoch:  93%|#033[32m█████████▎#033[0m| 26/28 [01:30<00:06,  3.46s/it]\u001B[0m\n",
      "\u001B[35mevaluating Epoch:  93%|#033[32m█████████▎#033[0m| 26/28 [01:30<00:06,  3.46s/it]#015evaluating Epoch:  93%|#033[32m█████████▎#033[0m| 26/28 [01:30<00:06,  3.46s/it]\u001B[0m\n",
      "\u001B[34mevaluating Epoch:  96%|#033[32m█████████▋#033[0m| 27/28 [01:33<00:03,  3.47s/it]\u001B[0m\n",
      "\u001B[34mevaluating Epoch:  96%|#033[32m█████████▋#033[0m| 27/28 [01:33<00:03,  3.47s/it]\u001B[0m\n",
      "\u001B[34mevaluating Epoch:  96%|#033[32m█████████▋#033[0m| 27/28 [01:33<00:03,  3.47s/it]\u001B[0m\n",
      "\u001B[34mevaluating Epoch:  96%|#033[32m█████████▋#033[0m| 27/28 [01:33<00:03,  3.47s/it]\u001B[0m\n",
      "\u001B[35mevaluating Epoch:  96%|#033[32m█████████▋#033[0m| 27/28 [01:33<00:03,  3.46s/it]#015evaluating Epoch:  96%|#033[32m█████████▋#033[0m| 27/28 [01:33<00:03,  3.46s/it]\u001B[0m\n",
      "\u001B[35mevaluating Epoch:  96%|#033[32m█████████▋#033[0m| 27/28 [01:33<00:03,  3.46s/it]\u001B[0m\n",
      "\u001B[35mevaluating Epoch:  96%|#033[32m█████████▋#033[0m| 27/28 [01:33<00:03,  3.46s/it]\u001B[0m\n",
      "\u001B[34mevaluating Epoch: 100%|#033[32m██████████#033[0m| 28/28 [01:37<00:00,  3.47s/it]\u001B[0m\n",
      "\u001B[34mevaluating Epoch: 100%|#033[32m██████████#033[0m| 28/28 [01:37<00:00,  3.47s/it]\u001B[0m\n",
      "\u001B[34mevaluating Epoch: 100%|#033[32m██████████#033[0m| 28/28 [01:37<00:00,  3.47s/it]\u001B[0m\n",
      "\u001B[34mevaluating Epoch: 100%|#033[32m██████████#033[0m| 28/28 [01:37<00:00,  3.47s/it]\u001B[0m\n",
      "\u001B[34mevaluating Epoch: 100%|#033[32m██████████#033[0m| 28/28 [01:37<00:00,  3.47s/it]\u001B[0m\n",
      "\u001B[34mevaluating Epoch: 100%|#033[32m██████████#033[0m| 28/28 [01:37<00:00,  3.48s/it]\u001B[0m\n",
      "\u001B[34m#015evaluating Epoch: 100%|#033[32m██████████#033[0m| 28/28 [01:37<00:00,  3.47s/it]\u001B[0m\n",
      "\u001B[34mevaluating Epoch: 100%|#033[32m██████████#033[0m| 28/28 [01:37<00:00,  3.48s/it]\u001B[0m\n",
      "\u001B[34meval_ppl=tensor(3.5981, device='cuda:0') eval_epoch_loss=tensor(1.2804, device='cuda:0')\u001B[0m\n",
      "\u001B[34mwe are about to save the PEFT modules\u001B[0m\n",
      "\u001B[35mevaluating Epoch: 100%|#033[32m██████████#033[0m| 28/28 [01:37<00:00,  3.46s/it]#015evaluating Epoch: 100%|#033[32m██████████#033[0m| 28/28 [01:37<00:00,  3.46s/it]#015evaluating Epoch: 100%|#033[32m██████████#033[0m| 28/28 [01:37<00:00,  3.46s/it]\u001B[0m\n",
      "\u001B[35mevaluating Epoch: 100%|#033[32m██████████#033[0m| 28/28 [01:37<00:00,  3.47s/it]\u001B[0m\n",
      "\u001B[35m#015evaluating Epoch: 100%|#033[32m██████████#033[0m| 28/28 [01:37<00:00,  3.47s/it]\u001B[0m\n",
      "\u001B[35m#015evaluating Epoch: 100%|#033[32m██████████#033[0m| 28/28 [01:37<00:00,  3.47s/it]\u001B[0m\n",
      "\u001B[35mevaluating Epoch: 100%|#033[32m██████████#033[0m| 28/28 [01:37<00:00,  3.46s/it]\u001B[0m\n",
      "\u001B[35mevaluating Epoch: 100%|#033[32m██████████#033[0m| 28/28 [01:37<00:00,  3.47s/it]\u001B[0m\n",
      "\u001B[35meval_ppl=tensor(3.5981, device='cuda:0') eval_epoch_loss=tensor(1.2804, device='cuda:0')\u001B[0m\n",
      "\u001B[35mwe are about to save the PEFT modules\u001B[0m\n",
      "\u001B[34mPEFT modules are saved in saved_peft_model directory\u001B[0m\n",
      "\u001B[34mbest eval loss on epoch 1 is 1.2804142236709595\u001B[0m\n",
      "\u001B[34mEpoch 2: train_perplexity=3.6948, train_epoch_loss=1.3069, epcoh time 265.81928994199984s\u001B[0m\n",
      "\u001B[35mPEFT modules are saved in saved_peft_model directory\u001B[0m\n",
      "\u001B[35mbest eval loss on epoch 1 is 1.2804142236709595\u001B[0m\n",
      "\u001B[35mEpoch 2: train_perplexity=3.6948, train_epoch_loss=1.3069, epcoh time 265.90237952100006s\u001B[0m\n",
      "\u001B[34mTraining Epoch2:   0%|#033[34m          #033[0m| 0/27 [00:00<?, ?it/s]\u001B[0m\n",
      "\u001B[34mTraining Epoch2:   0%|#033[34m          #033[0m| 0/27 [00:00<?, ?it/s]\u001B[0m\n",
      "\u001B[34mTraining Epoch2:   0%|#033[34m          #033[0m| 0/27 [00:00<?, ?it/s]\u001B[0m\n",
      "\u001B[34mTraining Epoch2:   0%|#033[34m          #033[0m| 0/27 [00:00<?, ?it/s]\u001B[0m\n",
      "\u001B[35mTraining Epoch2:   0%|#033[34m          #033[0m| 0/27 [00:00<?, ?it/s]\u001B[0m\n",
      "\u001B[35mTraining Epoch2:   0%|#033[34m          #033[0m| 0/27 [00:00<?, ?it/s]\u001B[0m\n",
      "\u001B[35mTraining Epoch2:   0%|#033[34m          #033[0m| 0/27 [00:00<?, ?it/s]\u001B[0m\n",
      "\u001B[35mTraining Epoch2:   0%|#033[34m          #033[0m| 0/27 [00:00<?, ?it/s]\u001B[0m\n",
      "\u001B[34mTraining Epoch2:   4%|#033[34m▎         #033[0m| 1/27 [00:09<04:13,  9.76s/it]\u001B[0m\n",
      "\u001B[34mTraining Epoch2:   4%|#033[34m▎         #033[0m| 1/27 [00:09<04:13,  9.76s/it]#015Training Epoch2:   4%|#033[34m▎         #033[0m| 1/27 [00:09<04:13,  9.76s/it]\u001B[0m\n",
      "\u001B[34mstep 0 is completed and loss is 1.3883271217346191\u001B[0m\n",
      "\u001B[34mTraining Epoch2:   4%|#033[34m▎         #033[0m| 1/27 [00:09<04:13,  9.77s/it]\u001B[0m\n",
      "\u001B[35mTraining Epoch2:   4%|#033[34m▎         #033[0m| 1/27 [00:09<04:13,  9.75s/it]\u001B[0m\n",
      "\u001B[35mTraining Epoch2:   4%|#033[34m▎         #033[0m| 1/27 [00:09<04:13,  9.76s/it]\u001B[0m\n",
      "\u001B[35mstep 0 is completed and loss is 1.3883271217346191\u001B[0m\n",
      "\u001B[35mTraining Epoch2:   4%|#033[34m▎         #033[0m| 1/27 [00:09<04:13,  9.76s/it]\u001B[0m\n",
      "\u001B[35mTraining Epoch2:   4%|#033[34m▎         #033[0m| 1/27 [00:09<04:13,  9.76s/it]\u001B[0m\n",
      "\u001B[34mTraining Epoch2:   7%|#033[34m▋         #033[0m| 2/27 [00:19<04:04,  9.80s/it]#015Training Epoch2:   7%|#033[34m▋         #033[0m| 2/27 [00:19<04:04,  9.80s/it]\u001B[0m\n",
      "\u001B[34mstep 1 is completed and loss is 1.048492193222046\u001B[0m\n",
      "\u001B[34mTraining Epoch2:   7%|#033[34m▋         #033[0m| 2/27 [00:19<04:04,  9.80s/it]\u001B[0m\n",
      "\u001B[34mTraining Epoch2:   7%|#033[34m▋         #033[0m| 2/27 [00:19<04:04,  9.80s/it]\u001B[0m\n",
      "\u001B[35mTraining Epoch2:   7%|#033[34m▋         #033[0m| 2/27 [00:19<04:04,  9.78s/it]\u001B[0m\n",
      "\u001B[35mTraining Epoch2:   7%|#033[34m▋         #033[0m| 2/27 [00:19<04:04,  9.78s/it]\u001B[0m\n",
      "\u001B[35mstep 1 is completed and loss is 1.048492193222046\u001B[0m\n",
      "\u001B[35mTraining Epoch2:   7%|#033[34m▋         #033[0m| 2/27 [00:19<04:04,  9.78s/it]\u001B[0m\n",
      "\u001B[35mTraining Epoch2:   7%|#033[34m▋         #033[0m| 2/27 [00:19<04:04,  9.78s/it]\u001B[0m\n",
      "\u001B[34mTraining Epoch2:  11%|#033[34m█         #033[0m| 3/27 [00:29<03:55,  9.81s/it]\u001B[0m\n",
      "\u001B[34mTraining Epoch2:  11%|#033[34m█         #033[0m| 3/27 [00:29<03:55,  9.81s/it]#015Training Epoch2:  11%|#033[34m█         #033[0m| 3/27 [00:29<03:55,  9.81s/it]\u001B[0m\n",
      "\u001B[34mstep 2 is completed and loss is 1.4538662433624268\u001B[0m\n",
      "\u001B[34mTraining Epoch2:  11%|#033[34m█         #033[0m| 3/27 [00:29<03:55,  9.81s/it]\u001B[0m\n",
      "\u001B[35mTraining Epoch2:  11%|#033[34m█         #033[0m| 3/27 [00:29<03:54,  9.79s/it]\u001B[0m\n",
      "\u001B[35mTraining Epoch2:  11%|#033[34m█         #033[0m| 3/27 [00:29<03:54,  9.79s/it]\u001B[0m\n",
      "\u001B[35mTraining Epoch2:  11%|#033[34m█         #033[0m| 3/27 [00:29<03:54,  9.79s/it]\u001B[0m\n",
      "\u001B[35mstep 2 is completed and loss is 1.4538662433624268\u001B[0m\n",
      "\u001B[35mTraining Epoch2:  11%|#033[34m█         #033[0m| 3/27 [00:29<03:54,  9.79s/it]\u001B[0m\n",
      "\u001B[34mTraining Epoch2:  15%|#033[34m█▍        #033[0m| 4/27 [00:39<03:45,  9.81s/it]\u001B[0m\n",
      "\u001B[34mTraining Epoch2:  15%|#033[34m█▍        #033[0m| 4/27 [00:39<03:45,  9.81s/it]\u001B[0m\n",
      "\u001B[34mTraining Epoch2:  15%|#033[34m█▍        #033[0m| 4/27 [00:39<03:45,  9.81s/it]\u001B[0m\n",
      "\u001B[34mstep 3 is completed and loss is 1.1800971031188965\u001B[0m\n",
      "\u001B[34mTraining Epoch2:  15%|#033[34m█▍        #033[0m| 4/27 [00:39<03:45,  9.81s/it]\u001B[0m\n",
      "\u001B[35mTraining Epoch2:  15%|#033[34m█▍        #033[0m| 4/27 [00:39<03:45,  9.79s/it]\u001B[0m\n",
      "\u001B[35mTraining Epoch2:  15%|#033[34m█▍        #033[0m| 4/27 [00:39<03:45,  9.79s/it]\u001B[0m\n",
      "\u001B[35mstep 3 is completed and loss is 1.1800971031188965\u001B[0m\n",
      "\u001B[35mTraining Epoch2:  15%|#033[34m█▍        #033[0m| 4/27 [00:39<03:45,  9.79s/it]\u001B[0m\n",
      "\u001B[35mTraining Epoch2:  15%|#033[34m█▍        #033[0m| 4/27 [00:39<03:45,  9.79s/it]\u001B[0m\n",
      "\u001B[34mstep 4 is completed and loss is 1.1673322916030884\u001B[0m\n",
      "\u001B[34mTraining Epoch2:  19%|#033[34m█▊        #033[0m| 5/27 [00:49<03:35,  9.81s/it]\u001B[0m\n",
      "\u001B[34mTraining Epoch2:  19%|#033[34m█▊        #033[0m| 5/27 [00:49<03:35,  9.81s/it]\u001B[0m\n",
      "\u001B[34mTraining Epoch2:  19%|#033[34m█▊        #033[0m| 5/27 [00:49<03:35,  9.81s/it]\u001B[0m\n",
      "\u001B[34mTraining Epoch2:  19%|#033[34m█▊        #033[0m| 5/27 [00:49<03:35,  9.81s/it]\u001B[0m\n",
      "\u001B[35mTraining Epoch2:  19%|#033[34m█▊        #033[0m| 5/27 [00:48<03:35,  9.80s/it]\u001B[0m\n",
      "\u001B[35mTraining Epoch2:  19%|#033[34m█▊        #033[0m| 5/27 [00:48<03:35,  9.80s/it]\u001B[0m\n",
      "\u001B[35mstep 4 is completed and loss is 1.1673322916030884\u001B[0m\n",
      "\u001B[35mTraining Epoch2:  19%|#033[34m█▊        #033[0m| 5/27 [00:48<03:35,  9.80s/it]\u001B[0m\n",
      "\u001B[35mTraining Epoch2:  19%|#033[34m█▊        #033[0m| 5/27 [00:48<03:35,  9.80s/it]\u001B[0m\n",
      "\u001B[35mTraining Epoch2:  22%|#033[34m██▏       #033[0m| 6/27 [00:58<03:25,  9.80s/it]\u001B[0m\n",
      "\u001B[35mTraining Epoch2:  22%|#033[34m██▏       #033[0m| 6/27 [00:58<03:25,  9.80s/it]\u001B[0m\n",
      "\u001B[35mstep 5 is completed and loss is 1.2616633176803589\u001B[0m\n",
      "\u001B[35mTraining Epoch2:  22%|#033[34m██▏       #033[0m| 6/27 [00:58<03:25,  9.80s/it]\u001B[0m\n",
      "\u001B[35mTraining Epoch2:  22%|#033[34m██▏       #033[0m| 6/27 [00:58<03:25,  9.80s/it]\u001B[0m\n",
      "\u001B[34mTraining Epoch2:  22%|#033[34m██▏       #033[0m| 6/27 [00:58<03:26,  9.82s/it]\u001B[0m\n",
      "\u001B[34mTraining Epoch2:  22%|#033[34m██▏       #033[0m| 6/27 [00:58<03:26,  9.82s/it]\u001B[0m\n",
      "\u001B[34mstep 5 is completed and loss is 1.2616633176803589\u001B[0m\n",
      "\u001B[34mTraining Epoch2:  22%|#033[34m██▏       #033[0m| 6/27 [00:58<03:26,  9.82s/it]\u001B[0m\n",
      "\u001B[34mTraining Epoch2:  22%|#033[34m██▏       #033[0m| 6/27 [00:58<03:26,  9.82s/it]\u001B[0m\n",
      "\u001B[34mTraining Epoch2:  26%|#033[34m██▌       #033[0m| 7/27 [01:08<03:16,  9.82s/it]\u001B[0m\n",
      "\u001B[34mstep 6 is completed and loss is 1.5092283487319946\u001B[0m\n",
      "\u001B[34mTraining Epoch2:  26%|#033[34m██▌       #033[0m| 7/27 [01:08<03:16,  9.82s/it]\u001B[0m\n",
      "\u001B[34mTraining Epoch2:  26%|#033[34m██▌       #033[0m| 7/27 [01:08<03:16,  9.82s/it]\u001B[0m\n",
      "\u001B[34mTraining Epoch2:  26%|#033[34m██▌       #033[0m| 7/27 [01:08<03:16,  9.82s/it]\u001B[0m\n",
      "\u001B[35mstep 6 is completed and loss is 1.5092283487319946\u001B[0m\n",
      "\u001B[35mTraining Epoch2:  26%|#033[34m██▌       #033[0m| 7/27 [01:08<03:16,  9.81s/it]\u001B[0m\n",
      "\u001B[35mTraining Epoch2:  26%|#033[34m██▌       #033[0m| 7/27 [01:08<03:16,  9.81s/it]#015Training Epoch2:  26%|#033[34m██▌       #033[0m| 7/27 [01:08<03:16,  9.81s/it]\u001B[0m\n",
      "\u001B[35mTraining Epoch2:  26%|#033[34m██▌       #033[0m| 7/27 [01:08<03:16,  9.81s/it]\u001B[0m\n",
      "\u001B[34mTraining Epoch2:  30%|#033[34m██▉       #033[0m| 8/27 [01:18<03:06,  9.82s/it]\u001B[0m\n",
      "\u001B[34mTraining Epoch2:  30%|#033[34m██▉       #033[0m| 8/27 [01:18<03:06,  9.82s/it]\u001B[0m\n",
      "\u001B[34mTraining Epoch2:  30%|#033[34m██▉       #033[0m| 8/27 [01:18<03:06,  9.82s/it]\u001B[0m\n",
      "\u001B[34mstep 7 is completed and loss is 1.3614155054092407\u001B[0m\n",
      "\u001B[34mTraining Epoch2:  30%|#033[34m██▉       #033[0m| 8/27 [01:18<03:06,  9.82s/it]\u001B[0m\n",
      "\u001B[35mTraining Epoch2:  30%|#033[34m██▉       #033[0m| 8/27 [01:18<03:06,  9.80s/it]\u001B[0m\n",
      "\u001B[35mTraining Epoch2:  30%|#033[34m██▉       #033[0m| 8/27 [01:18<03:06,  9.80s/it]\u001B[0m\n",
      "\u001B[35mTraining Epoch2:  30%|#033[34m██▉       #033[0m| 8/27 [01:18<03:06,  9.80s/it]\u001B[0m\n",
      "\u001B[35mstep 7 is completed and loss is 1.3614155054092407\u001B[0m\n",
      "\u001B[35mTraining Epoch2:  30%|#033[34m██▉       #033[0m| 8/27 [01:18<03:06,  9.81s/it]\u001B[0m\n",
      "\u001B[34mTraining Epoch2:  33%|#033[34m███▎      #033[0m| 9/27 [01:28<02:56,  9.82s/it]\u001B[0m\n",
      "\u001B[34mstep 8 is completed and loss is 1.5512663125991821\u001B[0m\n",
      "\u001B[34mTraining Epoch2:  33%|#033[34m███▎      #033[0m| 9/27 [01:28<02:56,  9.82s/it]\u001B[0m\n",
      "\u001B[34mTraining Epoch2:  33%|#033[34m███▎      #033[0m| 9/27 [01:28<02:56,  9.82s/it]\u001B[0m\n",
      "\u001B[34mTraining Epoch2:  33%|#033[34m███▎      #033[0m| 9/27 [01:28<02:56,  9.82s/it]\u001B[0m\n",
      "\u001B[35mstep 8 is completed and loss is 1.5512663125991821\u001B[0m\n",
      "\u001B[35mTraining Epoch2:  33%|#033[34m███▎      #033[0m| 9/27 [01:28<02:56,  9.80s/it]\u001B[0m\n",
      "\u001B[35mTraining Epoch2:  33%|#033[34m███▎      #033[0m| 9/27 [01:28<02:56,  9.80s/it]\u001B[0m\n",
      "\u001B[35mTraining Epoch2:  33%|#033[34m███▎      #033[0m| 9/27 [01:28<02:56,  9.80s/it]\u001B[0m\n",
      "\u001B[35mTraining Epoch2:  33%|#033[34m███▎      #033[0m| 9/27 [01:28<02:56,  9.80s/it]\u001B[0m\n",
      "\u001B[34mstep 9 is completed and loss is 1.131500244140625\u001B[0m\n",
      "\u001B[34mTraining Epoch2:  37%|#033[34m███▋      #033[0m| 10/27 [01:38<02:46,  9.82s/it]\u001B[0m\n",
      "\u001B[34mTraining Epoch2:  37%|#033[34m███▋      #033[0m| 10/27 [01:38<02:46,  9.81s/it]\u001B[0m\n",
      "\u001B[34mTraining Epoch2:  37%|#033[34m███▋      #033[0m| 10/27 [01:38<02:46,  9.81s/it]\u001B[0m\n",
      "\u001B[34mTraining Epoch2:  37%|#033[34m███▋      #033[0m| 10/27 [01:38<02:46,  9.81s/it]\u001B[0m\n",
      "\u001B[35mTraining Epoch2:  37%|#033[34m███▋      #033[0m| 10/27 [01:37<02:46,  9.80s/it]\u001B[0m\n",
      "\u001B[35mTraining Epoch2:  37%|#033[34m███▋      #033[0m| 10/27 [01:37<02:46,  9.80s/it]\u001B[0m\n",
      "\u001B[35mstep 9 is completed and loss is 1.131500244140625\u001B[0m\n",
      "\u001B[35mTraining Epoch2:  37%|#033[34m███▋      #033[0m| 10/27 [01:37<02:46,  9.80s/it]\u001B[0m\n",
      "\u001B[35mTraining Epoch2:  37%|#033[34m███▋      #033[0m| 10/27 [01:37<02:46,  9.80s/it]\u001B[0m\n",
      "\u001B[35mTraining Epoch2:  41%|#033[34m████      #033[0m| 11/27 [01:47<02:36,  9.80s/it]\u001B[0m\n",
      "\u001B[35mTraining Epoch2:  41%|#033[34m████      #033[0m| 11/27 [01:47<02:36,  9.80s/it]\u001B[0m\n",
      "\u001B[35mstep 10 is completed and loss is 1.1792696714401245\u001B[0m\n",
      "\u001B[35mTraining Epoch2:  41%|#033[34m████      #033[0m| 11/27 [01:47<02:36,  9.80s/it]\u001B[0m\n",
      "\u001B[35mTraining Epoch2:  41%|#033[34m████      #033[0m| 11/27 [01:47<02:36,  9.80s/it]\u001B[0m\n",
      "\u001B[34mTraining Epoch2:  41%|#033[34m████      #033[0m| 11/27 [01:47<02:36,  9.81s/it]\u001B[0m\n",
      "\u001B[34mstep 10 is completed and loss is 1.1792696714401245\u001B[0m\n",
      "\u001B[34mTraining Epoch2:  41%|#033[34m████      #033[0m| 11/27 [01:47<02:36,  9.81s/it]\u001B[0m\n",
      "\u001B[34mTraining Epoch2:  41%|#033[34m████      #033[0m| 11/27 [01:47<02:36,  9.81s/it]\u001B[0m\n",
      "\u001B[34mTraining Epoch2:  41%|#033[34m████      #033[0m| 11/27 [01:47<02:36,  9.81s/it]\u001B[0m\n",
      "\u001B[34mstep 11 is completed and loss is 1.1925073862075806\u001B[0m\n",
      "\u001B[34mTraining Epoch2:  44%|#033[34m████▍     #033[0m| 12/27 [01:57<02:27,  9.82s/it]\u001B[0m\n",
      "\u001B[34mTraining Epoch2:  44%|#033[34m████▍     #033[0m| 12/27 [01:57<02:27,  9.82s/it]#015Training Epoch2:  44%|#033[34m████▍     #033[0m| 12/27 [01:57<02:27,  9.82s/it]\u001B[0m\n",
      "\u001B[34mTraining Epoch2:  44%|#033[34m████▍     #033[0m| 12/27 [01:57<02:27,  9.82s/it]\u001B[0m\n",
      "\u001B[35mTraining Epoch2:  44%|#033[34m████▍     #033[0m| 12/27 [01:57<02:27,  9.81s/it]\u001B[0m\n",
      "\u001B[35mstep 11 is completed and loss is 1.1925073862075806\u001B[0m\n",
      "\u001B[35mTraining Epoch2:  44%|#033[34m████▍     #033[0m| 12/27 [01:57<02:27,  9.80s/it]#015Training Epoch2:  44%|#033[34m████▍     #033[0m| 12/27 [01:57<02:27,  9.80s/it]#015Training Epoch2:  44%|#033[34m████▍     #033[0m| 12/27 [01:57<02:27,  9.81s/it]\u001B[0m\n",
      "\u001B[34mTraining Epoch2:  48%|#033[34m████▊     #033[0m| 13/27 [02:07<02:18,  9.89s/it]\u001B[0m\n",
      "\u001B[34mstep 12 is completed and loss is 1.2916356325149536\u001B[0m\n",
      "\u001B[34mTraining Epoch2:  48%|#033[34m████▊     #033[0m| 13/27 [02:07<02:18,  9.89s/it]\u001B[0m\n",
      "\u001B[34mTraining Epoch2:  48%|#033[34m████▊     #033[0m| 13/27 [02:07<02:18,  9.89s/it]\u001B[0m\n",
      "\u001B[34mTraining Epoch2:  48%|#033[34m████▊     #033[0m| 13/27 [02:07<02:18,  9.89s/it]\u001B[0m\n",
      "\u001B[35mTraining Epoch2:  48%|#033[34m████▊     #033[0m| 13/27 [02:07<02:18,  9.88s/it]\u001B[0m\n",
      "\u001B[35mTraining Epoch2:  48%|#033[34m████▊     #033[0m| 13/27 [02:07<02:18,  9.88s/it]\u001B[0m\n",
      "\u001B[35mTraining Epoch2:  48%|#033[34m████▊     #033[0m| 13/27 [02:07<02:18,  9.88s/it]\u001B[0m\n",
      "\u001B[35mstep 12 is completed and loss is 1.2916356325149536\u001B[0m\n",
      "\u001B[35mTraining Epoch2:  48%|#033[34m████▊     #033[0m| 13/27 [02:07<02:18,  9.88s/it]\u001B[0m\n",
      "\u001B[34mTraining Epoch2:  52%|#033[34m█████▏    #033[0m| 14/27 [02:17<02:08,  9.87s/it]\u001B[0m\n",
      "\u001B[34mTraining Epoch2:  52%|#033[34m█████▏    #033[0m| 14/27 [02:17<02:08,  9.87s/it]\u001B[0m\n",
      "\u001B[34mTraining Epoch2:  52%|#033[34m█████▏    #033[0m| 14/27 [02:17<02:08,  9.87s/it]\u001B[0m\n",
      "\u001B[34mstep 13 is completed and loss is 0.9669908285140991\u001B[0m\n",
      "\u001B[34mTraining Epoch2:  52%|#033[34m█████▏    #033[0m| 14/27 [02:17<02:08,  9.87s/it]\u001B[0m\n",
      "\u001B[35mTraining Epoch2:  52%|#033[34m█████▏    #033[0m| 14/27 [02:17<02:08,  9.89s/it]\u001B[0m\n",
      "\u001B[35mTraining Epoch2:  52%|#033[34m█████▏    #033[0m| 14/27 [02:17<02:08,  9.89s/it]\u001B[0m\n",
      "\u001B[35mstep 13 is completed and loss is 0.9669908285140991\u001B[0m\n",
      "\u001B[35mTraining Epoch2:  52%|#033[34m█████▏    #033[0m| 14/27 [02:17<02:08,  9.89s/it]\u001B[0m\n",
      "\u001B[35mTraining Epoch2:  52%|#033[34m█████▏    #033[0m| 14/27 [02:17<02:08,  9.89s/it]\u001B[0m\n",
      "\u001B[34mTraining Epoch2:  56%|#033[34m█████▌    #033[0m| 15/27 [02:27<01:58,  9.85s/it]\u001B[0m\n",
      "\u001B[34mstep 14 is completed and loss is 1.3400242328643799\u001B[0m\n",
      "\u001B[34mTraining Epoch2:  56%|#033[34m█████▌    #033[0m| 15/27 [02:27<01:58,  9.85s/it]#015Training Epoch2:  56%|#033[34m█████▌    #033[0m| 15/27 [02:27<01:58,  9.85s/it]\u001B[0m\n",
      "\u001B[34mTraining Epoch2:  56%|#033[34m█████▌    #033[0m| 15/27 [02:27<01:58,  9.85s/it]\u001B[0m\n",
      "\u001B[35mTraining Epoch2:  56%|#033[34m█████▌    #033[0m| 15/27 [02:27<01:58,  9.87s/it]#015Training Epoch2:  56%|#033[34m█████▌    #033[0m| 15/27 [02:27<01:58,  9.87s/it]\u001B[0m\n",
      "\u001B[35mstep 14 is completed and loss is 1.3400242328643799\u001B[0m\n",
      "\u001B[35mTraining Epoch2:  56%|#033[34m█████▌    #033[0m| 15/27 [02:27<01:58,  9.87s/it]#015Training Epoch2:  56%|#033[34m█████▌    #033[0m| 15/27 [02:27<01:58,  9.87s/it]\u001B[0m\n",
      "\u001B[34mTraining Epoch2:  59%|#033[34m█████▉    #033[0m| 16/27 [02:37<01:48,  9.84s/it]\u001B[0m\n",
      "\u001B[34mTraining Epoch2:  59%|#033[34m█████▉    #033[0m| 16/27 [02:37<01:48,  9.84s/it]\u001B[0m\n",
      "\u001B[34mTraining Epoch2:  59%|#033[34m█████▉    #033[0m| 16/27 [02:37<01:48,  9.84s/it]\u001B[0m\n",
      "\u001B[34mstep 15 is completed and loss is 1.4473686218261719\u001B[0m\n",
      "\u001B[34mTraining Epoch2:  59%|#033[34m█████▉    #033[0m| 16/27 [02:37<01:48,  9.84s/it]\u001B[0m\n",
      "\u001B[35mTraining Epoch2:  59%|#033[34m█████▉    #033[0m| 16/27 [02:37<01:48,  9.85s/it]\u001B[0m\n",
      "\u001B[35mTraining Epoch2:  59%|#033[34m█████▉    #033[0m| 16/27 [02:37<01:48,  9.85s/it]\u001B[0m\n",
      "\u001B[35mTraining Epoch2:  59%|#033[34m█████▉    #033[0m| 16/27 [02:37<01:48,  9.85s/it]\u001B[0m\n",
      "\u001B[35mstep 15 is completed and loss is 1.4473686218261719\u001B[0m\n",
      "\u001B[35mTraining Epoch2:  59%|#033[34m█████▉    #033[0m| 16/27 [02:37<01:48,  9.85s/it]\u001B[0m\n",
      "\u001B[34mTraining Epoch2:  63%|#033[34m██████▎   #033[0m| 17/27 [02:47<01:38,  9.83s/it]\u001B[0m\n",
      "\u001B[34mTraining Epoch2:  63%|#033[34m██████▎   #033[0m| 17/27 [02:47<01:38,  9.83s/it]\u001B[0m\n",
      "\u001B[34mstep 16 is completed and loss is 1.4565622806549072\u001B[0m\n",
      "\u001B[34mTraining Epoch2:  63%|#033[34m██████▎   #033[0m| 17/27 [02:47<01:38,  9.83s/it]\u001B[0m\n",
      "\u001B[34mTraining Epoch2:  63%|#033[34m██████▎   #033[0m| 17/27 [02:47<01:38,  9.83s/it]\u001B[0m\n",
      "\u001B[35mTraining Epoch2:  63%|#033[34m██████▎   #033[0m| 17/27 [02:47<01:38,  9.84s/it]\u001B[0m\n",
      "\u001B[35mstep 16 is completed and loss is 1.4565622806549072\u001B[0m\n",
      "\u001B[35mTraining Epoch2:  63%|#033[34m██████▎   #033[0m| 17/27 [02:47<01:38,  9.84s/it]\u001B[0m\n",
      "\u001B[35mTraining Epoch2:  63%|#033[34m██████▎   #033[0m| 17/27 [02:47<01:38,  9.84s/it]\u001B[0m\n",
      "\u001B[35mTraining Epoch2:  63%|#033[34m██████▎   #033[0m| 17/27 [02:47<01:38,  9.84s/it]\u001B[0m\n",
      "\u001B[35mTraining Epoch2:  67%|#033[34m██████▋   #033[0m| 18/27 [02:56<01:28,  9.83s/it]#015Training Epoch2:  67%|#033[34m██████▋   #033[0m| 18/27 [02:56<01:28,  9.83s/it]\u001B[0m\n",
      "\u001B[35mTraining Epoch2:  67%|#033[34m██████▋   #033[0m| 18/27 [02:56<01:28,  9.83s/it]\u001B[0m\n",
      "\u001B[35mstep 17 is completed and loss is 1.355676293373108\u001B[0m\n",
      "\u001B[35mTraining Epoch2:  67%|#033[34m██████▋   #033[0m| 18/27 [02:56<01:28,  9.83s/it]\u001B[0m\n",
      "\u001B[34mTraining Epoch2:  67%|#033[34m██████▋   #033[0m| 18/27 [02:56<01:28,  9.83s/it]\u001B[0m\n",
      "\u001B[34mstep 17 is completed and loss is 1.355676293373108\u001B[0m\n",
      "\u001B[34mTraining Epoch2:  67%|#033[34m██████▋   #033[0m| 18/27 [02:56<01:28,  9.83s/it]\u001B[0m\n",
      "\u001B[34mTraining Epoch2:  67%|#033[34m██████▋   #033[0m| 18/27 [02:56<01:28,  9.83s/it]\u001B[0m\n",
      "\u001B[34mTraining Epoch2:  67%|#033[34m██████▋   #033[0m| 18/27 [02:56<01:28,  9.83s/it]\u001B[0m\n",
      "\u001B[34mTraining Epoch2:  70%|#033[34m███████   #033[0m| 19/27 [03:06<01:18,  9.82s/it]\u001B[0m\n",
      "\u001B[34mTraining Epoch2:  70%|#033[34m███████   #033[0m| 19/27 [03:06<01:18,  9.82s/it]\u001B[0m\n",
      "\u001B[34mTraining Epoch2:  70%|#033[34m███████   #033[0m| 19/27 [03:06<01:18,  9.82s/it]\u001B[0m\n",
      "\u001B[34mstep 18 is completed and loss is 1.2375887632369995\u001B[0m\n",
      "\u001B[34mTraining Epoch2:  70%|#033[34m███████   #033[0m| 19/27 [03:06<01:18,  9.82s/it]\u001B[0m\n",
      "\u001B[35mstep 18 is completed and loss is 1.2375887632369995\u001B[0m\n",
      "\u001B[35mTraining Epoch2:  70%|#033[34m███████   #033[0m| 19/27 [03:06<01:18,  9.82s/it]\u001B[0m\n",
      "\u001B[35mTraining Epoch2:  70%|#033[34m███████   #033[0m| 19/27 [03:06<01:18,  9.82s/it]\u001B[0m\n",
      "\u001B[35mTraining Epoch2:  70%|#033[34m███████   #033[0m| 19/27 [03:06<01:18,  9.82s/it]\u001B[0m\n",
      "\u001B[35mTraining Epoch2:  70%|#033[34m███████   #033[0m| 19/27 [03:06<01:18,  9.82s/it]\u001B[0m\n",
      "\u001B[34mTraining Epoch2:  74%|#033[34m███████▍  #033[0m| 20/27 [03:16<01:08,  9.82s/it]\u001B[0m\n",
      "\u001B[34mTraining Epoch2:  74%|#033[34m███████▍  #033[0m| 20/27 [03:16<01:08,  9.82s/it]\u001B[0m\n",
      "\u001B[34mstep 19 is completed and loss is 1.1435346603393555\u001B[0m\n",
      "\u001B[34mTraining Epoch2:  74%|#033[34m███████▍  #033[0m| 20/27 [03:16<01:08,  9.82s/it]\u001B[0m\n",
      "\u001B[34mTraining Epoch2:  74%|#033[34m███████▍  #033[0m| 20/27 [03:16<01:08,  9.82s/it]\u001B[0m\n",
      "\u001B[35mTraining Epoch2:  74%|#033[34m███████▍  #033[0m| 20/27 [03:16<01:08,  9.82s/it]\u001B[0m\n",
      "\u001B[35mTraining Epoch2:  74%|#033[34m███████▍  #033[0m| 20/27 [03:16<01:08,  9.82s/it]\u001B[0m\n",
      "\u001B[35mTraining Epoch2:  74%|#033[34m███████▍  #033[0m| 20/27 [03:16<01:08,  9.82s/it]\u001B[0m\n",
      "\u001B[35mstep 19 is completed and loss is 1.1435346603393555\u001B[0m\n",
      "\u001B[35mTraining Epoch2:  74%|#033[34m███████▍  #033[0m| 20/27 [03:16<01:08,  9.82s/it]\u001B[0m\n",
      "\u001B[34mstep 20 is completed and loss is 1.1846823692321777\u001B[0m\n",
      "\u001B[34mTraining Epoch2:  78%|#033[34m███████▊  #033[0m| 21/27 [03:26<00:58,  9.82s/it]\u001B[0m\n",
      "\u001B[34mTraining Epoch2:  78%|#033[34m███████▊  #033[0m| 21/27 [03:26<00:58,  9.82s/it]\u001B[0m\n",
      "\u001B[34mTraining Epoch2:  78%|#033[34m███████▊  #033[0m| 21/27 [03:26<00:58,  9.82s/it]\u001B[0m\n",
      "\u001B[34mTraining Epoch2:  78%|#033[34m███████▊  #033[0m| 21/27 [03:26<00:58,  9.82s/it]\u001B[0m\n",
      "\u001B[35mTraining Epoch2:  78%|#033[34m███████▊  #033[0m| 21/27 [03:26<00:58,  9.81s/it]\u001B[0m\n",
      "\u001B[35mstep 20 is completed and loss is 1.1846823692321777\u001B[0m\n",
      "\u001B[35mTraining Epoch2:  78%|#033[34m███████▊  #033[0m| 21/27 [03:26<00:58,  9.81s/it]\u001B[0m\n",
      "\u001B[35mTraining Epoch2:  78%|#033[34m███████▊  #033[0m| 21/27 [03:26<00:58,  9.81s/it]\u001B[0m\n",
      "\u001B[35mTraining Epoch2:  78%|#033[34m███████▊  #033[0m| 21/27 [03:26<00:58,  9.81s/it]\u001B[0m\n",
      "\u001B[34mTraining Epoch2:  81%|#033[34m████████▏ #033[0m| 22/27 [03:36<00:49,  9.82s/it]\u001B[0m\n",
      "\u001B[34mstep 21 is completed and loss is 1.2304186820983887\u001B[0m\n",
      "\u001B[34mTraining Epoch2:  81%|#033[34m████████▏ #033[0m| 22/27 [03:36<00:49,  9.82s/it]\u001B[0m\n",
      "\u001B[34mTraining Epoch2:  81%|#033[34m████████▏ #033[0m| 22/27 [03:36<00:49,  9.82s/it]\u001B[0m\n",
      "\u001B[34mTraining Epoch2:  81%|#033[34m████████▏ #033[0m| 22/27 [03:36<00:49,  9.82s/it]\u001B[0m\n",
      "\u001B[35mTraining Epoch2:  81%|#033[34m████████▏ #033[0m| 22/27 [03:36<00:49,  9.80s/it]\u001B[0m\n",
      "\u001B[35mTraining Epoch2:  81%|#033[34m████████▏ #033[0m| 22/27 [03:36<00:49,  9.80s/it]\u001B[0m\n",
      "\u001B[35mstep 21 is completed and loss is 1.2304186820983887\u001B[0m\n",
      "\u001B[35mTraining Epoch2:  81%|#033[34m████████▏ #033[0m| 22/27 [03:36<00:49,  9.80s/it]\u001B[0m\n",
      "\u001B[35mTraining Epoch2:  81%|#033[34m████████▏ #033[0m| 22/27 [03:36<00:49,  9.80s/it]\u001B[0m\n",
      "\u001B[35mTraining Epoch2:  85%|#033[34m████████▌ #033[0m| 23/27 [03:45<00:39,  9.80s/it]\u001B[0m\n",
      "\u001B[35mstep 22 is completed and loss is 1.380005121231079\u001B[0m\n",
      "\u001B[35mTraining Epoch2:  85%|#033[34m████████▌ #033[0m| 23/27 [03:45<00:39,  9.80s/it]\u001B[0m\n",
      "\u001B[35mTraining Epoch2:  85%|#033[34m████████▌ #033[0m| 23/27 [03:45<00:39,  9.80s/it]\u001B[0m\n",
      "\u001B[35mTraining Epoch2:  85%|#033[34m████████▌ #033[0m| 23/27 [03:45<00:39,  9.80s/it]\u001B[0m\n",
      "\u001B[34mstep 22 is completed and loss is 1.380005121231079\u001B[0m\n",
      "\u001B[34mTraining Epoch2:  85%|#033[34m████████▌ #033[0m| 23/27 [03:45<00:39,  9.81s/it]\u001B[0m\n",
      "\u001B[34mTraining Epoch2:  85%|#033[34m████████▌ #033[0m| 23/27 [03:45<00:39,  9.81s/it]\u001B[0m\n",
      "\u001B[34mTraining Epoch2:  85%|#033[34m████████▌ #033[0m| 23/27 [03:45<00:39,  9.81s/it]\u001B[0m\n",
      "\u001B[34mTraining Epoch2:  85%|#033[34m████████▌ #033[0m| 23/27 [03:45<00:39,  9.81s/it]\u001B[0m\n",
      "\u001B[34mTraining Epoch2:  89%|#033[34m████████▉ #033[0m| 24/27 [03:55<00:29,  9.82s/it]\u001B[0m\n",
      "\u001B[34mstep 23 is completed and loss is 1.2222181558609009\u001B[0m\n",
      "\u001B[34mTraining Epoch2:  89%|#033[34m████████▉ #033[0m| 24/27 [03:55<00:29,  9.82s/it]\u001B[0m\n",
      "\u001B[34mTraining Epoch2:  89%|#033[34m████████▉ #033[0m| 24/27 [03:55<00:29,  9.82s/it]\u001B[0m\n",
      "\u001B[34mTraining Epoch2:  89%|#033[34m████████▉ #033[0m| 24/27 [03:55<00:29,  9.82s/it]\u001B[0m\n",
      "\u001B[35mTraining Epoch2:  89%|#033[34m████████▉ #033[0m| 24/27 [03:55<00:29,  9.80s/it]\u001B[0m\n",
      "\u001B[35mstep 23 is completed and loss is 1.2222181558609009\u001B[0m\n",
      "\u001B[35mTraining Epoch2:  89%|#033[34m████████▉ #033[0m| 24/27 [03:55<00:29,  9.80s/it]\u001B[0m\n",
      "\u001B[35mTraining Epoch2:  89%|#033[34m████████▉ #033[0m| 24/27 [03:55<00:29,  9.80s/it]\u001B[0m\n",
      "\u001B[35mTraining Epoch2:  89%|#033[34m████████▉ #033[0m| 24/27 [03:55<00:29,  9.80s/it]\u001B[0m\n",
      "\u001B[34mTraining Epoch2:  93%|#033[34m█████████▎#033[0m| 25/27 [04:05<00:19,  9.85s/it]\u001B[0m\n",
      "\u001B[34mTraining Epoch2:  93%|#033[34m█████████▎#033[0m| 25/27 [04:05<00:19,  9.85s/it]\u001B[0m\n",
      "\u001B[34mstep 24 is completed and loss is 1.4600244760513306\u001B[0m\n",
      "\u001B[34mTraining Epoch2:  93%|#033[34m█████████▎#033[0m| 25/27 [04:05<00:19,  9.85s/it]\u001B[0m\n",
      "\u001B[34mTraining Epoch2:  93%|#033[34m█████████▎#033[0m| 25/27 [04:05<00:19,  9.85s/it]\u001B[0m\n",
      "\u001B[35mTraining Epoch2:  93%|#033[34m█████████▎#033[0m| 25/27 [04:05<00:19,  9.81s/it]\u001B[0m\n",
      "\u001B[35mTraining Epoch2:  93%|#033[34m█████████▎#033[0m| 25/27 [04:05<00:19,  9.81s/it]\u001B[0m\n",
      "\u001B[35mTraining Epoch2:  93%|#033[34m█████████▎#033[0m| 25/27 [04:05<00:19,  9.81s/it]\u001B[0m\n",
      "\u001B[35mstep 24 is completed and loss is 1.4600244760513306\u001B[0m\n",
      "\u001B[35mTraining Epoch2:  93%|#033[34m█████████▎#033[0m| 25/27 [04:05<00:19,  9.81s/it]\u001B[0m\n",
      "\u001B[34mTraining Epoch2:  96%|#033[34m█████████▋#033[0m| 26/27 [04:15<00:09,  9.84s/it]\u001B[0m\n",
      "\u001B[34mstep 25 is completed and loss is 1.1017210483551025\u001B[0m\n",
      "\u001B[34mTraining Epoch2:  96%|#033[34m█████████▋#033[0m| 26/27 [04:15<00:09,  9.84s/it]\u001B[0m\n",
      "\u001B[34mTraining Epoch2:  96%|#033[34m█████████▋#033[0m| 26/27 [04:15<00:09,  9.84s/it]\u001B[0m\n",
      "\u001B[34mTraining Epoch2:  96%|#033[34m█████████▋#033[0m| 26/27 [04:15<00:09,  9.84s/it]\u001B[0m\n",
      "\u001B[35mTraining Epoch2:  96%|#033[34m█████████▋#033[0m| 26/27 [04:15<00:09,  9.87s/it]\u001B[0m\n",
      "\u001B[35mTraining Epoch2:  96%|#033[34m█████████▋#033[0m| 26/27 [04:15<00:09,  9.87s/it]\u001B[0m\n",
      "\u001B[35mTraining Epoch2:  96%|#033[34m█████████▋#033[0m| 26/27 [04:15<00:09,  9.87s/it]\u001B[0m\n",
      "\u001B[35mstep 25 is completed and loss is 1.1017210483551025\u001B[0m\n",
      "\u001B[35mTraining Epoch2:  96%|#033[34m█████████▋#033[0m| 26/27 [04:15<00:09,  9.87s/it]\u001B[0m\n",
      "\u001B[34mTraining Epoch2: 100%|#033[34m██████████#033[0m| 27/27 [04:25<00:00,  9.83s/it]\u001B[0m\n",
      "\u001B[34mstep 26 is completed and loss is 1.0439342260360718\u001B[0m\n",
      "\u001B[34mTraining Epoch2: 100%|#033[34m██████████#033[0m| 27/27 [04:25<00:00,  9.83s/it]\u001B[0m\n",
      "\u001B[34mTraining Epoch2: 100%|#033[34m██████████#033[0m| 27/27 [04:25<00:00,  9.83s/it]\u001B[0m\n",
      "\u001B[34mTraining Epoch2: 100%|#033[34m██████████#033[0m| 27/27 [04:25<00:00,  9.83s/it]\u001B[0m\n",
      "\u001B[34mTraining Epoch2: 100%|#033[34m██████████#033[0m| 27/27 [04:25<00:00,  9.83s/it]\u001B[0m\n",
      "\u001B[34mTraining Epoch2: 100%|#033[34m██████████#033[0m| 27/27 [04:25<00:00,  9.83s/it]\u001B[0m\n",
      "\u001B[34mTraining Epoch2: 100%|#033[34m██████████#033[0m| 27/27 [04:25<00:00,  9.83s/it]\u001B[0m\n",
      "\u001B[34mTraining Epoch2: 100%|#033[34m██████████#033[0m| 27/27 [04:25<00:00,  9.83s/it]\u001B[0m\n",
      "\u001B[34mMax CUDA memory allocated was 8 GB\u001B[0m\n",
      "\u001B[34mMax CUDA memory reserved was 9 GB\u001B[0m\n",
      "\u001B[34mPeak active CUDA memory was 8 GB\u001B[0m\n",
      "\u001B[34mCuda Malloc retires : 0\u001B[0m\n",
      "\u001B[34mCPU Total Peak Memory consumed during the train (max): 2 GB\u001B[0m\n",
      "\u001B[34mevaluating Epoch:   0%|#033[32m          #033[0m| 0/28 [00:00<?, ?it/s]\u001B[0m\n",
      "\u001B[34mevaluating Epoch:   0%|#033[32m          #033[0m| 0/28 [00:00<?, ?it/s]\u001B[0m\n",
      "\u001B[34mevaluating Epoch:   0%|#033[32m          #033[0m| 0/28 [00:00<?, ?it/s]\u001B[0m\n",
      "\u001B[34mevaluating Epoch:   0%|#033[32m          #033[0m| 0/28 [00:00<?, ?it/s]\u001B[0m\n",
      "\u001B[35mTraining Epoch2: 100%|#033[34m██████████#033[0m| 27/27 [04:25<00:00,  9.89s/it]\u001B[0m\n",
      "\u001B[35mTraining Epoch2: 100%|#033[34m██████████#033[0m| 27/27 [04:25<00:00,  9.83s/it]\u001B[0m\n",
      "\u001B[35mTraining Epoch2: 100%|#033[34m██████████#033[0m| 27/27 [04:25<00:00,  9.89s/it]\u001B[0m\n",
      "\u001B[35mTraining Epoch2: 100%|#033[34m██████████#033[0m| 27/27 [04:25<00:00,  9.83s/it]\u001B[0m\n",
      "\u001B[35mstep 26 is completed and loss is 1.0439342260360718\u001B[0m\n",
      "\u001B[35mTraining Epoch2: 100%|#033[34m██████████#033[0m| 27/27 [04:25<00:00,  9.89s/it]\u001B[0m\n",
      "\u001B[35mTraining Epoch2: 100%|#033[34m██████████#033[0m| 27/27 [04:25<00:00,  9.83s/it]\u001B[0m\n",
      "\u001B[35mTraining Epoch2: 100%|#033[34m██████████#033[0m| 27/27 [04:25<00:00,  9.89s/it]\u001B[0m\n",
      "\u001B[35mTraining Epoch2: 100%|#033[34m██████████#033[0m| 27/27 [04:25<00:00,  9.83s/it]\u001B[0m\n",
      "\u001B[35mMax CUDA memory allocated was 8 GB\u001B[0m\n",
      "\u001B[35mMax CUDA memory reserved was 9 GB\u001B[0m\n",
      "\u001B[35mPeak active CUDA memory was 8 GB\u001B[0m\n",
      "\u001B[35mCuda Malloc retires : 0\u001B[0m\n",
      "\u001B[35mCPU Total Peak Memory consumed during the train (max): 2 GB\u001B[0m\n",
      "\u001B[35mevaluating Epoch:   0%|#033[32m          #033[0m| 0/28 [00:00<?, ?it/s]\u001B[0m\n",
      "\u001B[35mevaluating Epoch:   0%|#033[32m          #033[0m| 0/28 [00:00<?, ?it/s]\u001B[0m\n",
      "\u001B[35mevaluating Epoch:   0%|#033[32m          #033[0m| 0/28 [00:00<?, ?it/s]\u001B[0m\n",
      "\u001B[35mevaluating Epoch:   0%|#033[32m          #033[0m| 0/28 [00:00<?, ?it/s]\u001B[0m\n",
      "\u001B[34mevaluating Epoch:   4%|#033[32m▎         #033[0m| 1/28 [00:03<01:37,  3.59s/it]\u001B[0m\n",
      "\u001B[34mevaluating Epoch:   4%|#033[32m▎         #033[0m| 1/28 [00:03<01:36,  3.58s/it]#015evaluating Epoch:   4%|#033[32m▎         #033[0m| 1/28 [00:03<01:36,  3.59s/it]\u001B[0m\n",
      "\u001B[34mevaluating Epoch:   4%|#033[32m▎         #033[0m| 1/28 [00:03<01:36,  3.59s/it]\u001B[0m\n",
      "\u001B[35mevaluating Epoch:   4%|#033[32m▎         #033[0m| 1/28 [00:03<01:36,  3.59s/it]#015evaluating Epoch:   4%|#033[32m▎         #033[0m| 1/28 [00:03<01:36,  3.58s/it]\u001B[0m\n",
      "\u001B[35mevaluating Epoch:   4%|#033[32m▎         #033[0m| 1/28 [00:03<01:36,  3.59s/it]\u001B[0m\n",
      "\u001B[35mevaluating Epoch:   4%|#033[32m▎         #033[0m| 1/28 [00:03<01:36,  3.59s/it]\u001B[0m\n",
      "\u001B[34mevaluating Epoch:   7%|#033[32m▋         #033[0m| 2/28 [00:07<01:31,  3.52s/it]\u001B[0m\n",
      "\u001B[34mevaluating Epoch:   7%|#033[32m▋         #033[0m| 2/28 [00:07<01:31,  3.52s/it]\u001B[0m\n",
      "\u001B[34mevaluating Epoch:   7%|#033[32m▋         #033[0m| 2/28 [00:07<01:31,  3.52s/it]\u001B[0m\n",
      "\u001B[34mevaluating Epoch:   7%|#033[32m▋         #033[0m| 2/28 [00:07<01:31,  3.52s/it]\u001B[0m\n",
      "\u001B[35mevaluating Epoch:   7%|#033[32m▋         #033[0m| 2/28 [00:07<01:31,  3.52s/it]#015evaluating Epoch:   7%|#033[32m▋         #033[0m| 2/28 [00:07<01:31,  3.51s/it]\u001B[0m\n",
      "\u001B[35mevaluating Epoch:   7%|#033[32m▋         #033[0m| 2/28 [00:07<01:31,  3.52s/it]\u001B[0m\n",
      "\u001B[35mevaluating Epoch:   7%|#033[32m▋         #033[0m| 2/28 [00:07<01:31,  3.52s/it]\u001B[0m\n",
      "\u001B[34mevaluating Epoch:  11%|#033[32m█         #033[0m| 3/28 [00:10<01:27,  3.50s/it]#015evaluating Epoch:  11%|#033[32m█         #033[0m| 3/28 [00:10<01:27,  3.50s/it]#015evaluating Epoch:  11%|#033[32m█         #033[0m| 3/28 [00:10<01:27,  3.50s/it]\u001B[0m\n",
      "\u001B[34mevaluating Epoch:  11%|#033[32m█         #033[0m| 3/28 [00:10<01:27,  3.50s/it]\u001B[0m\n",
      "\u001B[35mevaluating Epoch:  11%|#033[32m█         #033[0m| 3/28 [00:10<01:27,  3.49s/it]\u001B[0m\n",
      "\u001B[35mevaluating Epoch:  11%|#033[32m█         #033[0m| 3/28 [00:10<01:27,  3.49s/it]\u001B[0m\n",
      "\u001B[35mevaluating Epoch:  11%|#033[32m█         #033[0m| 3/28 [00:10<01:27,  3.49s/it]\u001B[0m\n",
      "\u001B[35mevaluating Epoch:  11%|#033[32m█         #033[0m| 3/28 [00:10<01:27,  3.49s/it]\u001B[0m\n",
      "\u001B[34mevaluating Epoch:  14%|#033[32m█▍        #033[0m| 4/28 [00:13<01:23,  3.49s/it]\u001B[0m\n",
      "\u001B[34mevaluating Epoch:  14%|#033[32m█▍        #033[0m| 4/28 [00:13<01:23,  3.49s/it]\u001B[0m\n",
      "\u001B[34mevaluating Epoch:  14%|#033[32m█▍        #033[0m| 4/28 [00:13<01:23,  3.48s/it]\u001B[0m\n",
      "\u001B[34mevaluating Epoch:  14%|#033[32m█▍        #033[0m| 4/28 [00:14<01:23,  3.49s/it]\u001B[0m\n",
      "\u001B[35mevaluating Epoch:  14%|#033[32m█▍        #033[0m| 4/28 [00:13<01:23,  3.48s/it]\u001B[0m\n",
      "\u001B[35mevaluating Epoch:  14%|#033[32m█▍        #033[0m| 4/28 [00:13<01:23,  3.48s/it]\u001B[0m\n",
      "\u001B[35mevaluating Epoch:  14%|#033[32m█▍        #033[0m| 4/28 [00:13<01:23,  3.48s/it]\u001B[0m\n",
      "\u001B[35mevaluating Epoch:  14%|#033[32m█▍        #033[0m| 4/28 [00:13<01:23,  3.48s/it]\u001B[0m\n",
      "\u001B[34mevaluating Epoch:  18%|#033[32m█▊        #033[0m| 5/28 [00:17<01:20,  3.48s/it]\u001B[0m\n",
      "\u001B[34mevaluating Epoch:  18%|#033[32m█▊        #033[0m| 5/28 [00:17<01:20,  3.48s/it]\u001B[0m\n",
      "\u001B[34mevaluating Epoch:  18%|#033[32m█▊        #033[0m| 5/28 [00:17<01:20,  3.48s/it]\u001B[0m\n",
      "\u001B[34mevaluating Epoch:  18%|#033[32m█▊        #033[0m| 5/28 [00:17<01:20,  3.49s/it]\u001B[0m\n",
      "\u001B[35mevaluating Epoch:  18%|#033[32m█▊        #033[0m| 5/28 [00:17<01:19,  3.47s/it]\u001B[0m\n",
      "\u001B[35mevaluating Epoch:  18%|#033[32m█▊        #033[0m| 5/28 [00:17<01:19,  3.47s/it]\u001B[0m\n",
      "\u001B[35mevaluating Epoch:  18%|#033[32m█▊        #033[0m| 5/28 [00:17<01:19,  3.47s/it]\u001B[0m\n",
      "\u001B[35mevaluating Epoch:  18%|#033[32m█▊        #033[0m| 5/28 [00:17<01:19,  3.47s/it]\u001B[0m\n",
      "\u001B[34mevaluating Epoch:  21%|#033[32m██▏       #033[0m| 6/28 [00:20<01:16,  3.48s/it]\u001B[0m\n",
      "\u001B[34mevaluating Epoch:  21%|#033[32m██▏       #033[0m| 6/28 [00:20<01:16,  3.48s/it]#015evaluating Epoch:  21%|#033[32m██▏       #033[0m| 6/28 [00:20<01:16,  3.48s/it]\u001B[0m\n",
      "\u001B[34mevaluating Epoch:  21%|#033[32m██▏       #033[0m| 6/28 [00:20<01:16,  3.48s/it]\u001B[0m\n",
      "\u001B[35mevaluating Epoch:  21%|#033[32m██▏       #033[0m| 6/28 [00:20<01:16,  3.47s/it]\u001B[0m\n",
      "\u001B[35mevaluating Epoch:  21%|#033[32m██▏       #033[0m| 6/28 [00:20<01:16,  3.47s/it]\u001B[0m\n",
      "\u001B[35mevaluating Epoch:  21%|#033[32m██▏       #033[0m| 6/28 [00:20<01:16,  3.47s/it]\u001B[0m\n",
      "\u001B[35mevaluating Epoch:  21%|#033[32m██▏       #033[0m| 6/28 [00:20<01:16,  3.47s/it]\u001B[0m\n",
      "\u001B[34mevaluating Epoch:  25%|#033[32m██▌       #033[0m| 7/28 [00:24<01:12,  3.47s/it]#015evaluating Epoch:  25%|#033[32m██▌       #033[0m| 7/28 [00:24<01:12,  3.47s/it]\u001B[0m\n",
      "\u001B[34mevaluating Epoch:  25%|#033[32m██▌       #033[0m| 7/28 [00:24<01:12,  3.47s/it]#015evaluating Epoch:  25%|#033[32m██▌       #033[0m| 7/28 [00:24<01:12,  3.47s/it]\u001B[0m\n",
      "\u001B[35mevaluating Epoch:  25%|#033[32m██▌       #033[0m| 7/28 [00:24<01:12,  3.47s/it]\u001B[0m\n",
      "\u001B[35mevaluating Epoch:  25%|#033[32m██▌       #033[0m| 7/28 [00:24<01:12,  3.47s/it]\u001B[0m\n",
      "\u001B[35mevaluating Epoch:  25%|#033[32m██▌       #033[0m| 7/28 [00:24<01:12,  3.47s/it]\u001B[0m\n",
      "\u001B[35mevaluating Epoch:  25%|#033[32m██▌       #033[0m| 7/28 [00:24<01:12,  3.47s/it]\u001B[0m\n",
      "\u001B[34mevaluating Epoch:  29%|#033[32m██▊       #033[0m| 8/28 [00:27<01:09,  3.47s/it]#015evaluating Epoch:  29%|#033[32m██▊       #033[0m| 8/28 [00:27<01:09,  3.47s/it]\u001B[0m\n",
      "\u001B[34mevaluating Epoch:  29%|#033[32m██▊       #033[0m| 8/28 [00:27<01:09,  3.47s/it]\u001B[0m\n",
      "\u001B[34mevaluating Epoch:  29%|#033[32m██▊       #033[0m| 8/28 [00:27<01:09,  3.47s/it]\u001B[0m\n",
      "\u001B[35mevaluating Epoch:  29%|#033[32m██▊       #033[0m| 8/28 [00:27<01:09,  3.46s/it]\u001B[0m\n",
      "\u001B[35mevaluating Epoch:  29%|#033[32m██▊       #033[0m| 8/28 [00:27<01:09,  3.46s/it]#015evaluating Epoch:  29%|#033[32m██▊       #033[0m| 8/28 [00:27<01:09,  3.46s/it]\u001B[0m\n",
      "\u001B[35mevaluating Epoch:  29%|#033[32m██▊       #033[0m| 8/28 [00:27<01:09,  3.46s/it]\u001B[0m\n",
      "\u001B[34mevaluating Epoch:  32%|#033[32m███▏      #033[0m| 9/28 [00:31<01:05,  3.47s/it]#015evaluating Epoch:  32%|#033[32m███▏      #033[0m| 9/28 [00:31<01:05,  3.47s/it]\u001B[0m\n",
      "\u001B[34mevaluating Epoch:  32%|#033[32m███▏      #033[0m| 9/28 [00:31<01:05,  3.47s/it]\u001B[0m\n",
      "\u001B[34mevaluating Epoch:  32%|#033[32m███▏      #033[0m| 9/28 [00:31<01:05,  3.47s/it]\u001B[0m\n",
      "\u001B[35mevaluating Epoch:  32%|#033[32m███▏      #033[0m| 9/28 [00:31<01:05,  3.46s/it]\u001B[0m\n",
      "\u001B[35mevaluating Epoch:  32%|#033[32m███▏      #033[0m| 9/28 [00:31<01:05,  3.46s/it]#015evaluating Epoch:  32%|#033[32m███▏      #033[0m| 9/28 [00:31<01:05,  3.46s/it]\u001B[0m\n",
      "\u001B[35mevaluating Epoch:  32%|#033[32m███▏      #033[0m| 9/28 [00:31<01:05,  3.46s/it]\u001B[0m\n",
      "\u001B[34mevaluating Epoch:  36%|#033[32m███▌      #033[0m| 10/28 [00:34<01:02,  3.47s/it]\u001B[0m\n",
      "\u001B[34mevaluating Epoch:  36%|#033[32m███▌      #033[0m| 10/28 [00:34<01:02,  3.47s/it]#015evaluating Epoch:  36%|#033[32m███▌      #033[0m| 10/28 [00:34<01:02,  3.47s/it]\u001B[0m\n",
      "\u001B[34mevaluating Epoch:  36%|#033[32m███▌      #033[0m| 10/28 [00:34<01:02,  3.47s/it]\u001B[0m\n",
      "\u001B[35mevaluating Epoch:  36%|#033[32m███▌      #033[0m| 10/28 [00:34<01:02,  3.46s/it]\u001B[0m\n",
      "\u001B[35mevaluating Epoch:  36%|#033[32m███▌      #033[0m| 10/28 [00:34<01:02,  3.46s/it]\u001B[0m\n",
      "\u001B[35mevaluating Epoch:  36%|#033[32m███▌      #033[0m| 10/28 [00:34<01:02,  3.46s/it]\u001B[0m\n",
      "\u001B[35mevaluating Epoch:  36%|#033[32m███▌      #033[0m| 10/28 [00:34<01:02,  3.46s/it]\u001B[0m\n",
      "\u001B[34mevaluating Epoch:  39%|#033[32m███▉      #033[0m| 11/28 [00:38<00:58,  3.47s/it]#015evaluating Epoch:  39%|#033[32m███▉      #033[0m| 11/28 [00:38<00:58,  3.47s/it]#015evaluating Epoch:  39%|#033[32m███▉      #033[0m| 11/28 [00:38<00:58,  3.47s/it]\u001B[0m\n",
      "\u001B[34mevaluating Epoch:  39%|#033[32m███▉      #033[0m| 11/28 [00:38<00:58,  3.47s/it]\u001B[0m\n",
      "\u001B[35mevaluating Epoch:  39%|#033[32m███▉      #033[0m| 11/28 [00:38<00:58,  3.46s/it]\u001B[0m\n",
      "\u001B[35mevaluating Epoch:  39%|#033[32m███▉      #033[0m| 11/28 [00:38<00:58,  3.46s/it]\u001B[0m\n",
      "\u001B[35mevaluating Epoch:  39%|#033[32m███▉      #033[0m| 11/28 [00:38<00:58,  3.46s/it]\u001B[0m\n",
      "\u001B[35mevaluating Epoch:  39%|#033[32m███▉      #033[0m| 11/28 [00:38<00:58,  3.46s/it]\u001B[0m\n",
      "\u001B[34mevaluating Epoch:  43%|#033[32m████▎     #033[0m| 12/28 [00:41<00:55,  3.47s/it]\u001B[0m\n",
      "\u001B[34mevaluating Epoch:  43%|#033[32m████▎     #033[0m| 12/28 [00:41<00:55,  3.47s/it]\u001B[0m\n",
      "\u001B[34mevaluating Epoch:  43%|#033[32m████▎     #033[0m| 12/28 [00:41<00:55,  3.47s/it]\u001B[0m\n",
      "\u001B[34mevaluating Epoch:  43%|#033[32m████▎     #033[0m| 12/28 [00:41<00:55,  3.47s/it]\u001B[0m\n",
      "\u001B[35mevaluating Epoch:  43%|#033[32m████▎     #033[0m| 12/28 [00:41<00:55,  3.46s/it]#015evaluating Epoch:  43%|#033[32m████▎     #033[0m| 12/28 [00:41<00:55,  3.46s/it]\u001B[0m\n",
      "\u001B[35mevaluating Epoch:  43%|#033[32m████▎     #033[0m| 12/28 [00:41<00:55,  3.46s/it]\u001B[0m\n",
      "\u001B[35mevaluating Epoch:  43%|#033[32m████▎     #033[0m| 12/28 [00:41<00:55,  3.46s/it]\u001B[0m\n",
      "\u001B[34mevaluating Epoch:  46%|#033[32m████▋     #033[0m| 13/28 [00:45<00:52,  3.47s/it]\u001B[0m\n",
      "\u001B[34mevaluating Epoch:  46%|#033[32m████▋     #033[0m| 13/28 [00:45<00:52,  3.47s/it]\u001B[0m\n",
      "\u001B[34mevaluating Epoch:  46%|#033[32m████▋     #033[0m| 13/28 [00:45<00:52,  3.47s/it]\u001B[0m\n",
      "\u001B[34mevaluating Epoch:  46%|#033[32m████▋     #033[0m| 13/28 [00:45<00:52,  3.47s/it]\u001B[0m\n",
      "\u001B[35mevaluating Epoch:  46%|#033[32m████▋     #033[0m| 13/28 [00:45<00:51,  3.46s/it]\u001B[0m\n",
      "\u001B[35mevaluating Epoch:  46%|#033[32m████▋     #033[0m| 13/28 [00:45<00:51,  3.46s/it]\u001B[0m\n",
      "\u001B[35mevaluating Epoch:  46%|#033[32m████▋     #033[0m| 13/28 [00:45<00:51,  3.46s/it]\u001B[0m\n",
      "\u001B[35mevaluating Epoch:  46%|#033[32m████▋     #033[0m| 13/28 [00:45<00:51,  3.46s/it]\u001B[0m\n",
      "\u001B[34mevaluating Epoch:  50%|#033[32m█████     #033[0m| 14/28 [00:48<00:48,  3.47s/it]\u001B[0m\n",
      "\u001B[34mevaluating Epoch:  50%|#033[32m█████     #033[0m| 14/28 [00:48<00:48,  3.47s/it]#015evaluating Epoch:  50%|#033[32m█████     #033[0m| 14/28 [00:48<00:48,  3.47s/it]\u001B[0m\n",
      "\u001B[34mevaluating Epoch:  50%|#033[32m█████     #033[0m| 14/28 [00:48<00:48,  3.47s/it]\u001B[0m\n",
      "\u001B[35mevaluating Epoch:  50%|#033[32m█████     #033[0m| 14/28 [00:48<00:48,  3.46s/it]\u001B[0m\n",
      "\u001B[35mevaluating Epoch:  50%|#033[32m█████     #033[0m| 14/28 [00:48<00:48,  3.46s/it]\u001B[0m\n",
      "\u001B[35mevaluating Epoch:  50%|#033[32m█████     #033[0m| 14/28 [00:48<00:48,  3.46s/it]\u001B[0m\n",
      "\u001B[35mevaluating Epoch:  50%|#033[32m█████     #033[0m| 14/28 [00:48<00:48,  3.46s/it]\u001B[0m\n",
      "\u001B[34mevaluating Epoch:  54%|#033[32m█████▎    #033[0m| 15/28 [00:52<00:45,  3.47s/it]\u001B[0m\n",
      "\u001B[34mevaluating Epoch:  54%|#033[32m█████▎    #033[0m| 15/28 [00:52<00:45,  3.47s/it]#015evaluating Epoch:  54%|#033[32m█████▎    #033[0m| 15/28 [00:52<00:45,  3.47s/it]\u001B[0m\n",
      "\u001B[34mevaluating Epoch:  54%|#033[32m█████▎    #033[0m| 15/28 [00:52<00:45,  3.47s/it]\u001B[0m\n",
      "\u001B[35mevaluating Epoch:  54%|#033[32m█████▎    #033[0m| 15/28 [00:52<00:45,  3.46s/it]\u001B[0m\n",
      "\u001B[35mevaluating Epoch:  54%|#033[32m█████▎    #033[0m| 15/28 [00:52<00:45,  3.46s/it]\u001B[0m\n",
      "\u001B[35mevaluating Epoch:  54%|#033[32m█████▎    #033[0m| 15/28 [00:52<00:45,  3.46s/it]\u001B[0m\n",
      "\u001B[35mevaluating Epoch:  54%|#033[32m█████▎    #033[0m| 15/28 [00:52<00:45,  3.46s/it]\u001B[0m\n",
      "\u001B[34mevaluating Epoch:  57%|#033[32m█████▋    #033[0m| 16/28 [00:55<00:41,  3.47s/it]\u001B[0m\n",
      "\u001B[34mevaluating Epoch:  57%|#033[32m█████▋    #033[0m| 16/28 [00:55<00:41,  3.47s/it]#015evaluating Epoch:  57%|#033[32m█████▋    #033[0m| 16/28 [00:55<00:41,  3.47s/it]#015evaluating Epoch:  57%|#033[32m█████▋    #033[0m| 16/28 [00:55<00:41,  3.47s/it]\u001B[0m\n",
      "\u001B[35mevaluating Epoch:  57%|#033[32m█████▋    #033[0m| 16/28 [00:55<00:41,  3.46s/it]\u001B[0m\n",
      "\u001B[35mevaluating Epoch:  57%|#033[32m█████▋    #033[0m| 16/28 [00:55<00:41,  3.46s/it]\u001B[0m\n",
      "\u001B[35mevaluating Epoch:  57%|#033[32m█████▋    #033[0m| 16/28 [00:55<00:41,  3.46s/it]\u001B[0m\n",
      "\u001B[35mevaluating Epoch:  57%|#033[32m█████▋    #033[0m| 16/28 [00:55<00:41,  3.46s/it]\u001B[0m\n",
      "\u001B[34mevaluating Epoch:  61%|#033[32m██████    #033[0m| 17/28 [00:59<00:38,  3.47s/it]#015evaluating Epoch:  61%|#033[32m██████    #033[0m| 17/28 [00:59<00:38,  3.47s/it]\u001B[0m\n",
      "\u001B[34mevaluating Epoch:  61%|#033[32m██████    #033[0m| 17/28 [00:59<00:38,  3.47s/it]#015evaluating Epoch:  61%|#033[32m██████    #033[0m| 17/28 [00:59<00:38,  3.47s/it]\u001B[0m\n",
      "\u001B[35mevaluating Epoch:  61%|#033[32m██████    #033[0m| 17/28 [00:58<00:38,  3.46s/it]\u001B[0m\n",
      "\u001B[35mevaluating Epoch:  61%|#033[32m██████    #033[0m| 17/28 [00:58<00:38,  3.46s/it]\u001B[0m\n",
      "\u001B[35mevaluating Epoch:  61%|#033[32m██████    #033[0m| 17/28 [00:58<00:38,  3.46s/it]\u001B[0m\n",
      "\u001B[35mevaluating Epoch:  61%|#033[32m██████    #033[0m| 17/28 [00:58<00:38,  3.46s/it]\u001B[0m\n",
      "\u001B[34mevaluating Epoch:  64%|#033[32m██████▍   #033[0m| 18/28 [01:02<00:34,  3.47s/it]\u001B[0m\n",
      "\u001B[34mevaluating Epoch:  64%|#033[32m██████▍   #033[0m| 18/28 [01:02<00:34,  3.47s/it]#015evaluating Epoch:  64%|#033[32m██████▍   #033[0m| 18/28 [01:02<00:34,  3.47s/it]\u001B[0m\n",
      "\u001B[34mevaluating Epoch:  64%|#033[32m██████▍   #033[0m| 18/28 [01:02<00:34,  3.47s/it]\u001B[0m\n",
      "\u001B[35mevaluating Epoch:  64%|#033[32m██████▍   #033[0m| 18/28 [01:02<00:34,  3.46s/it]\u001B[0m\n",
      "\u001B[35mevaluating Epoch:  64%|#033[32m██████▍   #033[0m| 18/28 [01:02<00:34,  3.46s/it]\u001B[0m\n",
      "\u001B[35mevaluating Epoch:  64%|#033[32m██████▍   #033[0m| 18/28 [01:02<00:34,  3.46s/it]\u001B[0m\n",
      "\u001B[35mevaluating Epoch:  64%|#033[32m██████▍   #033[0m| 18/28 [01:02<00:34,  3.46s/it]\u001B[0m\n",
      "\u001B[34mevaluating Epoch:  68%|#033[32m██████▊   #033[0m| 19/28 [01:06<00:31,  3.47s/it]\u001B[0m\n",
      "\u001B[34mevaluating Epoch:  68%|#033[32m██████▊   #033[0m| 19/28 [01:06<00:31,  3.47s/it]#015evaluating Epoch:  68%|#033[32m██████▊   #033[0m| 19/28 [01:06<00:31,  3.47s/it]\u001B[0m\n",
      "\u001B[34mevaluating Epoch:  68%|#033[32m██████▊   #033[0m| 19/28 [01:06<00:31,  3.47s/it]\u001B[0m\n",
      "\u001B[35mevaluating Epoch:  68%|#033[32m██████▊   #033[0m| 19/28 [01:05<00:31,  3.46s/it]#015evaluating Epoch:  68%|#033[32m██████▊   #033[0m| 19/28 [01:05<00:31,  3.46s/it]\u001B[0m\n",
      "\u001B[35mevaluating Epoch:  68%|#033[32m██████▊   #033[0m| 19/28 [01:05<00:31,  3.46s/it]\u001B[0m\n",
      "\u001B[35mevaluating Epoch:  68%|#033[32m██████▊   #033[0m| 19/28 [01:05<00:31,  3.46s/it]\u001B[0m\n",
      "\u001B[34mevaluating Epoch:  71%|#033[32m███████▏  #033[0m| 20/28 [01:09<00:27,  3.47s/it]\u001B[0m\n",
      "\u001B[34mevaluating Epoch:  71%|#033[32m███████▏  #033[0m| 20/28 [01:09<00:27,  3.47s/it]\u001B[0m\n",
      "\u001B[34mevaluating Epoch:  71%|#033[32m███████▏  #033[0m| 20/28 [01:09<00:27,  3.47s/it]\u001B[0m\n",
      "\u001B[34mevaluating Epoch:  71%|#033[32m███████▏  #033[0m| 20/28 [01:09<00:27,  3.47s/it]\u001B[0m\n",
      "\u001B[35mevaluating Epoch:  71%|#033[32m███████▏  #033[0m| 20/28 [01:09<00:27,  3.46s/it]\u001B[0m\n",
      "\u001B[35mevaluating Epoch:  71%|#033[32m███████▏  #033[0m| 20/28 [01:09<00:27,  3.46s/it]\u001B[0m\n",
      "\u001B[35mevaluating Epoch:  71%|#033[32m███████▏  #033[0m| 20/28 [01:09<00:27,  3.46s/it]\u001B[0m\n",
      "\u001B[35mevaluating Epoch:  71%|#033[32m███████▏  #033[0m| 20/28 [01:09<00:27,  3.46s/it]\u001B[0m\n",
      "\u001B[34mevaluating Epoch:  75%|#033[32m███████▌  #033[0m| 21/28 [01:13<00:24,  3.47s/it]#015evaluating Epoch:  75%|#033[32m███████▌  #033[0m| 21/28 [01:13<00:24,  3.47s/it]#015evaluating Epoch:  75%|#033[32m███████▌  #033[0m| 21/28 [01:13<00:24,  3.47s/it]#015evaluating Epoch:  75%|#033[32m███████▌  #033[0m| 21/28 [01:13<00:24,  3.47s/it]\u001B[0m\n",
      "\u001B[35mevaluating Epoch:  75%|#033[32m███████▌  #033[0m| 21/28 [01:12<00:24,  3.46s/it]\u001B[0m\n",
      "\u001B[35mevaluating Epoch:  75%|#033[32m███████▌  #033[0m| 21/28 [01:12<00:24,  3.46s/it]\u001B[0m\n",
      "\u001B[35mevaluating Epoch:  75%|#033[32m███████▌  #033[0m| 21/28 [01:12<00:24,  3.46s/it]\u001B[0m\n",
      "\u001B[35mevaluating Epoch:  75%|#033[32m███████▌  #033[0m| 21/28 [01:12<00:24,  3.46s/it]\u001B[0m\n",
      "\u001B[34mevaluating Epoch:  79%|#033[32m███████▊  #033[0m| 22/28 [01:16<00:20,  3.47s/it]\u001B[0m\n",
      "\u001B[34mevaluating Epoch:  79%|#033[32m███████▊  #033[0m| 22/28 [01:16<00:20,  3.47s/it]\u001B[0m\n",
      "\u001B[34mevaluating Epoch:  79%|#033[32m███████▊  #033[0m| 22/28 [01:16<00:20,  3.47s/it]\u001B[0m\n",
      "\u001B[34mevaluating Epoch:  79%|#033[32m███████▊  #033[0m| 22/28 [01:16<00:20,  3.47s/it]\u001B[0m\n",
      "\u001B[35mevaluating Epoch:  79%|#033[32m███████▊  #033[0m| 22/28 [01:16<00:20,  3.46s/it]\u001B[0m\n",
      "\u001B[35mevaluating Epoch:  79%|#033[32m███████▊  #033[0m| 22/28 [01:16<00:20,  3.46s/it]\u001B[0m\n",
      "\u001B[35mevaluating Epoch:  79%|#033[32m███████▊  #033[0m| 22/28 [01:16<00:20,  3.46s/it]\u001B[0m\n",
      "\u001B[35mevaluating Epoch:  79%|#033[32m███████▊  #033[0m| 22/28 [01:16<00:20,  3.46s/it]\u001B[0m\n",
      "\u001B[34mevaluating Epoch:  82%|#033[32m████████▏ #033[0m| 23/28 [01:19<00:17,  3.47s/it]\u001B[0m\n",
      "\u001B[34mevaluating Epoch:  82%|#033[32m████████▏ #033[0m| 23/28 [01:19<00:17,  3.47s/it]\u001B[0m\n",
      "\u001B[34mevaluating Epoch:  82%|#033[32m████████▏ #033[0m| 23/28 [01:19<00:17,  3.47s/it]\u001B[0m\n",
      "\u001B[34mevaluating Epoch:  82%|#033[32m████████▏ #033[0m| 23/28 [01:19<00:17,  3.47s/it]\u001B[0m\n",
      "\u001B[35mevaluating Epoch:  82%|#033[32m████████▏ #033[0m| 23/28 [01:19<00:17,  3.46s/it]#015evaluating Epoch:  82%|#033[32m████████▏ #033[0m| 23/28 [01:19<00:17,  3.46s/it]\u001B[0m\n",
      "\u001B[35mevaluating Epoch:  82%|#033[32m████████▏ #033[0m| 23/28 [01:19<00:17,  3.46s/it]\u001B[0m\n",
      "\u001B[35mevaluating Epoch:  82%|#033[32m████████▏ #033[0m| 23/28 [01:19<00:17,  3.46s/it]\u001B[0m\n",
      "\u001B[35mevaluating Epoch:  86%|#033[32m████████▌ #033[0m| 24/28 [01:23<00:13,  3.46s/it]\u001B[0m\n",
      "\u001B[35mevaluating Epoch:  86%|#033[32m████████▌ #033[0m| 24/28 [01:23<00:13,  3.46s/it]\u001B[0m\n",
      "\u001B[35mevaluating Epoch:  86%|#033[32m████████▌ #033[0m| 24/28 [01:23<00:13,  3.46s/it]\u001B[0m\n",
      "\u001B[35mevaluating Epoch:  86%|#033[32m████████▌ #033[0m| 24/28 [01:23<00:13,  3.46s/it]\u001B[0m\n",
      "\u001B[34mevaluating Epoch:  86%|#033[32m████████▌ #033[0m| 24/28 [01:23<00:13,  3.47s/it]#015evaluating Epoch:  86%|#033[32m████████▌ #033[0m| 24/28 [01:23<00:13,  3.47s/it]\u001B[0m\n",
      "\u001B[34mevaluating Epoch:  86%|#033[32m████████▌ #033[0m| 24/28 [01:23<00:13,  3.47s/it]\u001B[0m\n",
      "\u001B[34mevaluating Epoch:  86%|#033[32m████████▌ #033[0m| 24/28 [01:23<00:13,  3.47s/it]\u001B[0m\n",
      "\u001B[34mevaluating Epoch:  89%|#033[32m████████▉ #033[0m| 25/28 [01:26<00:10,  3.47s/it]#015evaluating Epoch:  89%|#033[32m████████▉ #033[0m| 25/28 [01:26<00:10,  3.47s/it]\u001B[0m\n",
      "\u001B[34mevaluating Epoch:  89%|#033[32m████████▉ #033[0m| 25/28 [01:26<00:10,  3.47s/it]\u001B[0m\n",
      "\u001B[34mevaluating Epoch:  89%|#033[32m████████▉ #033[0m| 25/28 [01:26<00:10,  3.47s/it]\u001B[0m\n",
      "\u001B[35mevaluating Epoch:  89%|#033[32m████████▉ #033[0m| 25/28 [01:26<00:10,  3.46s/it]\u001B[0m\n",
      "\u001B[35mevaluating Epoch:  89%|#033[32m████████▉ #033[0m| 25/28 [01:26<00:10,  3.46s/it]\u001B[0m\n",
      "\u001B[35mevaluating Epoch:  89%|#033[32m████████▉ #033[0m| 25/28 [01:26<00:10,  3.46s/it]\u001B[0m\n",
      "\u001B[35mevaluating Epoch:  89%|#033[32m████████▉ #033[0m| 25/28 [01:26<00:10,  3.46s/it]\u001B[0m\n",
      "\u001B[35mevaluating Epoch:  93%|#033[32m█████████▎#033[0m| 26/28 [01:30<00:06,  3.46s/it]\u001B[0m\n",
      "\u001B[35mevaluating Epoch:  93%|#033[32m█████████▎#033[0m| 26/28 [01:30<00:06,  3.46s/it]\u001B[0m\n",
      "\u001B[35mevaluating Epoch:  93%|#033[32m█████████▎#033[0m| 26/28 [01:30<00:06,  3.46s/it]\u001B[0m\n",
      "\u001B[35mevaluating Epoch:  93%|#033[32m█████████▎#033[0m| 26/28 [01:30<00:06,  3.46s/it]\u001B[0m\n",
      "\u001B[34mevaluating Epoch:  93%|#033[32m█████████▎#033[0m| 26/28 [01:30<00:06,  3.47s/it]\u001B[0m\n",
      "\u001B[34mevaluating Epoch:  93%|#033[32m█████████▎#033[0m| 26/28 [01:30<00:06,  3.47s/it]\u001B[0m\n",
      "\u001B[34mevaluating Epoch:  93%|#033[32m█████████▎#033[0m| 26/28 [01:30<00:06,  3.47s/it]#015evaluating Epoch:  93%|#033[32m█████████▎#033[0m| 26/28 [01:30<00:06,  3.47s/it]\u001B[0m\n",
      "\u001B[34mevaluating Epoch:  96%|#033[32m█████████▋#033[0m| 27/28 [01:33<00:03,  3.47s/it]\u001B[0m\n",
      "\u001B[34mevaluating Epoch:  96%|#033[32m█████████▋#033[0m| 27/28 [01:33<00:03,  3.47s/it]\u001B[0m\n",
      "\u001B[34mevaluating Epoch:  96%|#033[32m█████████▋#033[0m| 27/28 [01:33<00:03,  3.47s/it]\u001B[0m\n",
      "\u001B[34mevaluating Epoch:  96%|#033[32m█████████▋#033[0m| 27/28 [01:33<00:03,  3.47s/it]\u001B[0m\n",
      "\u001B[35mevaluating Epoch:  96%|#033[32m█████████▋#033[0m| 27/28 [01:33<00:03,  3.46s/it]\u001B[0m\n",
      "\u001B[35mevaluating Epoch:  96%|#033[32m█████████▋#033[0m| 27/28 [01:33<00:03,  3.46s/it]#015evaluating Epoch:  96%|#033[32m█████████▋#033[0m| 27/28 [01:33<00:03,  3.46s/it]\u001B[0m\n",
      "\u001B[35mevaluating Epoch:  96%|#033[32m█████████▋#033[0m| 27/28 [01:33<00:03,  3.46s/it]\u001B[0m\n",
      "\u001B[34mevaluating Epoch: 100%|#033[32m██████████#033[0m| 28/28 [01:37<00:00,  3.47s/it]\u001B[0m\n",
      "\u001B[34mevaluating Epoch: 100%|#033[32m██████████#033[0m| 28/28 [01:37<00:00,  3.47s/it]\u001B[0m\n",
      "\u001B[34mevaluating Epoch: 100%|#033[32m██████████#033[0m| 28/28 [01:37<00:00,  3.47s/it]#015evaluating Epoch: 100%|#033[32m██████████#033[0m| 28/28 [01:37<00:00,  3.47s/it]\u001B[0m\n",
      "\u001B[34mevaluating Epoch: 100%|#033[32m██████████#033[0m| 28/28 [01:37<00:00,  3.47s/it]\u001B[0m\n",
      "\u001B[34mevaluating Epoch: 100%|#033[32m██████████#033[0m| 28/28 [01:37<00:00,  3.47s/it]\u001B[0m\n",
      "\u001B[34mevaluating Epoch: 100%|#033[32m██████████#033[0m| 28/28 [01:37<00:00,  3.47s/it]\u001B[0m\n",
      "\u001B[34mevaluating Epoch: 100%|#033[32m██████████#033[0m| 28/28 [01:37<00:00,  3.47s/it]\u001B[0m\n",
      "\u001B[35mevaluating Epoch: 100%|#033[32m██████████#033[0m| 28/28 [01:37<00:00,  3.46s/it]\u001B[0m\n",
      "\u001B[35mevaluating Epoch: 100%|#033[32m██████████#033[0m| 28/28 [01:37<00:00,  3.47s/it]\u001B[0m\n",
      "\u001B[35mevaluating Epoch: 100%|#033[32m██████████#033[0m| 28/28 [01:37<00:00,  3.46s/it]\u001B[0m\n",
      "\u001B[35mevaluating Epoch: 100%|#033[32m██████████#033[0m| 28/28 [01:37<00:00,  3.46s/it]\u001B[0m\n",
      "\u001B[35mevaluating Epoch: 100%|#033[32m██████████#033[0m| 28/28 [01:37<00:00,  3.47s/it]\u001B[0m\n",
      "\u001B[35mevaluating Epoch: 100%|#033[32m██████████#033[0m| 28/28 [01:37<00:00,  3.47s/it]\u001B[0m\n",
      "\u001B[35mevaluating Epoch: 100%|#033[32m██████████#033[0m| 28/28 [01:37<00:00,  3.46s/it]\u001B[0m\n",
      "\u001B[35mevaluating Epoch: 100%|#033[32m██████████#033[0m| 28/28 [01:37<00:00,  3.47s/it]\u001B[0m\n",
      "\u001B[35meval_ppl=tensor(3.5533, device='cuda:0') eval_epoch_loss=tensor(1.2679, device='cuda:0')\u001B[0m\n",
      "\u001B[35mwe are about to save the PEFT modules\u001B[0m\n",
      "\u001B[34meval_ppl=tensor(3.5533, device='cuda:0') eval_epoch_loss=tensor(1.2679, device='cuda:0')\u001B[0m\n",
      "\u001B[34mwe are about to save the PEFT modules\u001B[0m\n",
      "\u001B[34mPEFT modules are saved in saved_peft_model directory\u001B[0m\n",
      "\u001B[34mbest eval loss on epoch 2 is 1.267887830734253\u001B[0m\n",
      "\u001B[34mEpoch 3: train_perplexity=3.5882, train_epoch_loss=1.2777, epcoh time 265.86387495400004s\u001B[0m\n",
      "\u001B[34mTraining Epoch3:   0%|#033[34m          #033[0m| 0/27 [00:00<?, ?it/s]\u001B[0m\n",
      "\u001B[34mTraining Epoch3:   0%|#033[34m          #033[0m| 0/27 [00:00<?, ?it/s]\u001B[0m\n",
      "\u001B[34mTraining Epoch3:   0%|#033[34m          #033[0m| 0/27 [00:00<?, ?it/s]\u001B[0m\n",
      "\u001B[34mTraining Epoch3:   0%|#033[34m          #033[0m| 0/27 [00:00<?, ?it/s]\u001B[0m\n",
      "\u001B[35mPEFT modules are saved in saved_peft_model directory\u001B[0m\n",
      "\u001B[35mbest eval loss on epoch 2 is 1.267887830734253\u001B[0m\n",
      "\u001B[35mEpoch 3: train_perplexity=3.5882, train_epoch_loss=1.2777, epcoh time 265.90582477199996s\u001B[0m\n",
      "\u001B[35mTraining Epoch3:   0%|#033[34m          #033[0m| 0/27 [00:00<?, ?it/s]\u001B[0m\n",
      "\u001B[35mTraining Epoch3:   0%|#033[34m          #033[0m| 0/27 [00:00<?, ?it/s]\u001B[0m\n",
      "\u001B[35mTraining Epoch3:   0%|#033[34m          #033[0m| 0/27 [00:00<?, ?it/s]\u001B[0m\n",
      "\u001B[35mTraining Epoch3:   0%|#033[34m          #033[0m| 0/27 [00:00<?, ?it/s]\u001B[0m\n",
      "\u001B[34mstep 0 is completed and loss is 1.3726632595062256\u001B[0m\n",
      "\u001B[34mTraining Epoch3:   4%|#033[34m▎         #033[0m| 1/27 [00:09<04:13,  9.76s/it]\u001B[0m\n",
      "\u001B[34mTraining Epoch3:   4%|#033[34m▎         #033[0m| 1/27 [00:09<04:13,  9.76s/it]#015Training Epoch3:   4%|#033[34m▎         #033[0m| 1/27 [00:09<04:13,  9.75s/it]\u001B[0m\n",
      "\u001B[34mTraining Epoch3:   4%|#033[34m▎         #033[0m| 1/27 [00:09<04:13,  9.76s/it]\u001B[0m\n",
      "\u001B[35mstep 0 is completed and loss is 1.3726632595062256\u001B[0m\n",
      "\u001B[35mTraining Epoch3:   4%|#033[34m▎         #033[0m| 1/27 [00:09<04:13,  9.74s/it]\u001B[0m\n",
      "\u001B[35mTraining Epoch3:   4%|#033[34m▎         #033[0m| 1/27 [00:09<04:13,  9.74s/it]\u001B[0m\n",
      "\u001B[35mTraining Epoch3:   4%|#033[34m▎         #033[0m| 1/27 [00:09<04:13,  9.74s/it]\u001B[0m\n",
      "\u001B[35mTraining Epoch3:   4%|#033[34m▎         #033[0m| 1/27 [00:09<04:13,  9.74s/it]\u001B[0m\n",
      "\u001B[34mstep 1 is completed and loss is 1.0259582996368408\u001B[0m\n",
      "\u001B[34mTraining Epoch3:   7%|#033[34m▋         #033[0m| 2/27 [00:19<04:04,  9.80s/it]\u001B[0m\n",
      "\u001B[34mTraining Epoch3:   7%|#033[34m▋         #033[0m| 2/27 [00:19<04:04,  9.79s/it]\u001B[0m\n",
      "\u001B[34mTraining Epoch3:   7%|#033[34m▋         #033[0m| 2/27 [00:19<04:04,  9.80s/it]\u001B[0m\n",
      "\u001B[34mTraining Epoch3:   7%|#033[34m▋         #033[0m| 2/27 [00:19<04:04,  9.79s/it]\u001B[0m\n",
      "\u001B[35mTraining Epoch3:   7%|#033[34m▋         #033[0m| 2/27 [00:19<04:04,  9.78s/it]\u001B[0m\n",
      "\u001B[35mTraining Epoch3:   7%|#033[34m▋         #033[0m| 2/27 [00:19<04:04,  9.78s/it]#015Training Epoch3:   7%|#033[34m▋         #033[0m| 2/27 [00:19<04:04,  9.78s/it]\u001B[0m\n",
      "\u001B[35mstep 1 is completed and loss is 1.0259582996368408\u001B[0m\n",
      "\u001B[35mTraining Epoch3:   7%|#033[34m▋         #033[0m| 2/27 [00:19<04:04,  9.78s/it]\u001B[0m\n",
      "\u001B[34mTraining Epoch3:  11%|#033[34m█         #033[0m| 3/27 [00:29<03:55,  9.81s/it]\u001B[0m\n",
      "\u001B[34mstep 2 is completed and loss is 1.4376929998397827\u001B[0m\n",
      "\u001B[34mTraining Epoch3:  11%|#033[34m█         #033[0m| 3/27 [00:29<03:55,  9.81s/it]\u001B[0m\n",
      "\u001B[34mTraining Epoch3:  11%|#033[34m█         #033[0m| 3/27 [00:29<03:55,  9.81s/it]\u001B[0m\n",
      "\u001B[34mTraining Epoch3:  11%|#033[34m█         #033[0m| 3/27 [00:29<03:55,  9.81s/it]\u001B[0m\n",
      "\u001B[35mTraining Epoch3:  11%|#033[34m█         #033[0m| 3/27 [00:29<03:54,  9.79s/it]\u001B[0m\n",
      "\u001B[35mstep 2 is completed and loss is 1.4376929998397827\u001B[0m\n",
      "\u001B[35mTraining Epoch3:  11%|#033[34m█         #033[0m| 3/27 [00:29<03:55,  9.79s/it]#015Training Epoch3:  11%|#033[34m█         #033[0m| 3/27 [00:29<03:55,  9.79s/it]\u001B[0m\n",
      "\u001B[35mTraining Epoch3:  11%|#033[34m█         #033[0m| 3/27 [00:29<03:55,  9.79s/it]\u001B[0m\n",
      "\u001B[34mTraining Epoch3:  15%|#033[34m█▍        #033[0m| 4/27 [00:39<03:45,  9.80s/it]#015Training Epoch3:  15%|#033[34m█▍        #033[0m| 4/27 [00:39<03:45,  9.81s/it]\u001B[0m\n",
      "\u001B[34mstep 3 is completed and loss is 1.1647379398345947\u001B[0m\n",
      "\u001B[34mTraining Epoch3:  15%|#033[34m█▍        #033[0m| 4/27 [00:39<03:45,  9.80s/it]\u001B[0m\n",
      "\u001B[34mTraining Epoch3:  15%|#033[34m█▍        #033[0m| 4/27 [00:39<03:45,  9.81s/it]\u001B[0m\n",
      "\u001B[35mTraining Epoch3:  15%|#033[34m█▍        #033[0m| 4/27 [00:39<03:45,  9.79s/it]\u001B[0m\n",
      "\u001B[35mTraining Epoch3:  15%|#033[34m█▍        #033[0m| 4/27 [00:39<03:45,  9.80s/it]\u001B[0m\n",
      "\u001B[35mTraining Epoch3:  15%|#033[34m█▍        #033[0m| 4/27 [00:39<03:45,  9.80s/it]\u001B[0m\n",
      "\u001B[35mstep 3 is completed and loss is 1.1647379398345947\u001B[0m\n",
      "\u001B[35mTraining Epoch3:  15%|#033[34m█▍        #033[0m| 4/27 [00:39<03:45,  9.80s/it]\u001B[0m\n",
      "\u001B[34mTraining Epoch3:  19%|#033[34m█▊        #033[0m| 5/27 [00:49<03:35,  9.81s/it]\u001B[0m\n",
      "\u001B[34mTraining Epoch3:  19%|#033[34m█▊        #033[0m| 5/27 [00:49<03:35,  9.81s/it]\u001B[0m\n",
      "\u001B[34mTraining Epoch3:  19%|#033[34m█▊        #033[0m| 5/27 [00:49<03:35,  9.81s/it]\u001B[0m\n",
      "\u001B[34mstep 4 is completed and loss is 1.144359827041626\u001B[0m\n",
      "\u001B[34mTraining Epoch3:  19%|#033[34m█▊        #033[0m| 5/27 [00:49<03:35,  9.81s/it]\u001B[0m\n",
      "\u001B[35mTraining Epoch3:  19%|#033[34m█▊        #033[0m| 5/27 [00:48<03:35,  9.79s/it]\u001B[0m\n",
      "\u001B[35mstep 4 is completed and loss is 1.144359827041626\u001B[0m\n",
      "\u001B[35mTraining Epoch3:  19%|#033[34m█▊        #033[0m| 5/27 [00:48<03:35,  9.80s/it]\u001B[0m\n",
      "\u001B[35mTraining Epoch3:  19%|#033[34m█▊        #033[0m| 5/27 [00:48<03:35,  9.80s/it]\u001B[0m\n",
      "\u001B[35mTraining Epoch3:  19%|#033[34m█▊        #033[0m| 5/27 [00:48<03:35,  9.80s/it]\u001B[0m\n",
      "\u001B[34mTraining Epoch3:  22%|#033[34m██▏       #033[0m| 6/27 [00:58<03:26,  9.81s/it]\u001B[0m\n",
      "\u001B[34mTraining Epoch3:  22%|#033[34m██▏       #033[0m| 6/27 [00:58<03:26,  9.81s/it]\u001B[0m\n",
      "\u001B[34mstep 5 is completed and loss is 1.2438921928405762\u001B[0m\n",
      "\u001B[34mTraining Epoch3:  22%|#033[34m██▏       #033[0m| 6/27 [00:58<03:26,  9.81s/it]\u001B[0m\n",
      "\u001B[34mTraining Epoch3:  22%|#033[34m██▏       #033[0m| 6/27 [00:58<03:26,  9.81s/it]\u001B[0m\n",
      "\u001B[35mTraining Epoch3:  22%|#033[34m██▏       #033[0m| 6/27 [00:58<03:25,  9.80s/it]\u001B[0m\n",
      "\u001B[35mstep 5 is completed and loss is 1.2438921928405762\u001B[0m\n",
      "\u001B[35mTraining Epoch3:  22%|#033[34m██▏       #033[0m| 6/27 [00:58<03:25,  9.80s/it]\u001B[0m\n",
      "\u001B[35mTraining Epoch3:  22%|#033[34m██▏       #033[0m| 6/27 [00:58<03:25,  9.80s/it]\u001B[0m\n",
      "\u001B[35mTraining Epoch3:  22%|#033[34m██▏       #033[0m| 6/27 [00:58<03:25,  9.80s/it]\u001B[0m\n",
      "\u001B[34mTraining Epoch3:  26%|#033[34m██▌       #033[0m| 7/27 [01:08<03:16,  9.81s/it]\u001B[0m\n",
      "\u001B[34mTraining Epoch3:  26%|#033[34m██▌       #033[0m| 7/27 [01:08<03:16,  9.81s/it]\u001B[0m\n",
      "\u001B[34mstep 6 is completed and loss is 1.491533875465393\u001B[0m\n",
      "\u001B[34mTraining Epoch3:  26%|#033[34m██▌       #033[0m| 7/27 [01:08<03:16,  9.81s/it]\u001B[0m\n",
      "\u001B[34mTraining Epoch3:  26%|#033[34m██▌       #033[0m| 7/27 [01:08<03:16,  9.81s/it]\u001B[0m\n",
      "\u001B[35mTraining Epoch3:  26%|#033[34m██▌       #033[0m| 7/27 [01:08<03:16,  9.80s/it]\u001B[0m\n",
      "\u001B[35mTraining Epoch3:  26%|#033[34m██▌       #033[0m| 7/27 [01:08<03:16,  9.80s/it]\u001B[0m\n",
      "\u001B[35mTraining Epoch3:  26%|#033[34m██▌       #033[0m| 7/27 [01:08<03:16,  9.80s/it]\u001B[0m\n",
      "\u001B[35mstep 6 is completed and loss is 1.491533875465393\u001B[0m\n",
      "\u001B[35mTraining Epoch3:  26%|#033[34m██▌       #033[0m| 7/27 [01:08<03:16,  9.80s/it]\u001B[0m\n",
      "\u001B[34mTraining Epoch3:  30%|#033[34m██▉       #033[0m| 8/27 [01:18<03:06,  9.82s/it]#015Training Epoch3:  30%|#033[34m██▉       #033[0m| 8/27 [01:18<03:06,  9.82s/it]\u001B[0m\n",
      "\u001B[34mstep 7 is completed and loss is 1.3416829109191895\u001B[0m\n",
      "\u001B[34mTraining Epoch3:  30%|#033[34m██▉       #033[0m| 8/27 [01:18<03:06,  9.82s/it]\u001B[0m\n",
      "\u001B[34mTraining Epoch3:  30%|#033[34m██▉       #033[0m| 8/27 [01:18<03:06,  9.82s/it]\u001B[0m\n",
      "\u001B[35mTraining Epoch3:  30%|#033[34m██▉       #033[0m| 8/27 [01:18<03:06,  9.80s/it]\u001B[0m\n",
      "\u001B[35mTraining Epoch3:  30%|#033[34m██▉       #033[0m| 8/27 [01:18<03:06,  9.80s/it]\u001B[0m\n",
      "\u001B[35mTraining Epoch3:  30%|#033[34m██▉       #033[0m| 8/27 [01:18<03:06,  9.80s/it]\u001B[0m\n",
      "\u001B[35mstep 7 is completed and loss is 1.3416829109191895\u001B[0m\n",
      "\u001B[35mTraining Epoch3:  30%|#033[34m██▉       #033[0m| 8/27 [01:18<03:06,  9.80s/it]\u001B[0m\n",
      "\u001B[34mTraining Epoch3:  33%|#033[34m███▎      #033[0m| 9/27 [01:28<02:56,  9.81s/it]\u001B[0m\n",
      "\u001B[34mstep 8 is completed and loss is 1.5328326225280762\u001B[0m\n",
      "\u001B[34mTraining Epoch3:  33%|#033[34m███▎      #033[0m| 9/27 [01:28<02:56,  9.81s/it]\u001B[0m\n",
      "\u001B[34mTraining Epoch3:  33%|#033[34m███▎      #033[0m| 9/27 [01:28<02:56,  9.81s/it]\u001B[0m\n",
      "\u001B[34mTraining Epoch3:  33%|#033[34m███▎      #033[0m| 9/27 [01:28<02:56,  9.81s/it]\u001B[0m\n",
      "\u001B[35mTraining Epoch3:  33%|#033[34m███▎      #033[0m| 9/27 [01:28<02:56,  9.80s/it]\u001B[0m\n",
      "\u001B[35mTraining Epoch3:  33%|#033[34m███▎      #033[0m| 9/27 [01:28<02:56,  9.80s/it]\u001B[0m\n",
      "\u001B[35mstep 8 is completed and loss is 1.5328326225280762\u001B[0m\n",
      "\u001B[35mTraining Epoch3:  33%|#033[34m███▎      #033[0m| 9/27 [01:28<02:56,  9.80s/it]\u001B[0m\n",
      "\u001B[35mTraining Epoch3:  33%|#033[34m███▎      #033[0m| 9/27 [01:28<02:56,  9.80s/it]\u001B[0m\n",
      "\u001B[35mTraining Epoch3:  37%|#033[34m███▋      #033[0m| 10/27 [01:37<02:46,  9.81s/it]\u001B[0m\n",
      "\u001B[35mTraining Epoch3:  37%|#033[34m███▋      #033[0m| 10/27 [01:37<02:46,  9.81s/it]\u001B[0m\n",
      "\u001B[35mstep 9 is completed and loss is 1.1122612953186035\u001B[0m\n",
      "\u001B[35mTraining Epoch3:  37%|#033[34m███▋      #033[0m| 10/27 [01:37<02:46,  9.81s/it]\u001B[0m\n",
      "\u001B[35mTraining Epoch3:  37%|#033[34m███▋      #033[0m| 10/27 [01:37<02:46,  9.81s/it]\u001B[0m\n",
      "\u001B[34mTraining Epoch3:  37%|#033[34m███▋      #033[0m| 10/27 [01:38<02:46,  9.81s/it]\u001B[0m\n",
      "\u001B[34mstep 9 is completed and loss is 1.1122612953186035\u001B[0m\n",
      "\u001B[34mTraining Epoch3:  37%|#033[34m███▋      #033[0m| 10/27 [01:38<02:46,  9.81s/it]\u001B[0m\n",
      "\u001B[34mTraining Epoch3:  37%|#033[34m███▋      #033[0m| 10/27 [01:38<02:46,  9.81s/it]\u001B[0m\n",
      "\u001B[34mTraining Epoch3:  37%|#033[34m███▋      #033[0m| 10/27 [01:38<02:46,  9.81s/it]\u001B[0m\n",
      "\u001B[34mTraining Epoch3:  41%|#033[34m████      #033[0m| 11/27 [01:47<02:37,  9.81s/it]\u001B[0m\n",
      "\u001B[34mTraining Epoch3:  41%|#033[34m████      #033[0m| 11/27 [01:47<02:37,  9.81s/it]\u001B[0m\n",
      "\u001B[34mstep 10 is completed and loss is 1.1654006242752075\u001B[0m\n",
      "\u001B[34mTraining Epoch3:  41%|#033[34m████      #033[0m| 11/27 [01:47<02:37,  9.81s/it]\u001B[0m\n",
      "\u001B[34mTraining Epoch3:  41%|#033[34m████      #033[0m| 11/27 [01:47<02:37,  9.81s/it]\u001B[0m\n",
      "\u001B[35mTraining Epoch3:  41%|#033[34m████      #033[0m| 11/27 [01:47<02:36,  9.81s/it]\u001B[0m\n",
      "\u001B[35mTraining Epoch3:  41%|#033[34m████      #033[0m| 11/27 [01:47<02:36,  9.81s/it]\u001B[0m\n",
      "\u001B[35mstep 10 is completed and loss is 1.1654006242752075\u001B[0m\n",
      "\u001B[35mTraining Epoch3:  41%|#033[34m████      #033[0m| 11/27 [01:47<02:36,  9.81s/it]\u001B[0m\n",
      "\u001B[35mTraining Epoch3:  41%|#033[34m████      #033[0m| 11/27 [01:47<02:36,  9.81s/it]\u001B[0m\n",
      "\u001B[34mstep 11 is completed and loss is 1.1712815761566162\u001B[0m\n",
      "\u001B[34mTraining Epoch3:  44%|#033[34m████▍     #033[0m| 12/27 [01:57<02:27,  9.81s/it]\u001B[0m\n",
      "\u001B[34mTraining Epoch3:  44%|#033[34m████▍     #033[0m| 12/27 [01:57<02:27,  9.81s/it]\u001B[0m\n",
      "\u001B[34mTraining Epoch3:  44%|#033[34m████▍     #033[0m| 12/27 [01:57<02:27,  9.81s/it]\u001B[0m\n",
      "\u001B[34mTraining Epoch3:  44%|#033[34m████▍     #033[0m| 12/27 [01:57<02:27,  9.81s/it]\u001B[0m\n",
      "\u001B[35mTraining Epoch3:  44%|#033[34m████▍     #033[0m| 12/27 [01:57<02:27,  9.81s/it]\u001B[0m\n",
      "\u001B[35mstep 11 is completed and loss is 1.1712815761566162\u001B[0m\n",
      "\u001B[35mTraining Epoch3:  44%|#033[34m████▍     #033[0m| 12/27 [01:57<02:27,  9.81s/it]\u001B[0m\n",
      "\u001B[35mTraining Epoch3:  44%|#033[34m████▍     #033[0m| 12/27 [01:57<02:27,  9.81s/it]\u001B[0m\n",
      "\u001B[35mTraining Epoch3:  44%|#033[34m████▍     #033[0m| 12/27 [01:57<02:27,  9.81s/it]\u001B[0m\n",
      "\u001B[34mTraining Epoch3:  48%|#033[34m████▊     #033[0m| 13/27 [02:07<02:18,  9.87s/it]\u001B[0m\n",
      "\u001B[34mTraining Epoch3:  48%|#033[34m████▊     #033[0m| 13/27 [02:07<02:18,  9.87s/it]\u001B[0m\n",
      "\u001B[34mstep 12 is completed and loss is 1.2721587419509888\u001B[0m\n",
      "\u001B[34mTraining Epoch3:  48%|#033[34m████▊     #033[0m| 13/27 [02:07<02:18,  9.87s/it]\u001B[0m\n",
      "\u001B[34mTraining Epoch3:  48%|#033[34m████▊     #033[0m| 13/27 [02:07<02:18,  9.87s/it]\u001B[0m\n",
      "\u001B[35mTraining Epoch3:  48%|#033[34m████▊     #033[0m| 13/27 [02:07<02:17,  9.84s/it]\u001B[0m\n",
      "\u001B[35mTraining Epoch3:  48%|#033[34m████▊     #033[0m| 13/27 [02:07<02:17,  9.84s/it]\u001B[0m\n",
      "\u001B[35mstep 12 is completed and loss is 1.2721587419509888\u001B[0m\n",
      "\u001B[35mTraining Epoch3:  48%|#033[34m████▊     #033[0m| 13/27 [02:07<02:17,  9.84s/it]\u001B[0m\n",
      "\u001B[35mTraining Epoch3:  48%|#033[34m████▊     #033[0m| 13/27 [02:07<02:17,  9.84s/it]\u001B[0m\n",
      "\u001B[34mTraining Epoch3:  52%|#033[34m█████▏    #033[0m| 14/27 [02:17<02:08,  9.89s/it]\u001B[0m\n",
      "\u001B[34mTraining Epoch3:  52%|#033[34m█████▏    #033[0m| 14/27 [02:17<02:08,  9.89s/it]\u001B[0m\n",
      "\u001B[34mTraining Epoch3:  52%|#033[34m█████▏    #033[0m| 14/27 [02:17<02:08,  9.89s/it]\u001B[0m\n",
      "\u001B[34mstep 13 is completed and loss is 0.9502225518226624\u001B[0m\n",
      "\u001B[34mTraining Epoch3:  52%|#033[34m█████▏    #033[0m| 14/27 [02:17<02:08,  9.89s/it]\u001B[0m\n",
      "\u001B[35mTraining Epoch3:  52%|#033[34m█████▏    #033[0m| 14/27 [02:17<02:08,  9.90s/it]\u001B[0m\n",
      "\u001B[35mTraining Epoch3:  52%|#033[34m█████▏    #033[0m| 14/27 [02:17<02:08,  9.91s/it]\u001B[0m\n",
      "\u001B[35mstep 13 is completed and loss is 0.9502225518226624\u001B[0m\n",
      "\u001B[35mTraining Epoch3:  52%|#033[34m█████▏    #033[0m| 14/27 [02:17<02:08,  9.91s/it]\u001B[0m\n",
      "\u001B[35mTraining Epoch3:  52%|#033[34m█████▏    #033[0m| 14/27 [02:17<02:08,  9.91s/it]\u001B[0m\n",
      "\u001B[34mTraining Epoch3:  56%|#033[34m█████▌    #033[0m| 15/27 [02:27<01:58,  9.87s/it]\u001B[0m\n",
      "\u001B[34mTraining Epoch3:  56%|#033[34m█████▌    #033[0m| 15/27 [02:27<01:58,  9.87s/it]\u001B[0m\n",
      "\u001B[34mstep 14 is completed and loss is 1.3262782096862793\u001B[0m\n",
      "\u001B[34mTraining Epoch3:  56%|#033[34m█████▌    #033[0m| 15/27 [02:27<01:58,  9.87s/it]\u001B[0m\n",
      "\u001B[34mTraining Epoch3:  56%|#033[34m█████▌    #033[0m| 15/27 [02:27<01:58,  9.87s/it]\u001B[0m\n",
      "\u001B[35mstep 14 is completed and loss is 1.3262782096862793\u001B[0m\n",
      "\u001B[35mTraining Epoch3:  56%|#033[34m█████▌    #033[0m| 15/27 [02:27<01:58,  9.88s/it]\u001B[0m\n",
      "\u001B[35mTraining Epoch3:  56%|#033[34m█████▌    #033[0m| 15/27 [02:27<01:58,  9.88s/it]\u001B[0m\n",
      "\u001B[35mTraining Epoch3:  56%|#033[34m█████▌    #033[0m| 15/27 [02:27<01:58,  9.88s/it]\u001B[0m\n",
      "\u001B[35mTraining Epoch3:  56%|#033[34m█████▌    #033[0m| 15/27 [02:27<01:58,  9.88s/it]\u001B[0m\n",
      "\u001B[34mTraining Epoch3:  59%|#033[34m█████▉    #033[0m| 16/27 [02:37<01:48,  9.85s/it]\u001B[0m\n",
      "\u001B[34mTraining Epoch3:  59%|#033[34m█████▉    #033[0m| 16/27 [02:37<01:48,  9.85s/it]\u001B[0m\n",
      "\u001B[34mstep 15 is completed and loss is 1.4295989274978638\u001B[0m\n",
      "\u001B[34mTraining Epoch3:  59%|#033[34m█████▉    #033[0m| 16/27 [02:37<01:48,  9.85s/it]\u001B[0m\n",
      "\u001B[34mTraining Epoch3:  59%|#033[34m█████▉    #033[0m| 16/27 [02:37<01:48,  9.85s/it]\u001B[0m\n",
      "\u001B[35mstep 15 is completed and loss is 1.4295989274978638\u001B[0m\n",
      "\u001B[35mTraining Epoch3:  59%|#033[34m█████▉    #033[0m| 16/27 [02:37<01:48,  9.85s/it]\u001B[0m\n",
      "\u001B[35mTraining Epoch3:  59%|#033[34m█████▉    #033[0m| 16/27 [02:37<01:48,  9.85s/it]\u001B[0m\n",
      "\u001B[35mTraining Epoch3:  59%|#033[34m█████▉    #033[0m| 16/27 [02:37<01:48,  9.85s/it]\u001B[0m\n",
      "\u001B[35mTraining Epoch3:  59%|#033[34m█████▉    #033[0m| 16/27 [02:37<01:48,  9.85s/it]\u001B[0m\n",
      "\u001B[35mTraining Epoch3:  63%|#033[34m██████▎   #033[0m| 17/27 [02:46<01:38,  9.84s/it]#015Training Epoch3:  63%|#033[34m██████▎   #033[0m| 17/27 [02:46<01:38,  9.84s/it]\u001B[0m\n",
      "\u001B[35mTraining Epoch3:  63%|#033[34m██████▎   #033[0m| 17/27 [02:46<01:38,  9.84s/it]\u001B[0m\n",
      "\u001B[35mstep 16 is completed and loss is 1.4446970224380493\u001B[0m\n",
      "\u001B[35mTraining Epoch3:  63%|#033[34m██████▎   #033[0m| 17/27 [02:46<01:38,  9.84s/it]\u001B[0m\n",
      "\u001B[34mTraining Epoch3:  63%|#033[34m██████▎   #033[0m| 17/27 [02:47<01:38,  9.84s/it]\u001B[0m\n",
      "\u001B[34mTraining Epoch3:  63%|#033[34m██████▎   #033[0m| 17/27 [02:47<01:38,  9.84s/it]\u001B[0m\n",
      "\u001B[34mTraining Epoch3:  63%|#033[34m██████▎   #033[0m| 17/27 [02:47<01:38,  9.84s/it]\u001B[0m\n",
      "\u001B[34mstep 16 is completed and loss is 1.4446970224380493\u001B[0m\n",
      "\u001B[34mTraining Epoch3:  63%|#033[34m██████▎   #033[0m| 17/27 [02:47<01:38,  9.84s/it]\u001B[0m\n",
      "\u001B[34mstep 17 is completed and loss is 1.3346798419952393\u001B[0m\n",
      "\u001B[34mTraining Epoch3:  67%|#033[34m██████▋   #033[0m| 18/27 [02:56<01:28,  9.83s/it]\u001B[0m\n",
      "\u001B[34mTraining Epoch3:  67%|#033[34m██████▋   #033[0m| 18/27 [02:56<01:28,  9.83s/it]\u001B[0m\n",
      "\u001B[34mTraining Epoch3:  67%|#033[34m██████▋   #033[0m| 18/27 [02:56<01:28,  9.83s/it]\u001B[0m\n",
      "\u001B[34mTraining Epoch3:  67%|#033[34m██████▋   #033[0m| 18/27 [02:56<01:28,  9.83s/it]\u001B[0m\n",
      "\u001B[35mTraining Epoch3:  67%|#033[34m██████▋   #033[0m| 18/27 [02:56<01:28,  9.82s/it]\u001B[0m\n",
      "\u001B[35mTraining Epoch3:  67%|#033[34m██████▋   #033[0m| 18/27 [02:56<01:28,  9.82s/it]\u001B[0m\n",
      "\u001B[35mstep 17 is completed and loss is 1.3346798419952393\u001B[0m\n",
      "\u001B[35mTraining Epoch3:  67%|#033[34m██████▋   #033[0m| 18/27 [02:56<01:28,  9.83s/it]\u001B[0m\n",
      "\u001B[35mTraining Epoch3:  67%|#033[34m██████▋   #033[0m| 18/27 [02:56<01:28,  9.83s/it]\u001B[0m\n",
      "\u001B[34mTraining Epoch3:  70%|#033[34m███████   #033[0m| 19/27 [03:06<01:18,  9.82s/it]#015Training Epoch3:  70%|#033[34m███████   #033[0m| 19/27 [03:06<01:18,  9.82s/it]\u001B[0m\n",
      "\u001B[34mTraining Epoch3:  70%|#033[34m███████   #033[0m| 19/27 [03:06<01:18,  9.82s/it]\u001B[0m\n",
      "\u001B[34mstep 18 is completed and loss is 1.2227660417556763\u001B[0m\n",
      "\u001B[34mTraining Epoch3:  70%|#033[34m███████   #033[0m| 19/27 [03:06<01:18,  9.82s/it]\u001B[0m\n",
      "\u001B[35mTraining Epoch3:  70%|#033[34m███████   #033[0m| 19/27 [03:06<01:18,  9.82s/it]\u001B[0m\n",
      "\u001B[35mTraining Epoch3:  70%|#033[34m███████   #033[0m| 19/27 [03:06<01:18,  9.82s/it]\u001B[0m\n",
      "\u001B[35mTraining Epoch3:  70%|#033[34m███████   #033[0m| 19/27 [03:06<01:18,  9.82s/it]\u001B[0m\n",
      "\u001B[35mstep 18 is completed and loss is 1.2227660417556763\u001B[0m\n",
      "\u001B[35mTraining Epoch3:  70%|#033[34m███████   #033[0m| 19/27 [03:06<01:18,  9.82s/it]\u001B[0m\n",
      "\u001B[34mstep 19 is completed and loss is 1.1327589750289917\u001B[0m\n",
      "\u001B[34mTraining Epoch3:  74%|#033[34m███████▍  #033[0m| 20/27 [03:16<01:08,  9.82s/it]\u001B[0m\n",
      "\u001B[34mTraining Epoch3:  74%|#033[34m███████▍  #033[0m| 20/27 [03:16<01:08,  9.82s/it]\u001B[0m\n",
      "\u001B[34mTraining Epoch3:  74%|#033[34m███████▍  #033[0m| 20/27 [03:16<01:08,  9.82s/it]\u001B[0m\n",
      "\u001B[34mTraining Epoch3:  74%|#033[34m███████▍  #033[0m| 20/27 [03:16<01:08,  9.82s/it]\u001B[0m\n",
      "\u001B[35mTraining Epoch3:  74%|#033[34m███████▍  #033[0m| 20/27 [03:16<01:08,  9.82s/it]#015Training Epoch3:  74%|#033[34m███████▍  #033[0m| 20/27 [03:16<01:08,  9.82s/it]\u001B[0m\n",
      "\u001B[35mstep 19 is completed and loss is 1.1327589750289917\u001B[0m\n",
      "\u001B[35mTraining Epoch3:  74%|#033[34m███████▍  #033[0m| 20/27 [03:16<01:08,  9.82s/it]\u001B[0m\n",
      "\u001B[35mTraining Epoch3:  74%|#033[34m███████▍  #033[0m| 20/27 [03:16<01:08,  9.82s/it]\u001B[0m\n",
      "\u001B[34mstep 20 is completed and loss is 1.1670435667037964\u001B[0m\n",
      "\u001B[34mTraining Epoch3:  78%|#033[34m███████▊  #033[0m| 21/27 [03:26<00:58,  9.82s/it]\u001B[0m\n",
      "\u001B[34mTraining Epoch3:  78%|#033[34m███████▊  #033[0m| 21/27 [03:26<00:58,  9.82s/it]\u001B[0m\n",
      "\u001B[34mTraining Epoch3:  78%|#033[34m███████▊  #033[0m| 21/27 [03:26<00:58,  9.82s/it]\u001B[0m\n",
      "\u001B[34mTraining Epoch3:  78%|#033[34m███████▊  #033[0m| 21/27 [03:26<00:58,  9.82s/it]\u001B[0m\n",
      "\u001B[35mTraining Epoch3:  78%|#033[34m███████▊  #033[0m| 21/27 [03:26<00:58,  9.82s/it]\u001B[0m\n",
      "\u001B[35mstep 20 is completed and loss is 1.1670435667037964\u001B[0m\n",
      "\u001B[35mTraining Epoch3:  78%|#033[34m███████▊  #033[0m| 21/27 [03:26<00:58,  9.82s/it]\u001B[0m\n",
      "\u001B[35mTraining Epoch3:  78%|#033[34m███████▊  #033[0m| 21/27 [03:26<00:58,  9.82s/it]\u001B[0m\n",
      "\u001B[35mTraining Epoch3:  78%|#033[34m███████▊  #033[0m| 21/27 [03:26<00:58,  9.82s/it]\u001B[0m\n",
      "\u001B[35mTraining Epoch3:  81%|#033[34m████████▏ #033[0m| 22/27 [03:36<00:49,  9.82s/it]\u001B[0m\n",
      "\u001B[35mTraining Epoch3:  81%|#033[34m████████▏ #033[0m| 22/27 [03:36<00:49,  9.82s/it]#015Training Epoch3:  81%|#033[34m████████▏ #033[0m| 22/27 [03:36<00:49,  9.82s/it]\u001B[0m\n",
      "\u001B[35mstep 21 is completed and loss is 1.2128055095672607\u001B[0m\n",
      "\u001B[35mTraining Epoch3:  81%|#033[34m████████▏ #033[0m| 22/27 [03:36<00:49,  9.82s/it]\u001B[0m\n",
      "\u001B[34mstep 21 is completed and loss is 1.2128055095672607\u001B[0m\n",
      "\u001B[34mTraining Epoch3:  81%|#033[34m████████▏ #033[0m| 22/27 [03:36<00:49,  9.81s/it]\u001B[0m\n",
      "\u001B[34mTraining Epoch3:  81%|#033[34m████████▏ #033[0m| 22/27 [03:36<00:49,  9.81s/it]\u001B[0m\n",
      "\u001B[34mTraining Epoch3:  81%|#033[34m████████▏ #033[0m| 22/27 [03:36<00:49,  9.81s/it]\u001B[0m\n",
      "\u001B[34mTraining Epoch3:  81%|#033[34m████████▏ #033[0m| 22/27 [03:36<00:49,  9.82s/it]\u001B[0m\n",
      "\u001B[34mstep 22 is completed and loss is 1.36762273311615\u001B[0m\n",
      "\u001B[34mTraining Epoch3:  85%|#033[34m████████▌ #033[0m| 23/27 [03:45<00:39,  9.82s/it]\u001B[0m\n",
      "\u001B[34mTraining Epoch3:  85%|#033[34m████████▌ #033[0m| 23/27 [03:45<00:39,  9.82s/it]\u001B[0m\n",
      "\u001B[34mTraining Epoch3:  85%|#033[34m████████▌ #033[0m| 23/27 [03:45<00:39,  9.82s/it]\u001B[0m\n",
      "\u001B[34mTraining Epoch3:  85%|#033[34m████████▌ #033[0m| 23/27 [03:45<00:39,  9.82s/it]\u001B[0m\n",
      "\u001B[35mTraining Epoch3:  85%|#033[34m████████▌ #033[0m| 23/27 [03:45<00:39,  9.82s/it]\u001B[0m\n",
      "\u001B[35mTraining Epoch3:  85%|#033[34m████████▌ #033[0m| 23/27 [03:45<00:39,  9.82s/it]\u001B[0m\n",
      "\u001B[35mstep 22 is completed and loss is 1.36762273311615\u001B[0m\n",
      "\u001B[35mTraining Epoch3:  85%|#033[34m████████▌ #033[0m| 23/27 [03:45<00:39,  9.82s/it]\u001B[0m\n",
      "\u001B[35mTraining Epoch3:  85%|#033[34m████████▌ #033[0m| 23/27 [03:45<00:39,  9.82s/it]\u001B[0m\n",
      "\u001B[34mstep 23 is completed and loss is 1.211220383644104\u001B[0m\n",
      "\u001B[34mTraining Epoch3:  89%|#033[34m████████▉ #033[0m| 24/27 [03:55<00:29,  9.82s/it]\u001B[0m\n",
      "\u001B[34mTraining Epoch3:  89%|#033[34m████████▉ #033[0m| 24/27 [03:55<00:29,  9.82s/it]\u001B[0m\n",
      "\u001B[34mTraining Epoch3:  89%|#033[34m████████▉ #033[0m| 24/27 [03:55<00:29,  9.82s/it]\u001B[0m\n",
      "\u001B[34mTraining Epoch3:  89%|#033[34m████████▉ #033[0m| 24/27 [03:55<00:29,  9.82s/it]\u001B[0m\n",
      "\u001B[35mTraining Epoch3:  89%|#033[34m████████▉ #033[0m| 24/27 [03:55<00:29,  9.81s/it]\u001B[0m\n",
      "\u001B[35mTraining Epoch3:  89%|#033[34m████████▉ #033[0m| 24/27 [03:55<00:29,  9.82s/it]\u001B[0m\n",
      "\u001B[35mstep 23 is completed and loss is 1.211220383644104\u001B[0m\n",
      "\u001B[35mTraining Epoch3:  89%|#033[34m████████▉ #033[0m| 24/27 [03:55<00:29,  9.82s/it]\u001B[0m\n",
      "\u001B[35mTraining Epoch3:  89%|#033[34m████████▉ #033[0m| 24/27 [03:55<00:29,  9.82s/it]\u001B[0m\n",
      "\u001B[34mstep 24 is completed and loss is 1.4484727382659912\u001B[0m\n",
      "\u001B[34mTraining Epoch3:  93%|#033[34m█████████▎#033[0m| 25/27 [04:05<00:19,  9.82s/it]\u001B[0m\n",
      "\u001B[34mTraining Epoch3:  93%|#033[34m█████████▎#033[0m| 25/27 [04:05<00:19,  9.82s/it]\u001B[0m\n",
      "\u001B[34mTraining Epoch3:  93%|#033[34m█████████▎#033[0m| 25/27 [04:05<00:19,  9.82s/it]\u001B[0m\n",
      "\u001B[34mTraining Epoch3:  93%|#033[34m█████████▎#033[0m| 25/27 [04:05<00:19,  9.82s/it]\u001B[0m\n",
      "\u001B[35mTraining Epoch3:  93%|#033[34m█████████▎#033[0m| 25/27 [04:05<00:19,  9.84s/it]\u001B[0m\n",
      "\u001B[35mstep 24 is completed and loss is 1.4484727382659912\u001B[0m\n",
      "\u001B[35mTraining Epoch3:  93%|#033[34m█████████▎#033[0m| 25/27 [04:05<00:19,  9.84s/it]\u001B[0m\n",
      "\u001B[35mTraining Epoch3:  93%|#033[34m█████████▎#033[0m| 25/27 [04:05<00:19,  9.84s/it]\u001B[0m\n",
      "\u001B[35mTraining Epoch3:  93%|#033[34m█████████▎#033[0m| 25/27 [04:05<00:19,  9.84s/it]\u001B[0m\n",
      "\u001B[34mstep 25 is completed and loss is 1.0838966369628906\u001B[0m\n",
      "\u001B[34mTraining Epoch3:  96%|#033[34m█████████▋#033[0m| 26/27 [04:15<00:09,  9.86s/it]\u001B[0m\n",
      "\u001B[34mTraining Epoch3:  96%|#033[34m█████████▋#033[0m| 26/27 [04:15<00:09,  9.86s/it]\u001B[0m\n",
      "\u001B[34mTraining Epoch3:  96%|#033[34m█████████▋#033[0m| 26/27 [04:15<00:09,  9.86s/it]\u001B[0m\n",
      "\u001B[34mTraining Epoch3:  96%|#033[34m█████████▋#033[0m| 26/27 [04:15<00:09,  9.86s/it]\u001B[0m\n",
      "\u001B[35mTraining Epoch3:  96%|#033[34m█████████▋#033[0m| 26/27 [04:15<00:09,  9.82s/it]\u001B[0m\n",
      "\u001B[35mstep 25 is completed and loss is 1.0838966369628906\u001B[0m\n",
      "\u001B[35mTraining Epoch3:  96%|#033[34m█████████▋#033[0m| 26/27 [04:15<00:09,  9.82s/it]\u001B[0m\n",
      "\u001B[35mTraining Epoch3:  96%|#033[34m█████████▋#033[0m| 26/27 [04:15<00:09,  9.82s/it]\u001B[0m\n",
      "\u001B[35mTraining Epoch3:  96%|#033[34m█████████▋#033[0m| 26/27 [04:15<00:09,  9.82s/it]\u001B[0m\n",
      "\u001B[34mstep 26 is completed and loss is 1.0308130979537964\u001B[0m\n",
      "\u001B[34mTraining Epoch3: 100%|#033[34m██████████#033[0m| 27/27 [04:25<00:00,  9.88s/it]\u001B[0m\n",
      "\u001B[34mTraining Epoch3: 100%|#033[34m██████████#033[0m| 27/27 [04:25<00:00,  9.83s/it]\u001B[0m\n",
      "\u001B[34mTraining Epoch3: 100%|#033[34m██████████#033[0m| 27/27 [04:25<00:00,  9.88s/it]\u001B[0m\n",
      "\u001B[34mTraining Epoch3: 100%|#033[34m██████████#033[0m| 27/27 [04:25<00:00,  9.83s/it]\u001B[0m\n",
      "\u001B[34mTraining Epoch3: 100%|#033[34m██████████#033[0m| 27/27 [04:25<00:00,  9.88s/it]\u001B[0m\n",
      "\u001B[34mTraining Epoch3: 100%|#033[34m██████████#033[0m| 27/27 [04:25<00:00,  9.83s/it]\u001B[0m\n",
      "\u001B[34mTraining Epoch3: 100%|#033[34m██████████#033[0m| 27/27 [04:25<00:00,  9.88s/it]\u001B[0m\n",
      "\u001B[34mTraining Epoch3: 100%|#033[34m██████████#033[0m| 27/27 [04:25<00:00,  9.83s/it]\u001B[0m\n",
      "\u001B[34mMax CUDA memory allocated was 8 GB\u001B[0m\n",
      "\u001B[34mMax CUDA memory reserved was 9 GB\u001B[0m\n",
      "\u001B[34mPeak active CUDA memory was 8 GB\u001B[0m\n",
      "\u001B[34mCuda Malloc retires : 0\u001B[0m\n",
      "\u001B[34mCPU Total Peak Memory consumed during the train (max): 2 GB\u001B[0m\n",
      "\u001B[34mevaluating Epoch:   0%|#033[32m          #033[0m| 0/28 [00:00<?, ?it/s]#015evaluating Epoch:   0%|#033[32m          #033[0m| 0/28 [00:00<?, ?it/s]\u001B[0m\n",
      "\u001B[34mevaluating Epoch:   0%|#033[32m          #033[0m| 0/28 [00:00<?, ?it/s]\u001B[0m\n",
      "\u001B[34mevaluating Epoch:   0%|#033[32m          #033[0m| 0/28 [00:00<?, ?it/s]\u001B[0m\n",
      "\u001B[35mstep 26 is completed and loss is 1.0308130979537964\u001B[0m\n",
      "\u001B[35mTraining Epoch3: 100%|#033[34m██████████#033[0m| 27/27 [04:25<00:00,  9.88s/it]\u001B[0m\n",
      "\u001B[35mTraining Epoch3: 100%|#033[34m██████████#033[0m| 27/27 [04:25<00:00,  9.83s/it]\u001B[0m\n",
      "\u001B[35mTraining Epoch3: 100%|#033[34m██████████#033[0m| 27/27 [04:25<00:00,  9.88s/it]\u001B[0m\n",
      "\u001B[35mTraining Epoch3: 100%|#033[34m██████████#033[0m| 27/27 [04:25<00:00,  9.83s/it]\u001B[0m\n",
      "\u001B[35mTraining Epoch3: 100%|#033[34m██████████#033[0m| 27/27 [04:25<00:00,  9.88s/it]\u001B[0m\n",
      "\u001B[35mTraining Epoch3: 100%|#033[34m██████████#033[0m| 27/27 [04:25<00:00,  9.88s/it]#015Training Epoch3: 100%|#033[34m██████████#033[0m| 27/27 [04:25<00:00,  9.83s/it]\u001B[0m\n",
      "\u001B[35mTraining Epoch3: 100%|#033[34m██████████#033[0m| 27/27 [04:25<00:00,  9.83s/it]\u001B[0m\n",
      "\u001B[35mMax CUDA memory allocated was 8 GB\u001B[0m\n",
      "\u001B[35mMax CUDA memory reserved was 9 GB\u001B[0m\n",
      "\u001B[35mPeak active CUDA memory was 8 GB\u001B[0m\n",
      "\u001B[35mCuda Malloc retires : 0\u001B[0m\n",
      "\u001B[35mCPU Total Peak Memory consumed during the train (max): 2 GB\u001B[0m\n",
      "\u001B[35mevaluating Epoch:   0%|#033[32m          #033[0m| 0/28 [00:00<?, ?it/s]\u001B[0m\n",
      "\u001B[35mevaluating Epoch:   0%|#033[32m          #033[0m| 0/28 [00:00<?, ?it/s]\u001B[0m\n",
      "\u001B[35mevaluating Epoch:   0%|#033[32m          #033[0m| 0/28 [00:00<?, ?it/s]\u001B[0m\n",
      "\u001B[35mevaluating Epoch:   0%|#033[32m          #033[0m| 0/28 [00:00<?, ?it/s]\u001B[0m\n",
      "\u001B[34mevaluating Epoch:   4%|#033[32m▎         #033[0m| 1/28 [00:03<01:36,  3.58s/it]\u001B[0m\n",
      "\u001B[34mevaluating Epoch:   4%|#033[32m▎         #033[0m| 1/28 [00:03<01:36,  3.58s/it]\u001B[0m\n",
      "\u001B[34mevaluating Epoch:   4%|#033[32m▎         #033[0m| 1/28 [00:03<01:36,  3.58s/it]#015evaluating Epoch:   4%|#033[32m▎         #033[0m| 1/28 [00:03<01:36,  3.58s/it]\u001B[0m\n",
      "\u001B[35mevaluating Epoch:   4%|#033[32m▎         #033[0m| 1/28 [00:03<01:36,  3.59s/it]#015evaluating Epoch:   4%|#033[32m▎         #033[0m| 1/28 [00:03<01:36,  3.58s/it]\u001B[0m\n",
      "\u001B[35mevaluating Epoch:   4%|#033[32m▎         #033[0m| 1/28 [00:03<01:36,  3.58s/it]\u001B[0m\n",
      "\u001B[35mevaluating Epoch:   4%|#033[32m▎         #033[0m| 1/28 [00:03<01:36,  3.58s/it]\u001B[0m\n",
      "\u001B[34mevaluating Epoch:   7%|#033[32m▋         #033[0m| 2/28 [00:07<01:31,  3.51s/it]\u001B[0m\n",
      "\u001B[34mevaluating Epoch:   7%|#033[32m▋         #033[0m| 2/28 [00:07<01:31,  3.51s/it]\u001B[0m\n",
      "\u001B[34mevaluating Epoch:   7%|#033[32m▋         #033[0m| 2/28 [00:07<01:31,  3.51s/it]\u001B[0m\n",
      "\u001B[34mevaluating Epoch:   7%|#033[32m▋         #033[0m| 2/28 [00:07<01:31,  3.51s/it]\u001B[0m\n",
      "\u001B[35mevaluating Epoch:   7%|#033[32m▋         #033[0m| 2/28 [00:07<01:31,  3.51s/it]\u001B[0m\n",
      "\u001B[35mevaluating Epoch:   7%|#033[32m▋         #033[0m| 2/28 [00:07<01:31,  3.51s/it]\u001B[0m\n",
      "\u001B[35mevaluating Epoch:   7%|#033[32m▋         #033[0m| 2/28 [00:07<01:31,  3.51s/it]\u001B[0m\n",
      "\u001B[35mevaluating Epoch:   7%|#033[32m▋         #033[0m| 2/28 [00:07<01:31,  3.51s/it]\u001B[0m\n",
      "\u001B[35mevaluating Epoch:  11%|#033[32m█         #033[0m| 3/28 [00:10<01:27,  3.49s/it]\u001B[0m\n",
      "\u001B[35mevaluating Epoch:  11%|#033[32m█         #033[0m| 3/28 [00:10<01:27,  3.49s/it]\u001B[0m\n",
      "\u001B[35mevaluating Epoch:  11%|#033[32m█         #033[0m| 3/28 [00:10<01:27,  3.49s/it]\u001B[0m\n",
      "\u001B[35mevaluating Epoch:  11%|#033[32m█         #033[0m| 3/28 [00:10<01:27,  3.49s/it]\u001B[0m\n",
      "\u001B[34mevaluating Epoch:  11%|#033[32m█         #033[0m| 3/28 [00:10<01:27,  3.49s/it]\u001B[0m\n",
      "\u001B[34mevaluating Epoch:  11%|#033[32m█         #033[0m| 3/28 [00:10<01:27,  3.49s/it]#015evaluating Epoch:  11%|#033[32m█         #033[0m| 3/28 [00:10<01:27,  3.49s/it]\u001B[0m\n",
      "\u001B[34mevaluating Epoch:  11%|#033[32m█         #033[0m| 3/28 [00:10<01:27,  3.49s/it]\u001B[0m\n",
      "\u001B[34mevaluating Epoch:  14%|#033[32m█▍        #033[0m| 4/28 [00:13<01:23,  3.48s/it]#015evaluating Epoch:  14%|#033[32m█▍        #033[0m| 4/28 [00:13<01:23,  3.48s/it]\u001B[0m\n",
      "\u001B[34mevaluating Epoch:  14%|#033[32m█▍        #033[0m| 4/28 [00:13<01:23,  3.48s/it]\u001B[0m\n",
      "\u001B[34mevaluating Epoch:  14%|#033[32m█▍        #033[0m| 4/28 [00:13<01:23,  3.48s/it]\u001B[0m\n",
      "\u001B[35mevaluating Epoch:  14%|#033[32m█▍        #033[0m| 4/28 [00:13<01:23,  3.48s/it]\u001B[0m\n",
      "\u001B[35mevaluating Epoch:  14%|#033[32m█▍        #033[0m| 4/28 [00:13<01:23,  3.48s/it]\u001B[0m\n",
      "\u001B[35mevaluating Epoch:  14%|#033[32m█▍        #033[0m| 4/28 [00:13<01:23,  3.48s/it]\u001B[0m\n",
      "\u001B[35mevaluating Epoch:  14%|#033[32m█▍        #033[0m| 4/28 [00:13<01:23,  3.48s/it]\u001B[0m\n",
      "\u001B[35mevaluating Epoch:  18%|#033[32m█▊        #033[0m| 5/28 [00:17<01:19,  3.47s/it]\u001B[0m\n",
      "\u001B[35mevaluating Epoch:  18%|#033[32m█▊        #033[0m| 5/28 [00:17<01:19,  3.47s/it]\u001B[0m\n",
      "\u001B[35mevaluating Epoch:  18%|#033[32m█▊        #033[0m| 5/28 [00:17<01:19,  3.47s/it]\u001B[0m\n",
      "\u001B[35mevaluating Epoch:  18%|#033[32m█▊        #033[0m| 5/28 [00:17<01:19,  3.47s/it]\u001B[0m\n",
      "\u001B[34mevaluating Epoch:  18%|#033[32m█▊        #033[0m| 5/28 [00:17<01:19,  3.47s/it]#015evaluating Epoch:  18%|#033[32m█▊        #033[0m| 5/28 [00:17<01:19,  3.47s/it]\u001B[0m\n",
      "\u001B[34mevaluating Epoch:  18%|#033[32m█▊        #033[0m| 5/28 [00:17<01:19,  3.47s/it]#015evaluating Epoch:  18%|#033[32m█▊        #033[0m| 5/28 [00:17<01:19,  3.47s/it]\u001B[0m\n",
      "\u001B[34mevaluating Epoch:  21%|#033[32m██▏       #033[0m| 6/28 [00:20<01:16,  3.47s/it]\u001B[0m\n",
      "\u001B[34mevaluating Epoch:  21%|#033[32m██▏       #033[0m| 6/28 [00:20<01:16,  3.47s/it]\u001B[0m\n",
      "\u001B[34mevaluating Epoch:  21%|#033[32m██▏       #033[0m| 6/28 [00:20<01:16,  3.47s/it]\u001B[0m\n",
      "\u001B[34mevaluating Epoch:  21%|#033[32m██▏       #033[0m| 6/28 [00:20<01:16,  3.47s/it]\u001B[0m\n",
      "\u001B[35mevaluating Epoch:  21%|#033[32m██▏       #033[0m| 6/28 [00:20<01:16,  3.47s/it]#015evaluating Epoch:  21%|#033[32m██▏       #033[0m| 6/28 [00:20<01:16,  3.47s/it]\u001B[0m\n",
      "\u001B[35mevaluating Epoch:  21%|#033[32m██▏       #033[0m| 6/28 [00:20<01:16,  3.47s/it]\u001B[0m\n",
      "\u001B[35mevaluating Epoch:  21%|#033[32m██▏       #033[0m| 6/28 [00:20<01:16,  3.47s/it]\u001B[0m\n",
      "\u001B[35mevaluating Epoch:  25%|#033[32m██▌       #033[0m| 7/28 [00:24<01:12,  3.46s/it]\u001B[0m\n",
      "\u001B[35mevaluating Epoch:  25%|#033[32m██▌       #033[0m| 7/28 [00:24<01:12,  3.46s/it]\u001B[0m\n",
      "\u001B[35mevaluating Epoch:  25%|#033[32m██▌       #033[0m| 7/28 [00:24<01:12,  3.47s/it]\u001B[0m\n",
      "\u001B[35mevaluating Epoch:  25%|#033[32m██▌       #033[0m| 7/28 [00:24<01:12,  3.46s/it]\u001B[0m\n",
      "\u001B[34mevaluating Epoch:  25%|#033[32m██▌       #033[0m| 7/28 [00:24<01:12,  3.47s/it]\u001B[0m\n",
      "\u001B[34mevaluating Epoch:  25%|#033[32m██▌       #033[0m| 7/28 [00:24<01:12,  3.47s/it]\u001B[0m\n",
      "\u001B[34mevaluating Epoch:  25%|#033[32m██▌       #033[0m| 7/28 [00:24<01:12,  3.47s/it]\u001B[0m\n",
      "\u001B[34mevaluating Epoch:  25%|#033[32m██▌       #033[0m| 7/28 [00:24<01:12,  3.47s/it]\u001B[0m\n",
      "\u001B[34mevaluating Epoch:  29%|#033[32m██▊       #033[0m| 8/28 [00:27<01:09,  3.47s/it]\u001B[0m\n",
      "\u001B[34mevaluating Epoch:  29%|#033[32m██▊       #033[0m| 8/28 [00:27<01:09,  3.47s/it]\u001B[0m\n",
      "\u001B[34mevaluating Epoch:  29%|#033[32m██▊       #033[0m| 8/28 [00:27<01:09,  3.47s/it]#015evaluating Epoch:  29%|#033[32m██▊       #033[0m| 8/28 [00:27<01:09,  3.47s/it]\u001B[0m\n",
      "\u001B[35mevaluating Epoch:  29%|#033[32m██▊       #033[0m| 8/28 [00:27<01:09,  3.46s/it]\u001B[0m\n",
      "\u001B[35mevaluating Epoch:  29%|#033[32m██▊       #033[0m| 8/28 [00:27<01:09,  3.46s/it]\u001B[0m\n",
      "\u001B[35mevaluating Epoch:  29%|#033[32m██▊       #033[0m| 8/28 [00:27<01:09,  3.46s/it]\u001B[0m\n",
      "\u001B[35mevaluating Epoch:  29%|#033[32m██▊       #033[0m| 8/28 [00:27<01:09,  3.46s/it]\u001B[0m\n",
      "\u001B[34mevaluating Epoch:  32%|#033[32m███▏      #033[0m| 9/28 [00:31<01:05,  3.47s/it]\u001B[0m\n",
      "\u001B[34mevaluating Epoch:  32%|#033[32m███▏      #033[0m| 9/28 [00:31<01:05,  3.47s/it]#015evaluating Epoch:  32%|#033[32m███▏      #033[0m| 9/28 [00:31<01:05,  3.47s/it]\u001B[0m\n",
      "\u001B[34mevaluating Epoch:  32%|#033[32m███▏      #033[0m| 9/28 [00:31<01:05,  3.47s/it]\u001B[0m\n",
      "\u001B[35mevaluating Epoch:  32%|#033[32m███▏      #033[0m| 9/28 [00:31<01:05,  3.46s/it]\u001B[0m\n",
      "\u001B[35mevaluating Epoch:  32%|#033[32m███▏      #033[0m| 9/28 [00:31<01:05,  3.46s/it]\u001B[0m\n",
      "\u001B[35mevaluating Epoch:  32%|#033[32m███▏      #033[0m| 9/28 [00:31<01:05,  3.46s/it]\u001B[0m\n",
      "\u001B[35mevaluating Epoch:  32%|#033[32m███▏      #033[0m| 9/28 [00:31<01:05,  3.46s/it]\u001B[0m\n",
      "\u001B[34mevaluating Epoch:  36%|#033[32m███▌      #033[0m| 10/28 [00:34<01:02,  3.47s/it]\u001B[0m\n",
      "\u001B[34mevaluating Epoch:  36%|#033[32m███▌      #033[0m| 10/28 [00:34<01:02,  3.47s/it]#015evaluating Epoch:  36%|#033[32m███▌      #033[0m| 10/28 [00:34<01:02,  3.47s/it]\u001B[0m\n",
      "\u001B[34mevaluating Epoch:  36%|#033[32m███▌      #033[0m| 10/28 [00:34<01:02,  3.47s/it]\u001B[0m\n",
      "\u001B[35mevaluating Epoch:  36%|#033[32m███▌      #033[0m| 10/28 [00:34<01:02,  3.46s/it]\u001B[0m\n",
      "\u001B[35mevaluating Epoch:  36%|#033[32m███▌      #033[0m| 10/28 [00:34<01:02,  3.46s/it]#015evaluating Epoch:  36%|#033[32m███▌      #033[0m| 10/28 [00:34<01:02,  3.46s/it]\u001B[0m\n",
      "\u001B[35mevaluating Epoch:  36%|#033[32m███▌      #033[0m| 10/28 [00:34<01:02,  3.46s/it]\u001B[0m\n",
      "\u001B[34mevaluating Epoch:  39%|#033[32m███▉      #033[0m| 11/28 [00:38<00:59,  3.47s/it]#015evaluating Epoch:  39%|#033[32m███▉      #033[0m| 11/28 [00:38<00:59,  3.47s/it]\u001B[0m\n",
      "\u001B[34mevaluating Epoch:  39%|#033[32m███▉      #033[0m| 11/28 [00:38<00:59,  3.47s/it]\u001B[0m\n",
      "\u001B[34mevaluating Epoch:  39%|#033[32m███▉      #033[0m| 11/28 [00:38<00:59,  3.47s/it]\u001B[0m\n",
      "\u001B[35mevaluating Epoch:  39%|#033[32m███▉      #033[0m| 11/28 [00:38<00:58,  3.46s/it]\u001B[0m\n",
      "\u001B[35mevaluating Epoch:  39%|#033[32m███▉      #033[0m| 11/28 [00:38<00:58,  3.46s/it]\u001B[0m\n",
      "\u001B[35mevaluating Epoch:  39%|#033[32m███▉      #033[0m| 11/28 [00:38<00:58,  3.46s/it]\u001B[0m\n",
      "\u001B[35mevaluating Epoch:  39%|#033[32m███▉      #033[0m| 11/28 [00:38<00:58,  3.46s/it]\u001B[0m\n",
      "\u001B[34mevaluating Epoch:  43%|#033[32m████▎     #033[0m| 12/28 [00:41<00:55,  3.47s/it]\u001B[0m\n",
      "\u001B[34mevaluating Epoch:  43%|#033[32m████▎     #033[0m| 12/28 [00:41<00:55,  3.47s/it]\u001B[0m\n",
      "\u001B[34mevaluating Epoch:  43%|#033[32m████▎     #033[0m| 12/28 [00:41<00:55,  3.47s/it]\u001B[0m\n",
      "\u001B[34mevaluating Epoch:  43%|#033[32m████▎     #033[0m| 12/28 [00:41<00:55,  3.47s/it]\u001B[0m\n",
      "\u001B[35mevaluating Epoch:  43%|#033[32m████▎     #033[0m| 12/28 [00:41<00:55,  3.46s/it]\u001B[0m\n",
      "\u001B[35mevaluating Epoch:  43%|#033[32m████▎     #033[0m| 12/28 [00:41<00:55,  3.46s/it]#015evaluating Epoch:  43%|#033[32m████▎     #033[0m| 12/28 [00:41<00:55,  3.46s/it]\u001B[0m\n",
      "\u001B[35mevaluating Epoch:  43%|#033[32m████▎     #033[0m| 12/28 [00:41<00:55,  3.46s/it]\u001B[0m\n",
      "\u001B[34mevaluating Epoch:  46%|#033[32m████▋     #033[0m| 13/28 [00:45<00:51,  3.47s/it]#015evaluating Epoch:  46%|#033[32m████▋     #033[0m| 13/28 [00:45<00:51,  3.47s/it]\u001B[0m\n",
      "\u001B[34mevaluating Epoch:  46%|#033[32m████▋     #033[0m| 13/28 [00:45<00:51,  3.47s/it]#015evaluating Epoch:  46%|#033[32m████▋     #033[0m| 13/28 [00:45<00:51,  3.47s/it]\u001B[0m\n",
      "\u001B[35mevaluating Epoch:  46%|#033[32m████▋     #033[0m| 13/28 [00:45<00:51,  3.46s/it]\u001B[0m\n",
      "\u001B[35mevaluating Epoch:  46%|#033[32m████▋     #033[0m| 13/28 [00:45<00:51,  3.46s/it]#015evaluating Epoch:  46%|#033[32m████▋     #033[0m| 13/28 [00:45<00:51,  3.46s/it]\u001B[0m\n",
      "\u001B[35mevaluating Epoch:  46%|#033[32m████▋     #033[0m| 13/28 [00:45<00:51,  3.46s/it]\u001B[0m\n",
      "\u001B[35mevaluating Epoch:  50%|#033[32m█████     #033[0m| 14/28 [00:48<00:48,  3.46s/it]#015evaluating Epoch:  50%|#033[32m█████     #033[0m| 14/28 [00:48<00:48,  3.46s/it]\u001B[0m\n",
      "\u001B[35mevaluating Epoch:  50%|#033[32m█████     #033[0m| 14/28 [00:48<00:48,  3.46s/it]\u001B[0m\n",
      "\u001B[35mevaluating Epoch:  50%|#033[32m█████     #033[0m| 14/28 [00:48<00:48,  3.46s/it]\u001B[0m\n",
      "\u001B[34mevaluating Epoch:  50%|#033[32m█████     #033[0m| 14/28 [00:48<00:48,  3.47s/it]\u001B[0m\n",
      "\u001B[34mevaluating Epoch:  50%|#033[32m█████     #033[0m| 14/28 [00:48<00:48,  3.47s/it]#015evaluating Epoch:  50%|#033[32m█████     #033[0m| 14/28 [00:48<00:48,  3.47s/it]\u001B[0m\n",
      "\u001B[34mevaluating Epoch:  50%|#033[32m█████     #033[0m| 14/28 [00:48<00:48,  3.47s/it]\u001B[0m\n",
      "\u001B[34mevaluating Epoch:  54%|#033[32m█████▎    #033[0m| 15/28 [00:52<00:45,  3.47s/it]#015evaluating Epoch:  54%|#033[32m█████▎    #033[0m| 15/28 [00:52<00:45,  3.47s/it]\u001B[0m\n",
      "\u001B[34mevaluating Epoch:  54%|#033[32m█████▎    #033[0m| 15/28 [00:52<00:45,  3.47s/it]\u001B[0m\n",
      "\u001B[34mevaluating Epoch:  54%|#033[32m█████▎    #033[0m| 15/28 [00:52<00:45,  3.47s/it]\u001B[0m\n",
      "\u001B[35mevaluating Epoch:  54%|#033[32m█████▎    #033[0m| 15/28 [00:52<00:45,  3.46s/it]#015evaluating Epoch:  54%|#033[32m█████▎    #033[0m| 15/28 [00:52<00:45,  3.46s/it]\u001B[0m\n",
      "\u001B[35mevaluating Epoch:  54%|#033[32m█████▎    #033[0m| 15/28 [00:52<00:45,  3.46s/it]\u001B[0m\n",
      "\u001B[35mevaluating Epoch:  54%|#033[32m█████▎    #033[0m| 15/28 [00:52<00:45,  3.46s/it]\u001B[0m\n",
      "\u001B[35mevaluating Epoch:  57%|#033[32m█████▋    #033[0m| 16/28 [00:55<00:41,  3.47s/it]\u001B[0m\n",
      "\u001B[35mevaluating Epoch:  57%|#033[32m█████▋    #033[0m| 16/28 [00:55<00:41,  3.47s/it]\u001B[0m\n",
      "\u001B[35mevaluating Epoch:  57%|#033[32m█████▋    #033[0m| 16/28 [00:55<00:41,  3.47s/it]\u001B[0m\n",
      "\u001B[35mevaluating Epoch:  57%|#033[32m█████▋    #033[0m| 16/28 [00:55<00:41,  3.47s/it]\u001B[0m\n",
      "\u001B[34mevaluating Epoch:  57%|#033[32m█████▋    #033[0m| 16/28 [00:55<00:41,  3.47s/it]\u001B[0m\n",
      "\u001B[34mevaluating Epoch:  57%|#033[32m█████▋    #033[0m| 16/28 [00:55<00:41,  3.47s/it]#015evaluating Epoch:  57%|#033[32m█████▋    #033[0m| 16/28 [00:55<00:41,  3.47s/it]\u001B[0m\n",
      "\u001B[34mevaluating Epoch:  57%|#033[32m█████▋    #033[0m| 16/28 [00:55<00:41,  3.47s/it]\u001B[0m\n",
      "\u001B[34mevaluating Epoch:  61%|#033[32m██████    #033[0m| 17/28 [00:59<00:38,  3.47s/it]\u001B[0m\n",
      "\u001B[34mevaluating Epoch:  61%|#033[32m██████    #033[0m| 17/28 [00:59<00:38,  3.47s/it]#015evaluating Epoch:  61%|#033[32m██████    #033[0m| 17/28 [00:59<00:38,  3.47s/it]\u001B[0m\n",
      "\u001B[34mevaluating Epoch:  61%|#033[32m██████    #033[0m| 17/28 [00:59<00:38,  3.47s/it]\u001B[0m\n",
      "\u001B[35mevaluating Epoch:  61%|#033[32m██████    #033[0m| 17/28 [00:58<00:38,  3.47s/it]\u001B[0m\n",
      "\u001B[35mevaluating Epoch:  61%|#033[32m██████    #033[0m| 17/28 [00:58<00:38,  3.47s/it]\u001B[0m\n",
      "\u001B[35mevaluating Epoch:  61%|#033[32m██████    #033[0m| 17/28 [00:58<00:38,  3.47s/it]\u001B[0m\n",
      "\u001B[35mevaluating Epoch:  61%|#033[32m██████    #033[0m| 17/28 [00:58<00:38,  3.47s/it]\u001B[0m\n",
      "\u001B[35mevaluating Epoch:  64%|#033[32m██████▍   #033[0m| 18/28 [01:02<00:34,  3.47s/it]\u001B[0m\n",
      "\u001B[35mevaluating Epoch:  64%|#033[32m██████▍   #033[0m| 18/28 [01:02<00:34,  3.47s/it]\u001B[0m\n",
      "\u001B[35mevaluating Epoch:  64%|#033[32m██████▍   #033[0m| 18/28 [01:02<00:34,  3.47s/it]\u001B[0m\n",
      "\u001B[35mevaluating Epoch:  64%|#033[32m██████▍   #033[0m| 18/28 [01:02<00:34,  3.47s/it]\u001B[0m\n",
      "\u001B[34mevaluating Epoch:  64%|#033[32m██████▍   #033[0m| 18/28 [01:02<00:34,  3.46s/it]\u001B[0m\n",
      "\u001B[34mevaluating Epoch:  64%|#033[32m██████▍   #033[0m| 18/28 [01:02<00:34,  3.46s/it]\u001B[0m\n",
      "\u001B[34mevaluating Epoch:  64%|#033[32m██████▍   #033[0m| 18/28 [01:02<00:34,  3.46s/it]\u001B[0m\n",
      "\u001B[34mevaluating Epoch:  64%|#033[32m██████▍   #033[0m| 18/28 [01:02<00:34,  3.46s/it]\u001B[0m\n",
      "\u001B[34mevaluating Epoch:  68%|#033[32m██████▊   #033[0m| 19/28 [01:05<00:31,  3.47s/it]\u001B[0m\n",
      "\u001B[34mevaluating Epoch:  68%|#033[32m██████▊   #033[0m| 19/28 [01:05<00:31,  3.47s/it]#015evaluating Epoch:  68%|#033[32m██████▊   #033[0m| 19/28 [01:05<00:31,  3.47s/it]\u001B[0m\n",
      "\u001B[34mevaluating Epoch:  68%|#033[32m██████▊   #033[0m| 19/28 [01:05<00:31,  3.47s/it]\u001B[0m\n",
      "\u001B[35mevaluating Epoch:  68%|#033[32m██████▊   #033[0m| 19/28 [01:05<00:31,  3.47s/it]\u001B[0m\n",
      "\u001B[35mevaluating Epoch:  68%|#033[32m██████▊   #033[0m| 19/28 [01:05<00:31,  3.47s/it]\u001B[0m\n",
      "\u001B[35mevaluating Epoch:  68%|#033[32m██████▊   #033[0m| 19/28 [01:05<00:31,  3.47s/it]\u001B[0m\n",
      "\u001B[35mevaluating Epoch:  68%|#033[32m██████▊   #033[0m| 19/28 [01:05<00:31,  3.47s/it]\u001B[0m\n",
      "\u001B[35mevaluating Epoch:  71%|#033[32m███████▏  #033[0m| 20/28 [01:09<00:27,  3.46s/it]\u001B[0m\n",
      "\u001B[35mevaluating Epoch:  71%|#033[32m███████▏  #033[0m| 20/28 [01:09<00:27,  3.46s/it]\u001B[0m\n",
      "\u001B[35mevaluating Epoch:  71%|#033[32m███████▏  #033[0m| 20/28 [01:09<00:27,  3.46s/it]\u001B[0m\n",
      "\u001B[35mevaluating Epoch:  71%|#033[32m███████▏  #033[0m| 20/28 [01:09<00:27,  3.46s/it]\u001B[0m\n",
      "\u001B[34mevaluating Epoch:  71%|#033[32m███████▏  #033[0m| 20/28 [01:09<00:27,  3.47s/it]\u001B[0m\n",
      "\u001B[34mevaluating Epoch:  71%|#033[32m███████▏  #033[0m| 20/28 [01:09<00:27,  3.47s/it]\u001B[0m\n",
      "\u001B[34mevaluating Epoch:  71%|#033[32m███████▏  #033[0m| 20/28 [01:09<00:27,  3.47s/it]\u001B[0m\n",
      "\u001B[34mevaluating Epoch:  71%|#033[32m███████▏  #033[0m| 20/28 [01:09<00:27,  3.47s/it]\u001B[0m\n",
      "\u001B[34mevaluating Epoch:  75%|#033[32m███████▌  #033[0m| 21/28 [01:12<00:24,  3.47s/it]#015evaluating Epoch:  75%|#033[32m███████▌  #033[0m| 21/28 [01:12<00:24,  3.47s/it]\u001B[0m\n",
      "\u001B[34mevaluating Epoch:  75%|#033[32m███████▌  #033[0m| 21/28 [01:12<00:24,  3.47s/it]\u001B[0m\n",
      "\u001B[34mevaluating Epoch:  75%|#033[32m███████▌  #033[0m| 21/28 [01:12<00:24,  3.47s/it]\u001B[0m\n",
      "\u001B[35mevaluating Epoch:  75%|#033[32m███████▌  #033[0m| 21/28 [01:12<00:24,  3.46s/it]\u001B[0m\n",
      "\u001B[35mevaluating Epoch:  75%|#033[32m███████▌  #033[0m| 21/28 [01:12<00:24,  3.46s/it]\u001B[0m\n",
      "\u001B[35mevaluating Epoch:  75%|#033[32m███████▌  #033[0m| 21/28 [01:12<00:24,  3.46s/it]\u001B[0m\n",
      "\u001B[35mevaluating Epoch:  75%|#033[32m███████▌  #033[0m| 21/28 [01:12<00:24,  3.46s/it]\u001B[0m\n",
      "\u001B[35mevaluating Epoch:  79%|#033[32m███████▊  #033[0m| 22/28 [01:16<00:20,  3.47s/it]\u001B[0m\n",
      "\u001B[35mevaluating Epoch:  79%|#033[32m███████▊  #033[0m| 22/28 [01:16<00:20,  3.47s/it]\u001B[0m\n",
      "\u001B[35mevaluating Epoch:  79%|#033[32m███████▊  #033[0m| 22/28 [01:16<00:20,  3.47s/it]\u001B[0m\n",
      "\u001B[35mevaluating Epoch:  79%|#033[32m███████▊  #033[0m| 22/28 [01:16<00:20,  3.47s/it]\u001B[0m\n",
      "\u001B[34mevaluating Epoch:  79%|#033[32m███████▊  #033[0m| 22/28 [01:16<00:20,  3.47s/it]#015evaluating Epoch:  79%|#033[32m███████▊  #033[0m| 22/28 [01:16<00:20,  3.47s/it]\u001B[0m\n",
      "\u001B[34mevaluating Epoch:  79%|#033[32m███████▊  #033[0m| 22/28 [01:16<00:20,  3.47s/it]\u001B[0m\n",
      "\u001B[34mevaluating Epoch:  79%|#033[32m███████▊  #033[0m| 22/28 [01:16<00:20,  3.47s/it]\u001B[0m\n",
      "\u001B[34mevaluating Epoch:  82%|#033[32m████████▏ #033[0m| 23/28 [01:19<00:17,  3.47s/it]\u001B[0m\n",
      "\u001B[34mevaluating Epoch:  82%|#033[32m████████▏ #033[0m| 23/28 [01:19<00:17,  3.47s/it]\u001B[0m\n",
      "\u001B[34mevaluating Epoch:  82%|#033[32m████████▏ #033[0m| 23/28 [01:19<00:17,  3.47s/it]\u001B[0m\n",
      "\u001B[34mevaluating Epoch:  82%|#033[32m████████▏ #033[0m| 23/28 [01:19<00:17,  3.47s/it]\u001B[0m\n",
      "\u001B[35mevaluating Epoch:  82%|#033[32m████████▏ #033[0m| 23/28 [01:19<00:17,  3.46s/it]#015evaluating Epoch:  82%|#033[32m████████▏ #033[0m| 23/28 [01:19<00:17,  3.46s/it]#015evaluating Epoch:  82%|#033[32m████████▏ #033[0m| 23/28 [01:19<00:17,  3.46s/it]\u001B[0m\n",
      "\u001B[35mevaluating Epoch:  82%|#033[32m████████▏ #033[0m| 23/28 [01:19<00:17,  3.46s/it]\u001B[0m\n",
      "\u001B[34mevaluating Epoch:  86%|#033[32m████████▌ #033[0m| 24/28 [01:23<00:13,  3.46s/it]\u001B[0m\n",
      "\u001B[34mevaluating Epoch:  86%|#033[32m████████▌ #033[0m| 24/28 [01:23<00:13,  3.46s/it]\u001B[0m\n",
      "\u001B[34mevaluating Epoch:  86%|#033[32m████████▌ #033[0m| 24/28 [01:23<00:13,  3.46s/it]\u001B[0m\n",
      "\u001B[34mevaluating Epoch:  86%|#033[32m████████▌ #033[0m| 24/28 [01:23<00:13,  3.46s/it]\u001B[0m\n",
      "\u001B[35mevaluating Epoch:  86%|#033[32m████████▌ #033[0m| 24/28 [01:23<00:13,  3.47s/it]#015evaluating Epoch:  86%|#033[32m████████▌ #033[0m| 24/28 [01:23<00:13,  3.47s/it]\u001B[0m\n",
      "\u001B[35mevaluating Epoch:  86%|#033[32m████████▌ #033[0m| 24/28 [01:23<00:13,  3.47s/it]\u001B[0m\n",
      "\u001B[35mevaluating Epoch:  86%|#033[32m████████▌ #033[0m| 24/28 [01:23<00:13,  3.47s/it]\u001B[0m\n",
      "\u001B[34mevaluating Epoch:  89%|#033[32m████████▉ #033[0m| 25/28 [01:26<00:10,  3.46s/it]#015evaluating Epoch:  89%|#033[32m████████▉ #033[0m| 25/28 [01:26<00:10,  3.46s/it]\u001B[0m\n",
      "\u001B[34mevaluating Epoch:  89%|#033[32m████████▉ #033[0m| 25/28 [01:26<00:10,  3.46s/it]\u001B[0m\n",
      "\u001B[34mevaluating Epoch:  89%|#033[32m████████▉ #033[0m| 25/28 [01:26<00:10,  3.46s/it]\u001B[0m\n",
      "\u001B[35mevaluating Epoch:  89%|#033[32m████████▉ #033[0m| 25/28 [01:26<00:10,  3.46s/it]#015evaluating Epoch:  89%|#033[32m████████▉ #033[0m| 25/28 [01:26<00:10,  3.46s/it]\u001B[0m\n",
      "\u001B[35mevaluating Epoch:  89%|#033[32m████████▉ #033[0m| 25/28 [01:26<00:10,  3.46s/it]\u001B[0m\n",
      "\u001B[35mevaluating Epoch:  89%|#033[32m████████▉ #033[0m| 25/28 [01:26<00:10,  3.46s/it]\u001B[0m\n",
      "\u001B[34mevaluating Epoch:  93%|#033[32m█████████▎#033[0m| 26/28 [01:30<00:06,  3.46s/it]\u001B[0m\n",
      "\u001B[34mevaluating Epoch:  93%|#033[32m█████████▎#033[0m| 26/28 [01:30<00:06,  3.46s/it]\u001B[0m\n",
      "\u001B[34mevaluating Epoch:  93%|#033[32m█████████▎#033[0m| 26/28 [01:30<00:06,  3.46s/it]\u001B[0m\n",
      "\u001B[34mevaluating Epoch:  93%|#033[32m█████████▎#033[0m| 26/28 [01:30<00:06,  3.46s/it]\u001B[0m\n",
      "\u001B[35mevaluating Epoch:  93%|#033[32m█████████▎#033[0m| 26/28 [01:30<00:06,  3.46s/it]\u001B[0m\n",
      "\u001B[35mevaluating Epoch:  93%|#033[32m█████████▎#033[0m| 26/28 [01:30<00:06,  3.46s/it]\u001B[0m\n",
      "\u001B[35mevaluating Epoch:  93%|#033[32m█████████▎#033[0m| 26/28 [01:30<00:06,  3.46s/it]\u001B[0m\n",
      "\u001B[35mevaluating Epoch:  93%|#033[32m█████████▎#033[0m| 26/28 [01:30<00:06,  3.46s/it]\u001B[0m\n",
      "\u001B[34mevaluating Epoch:  96%|#033[32m█████████▋#033[0m| 27/28 [01:33<00:03,  3.46s/it]\u001B[0m\n",
      "\u001B[34mevaluating Epoch:  96%|#033[32m█████████▋#033[0m| 27/28 [01:33<00:03,  3.46s/it]\u001B[0m\n",
      "\u001B[34mevaluating Epoch:  96%|#033[32m█████████▋#033[0m| 27/28 [01:33<00:03,  3.46s/it]\u001B[0m\n",
      "\u001B[34mevaluating Epoch:  96%|#033[32m█████████▋#033[0m| 27/28 [01:33<00:03,  3.46s/it]\u001B[0m\n",
      "\u001B[35mevaluating Epoch:  96%|#033[32m█████████▋#033[0m| 27/28 [01:33<00:03,  3.46s/it]#015evaluating Epoch:  96%|#033[32m█████████▋#033[0m| 27/28 [01:33<00:03,  3.46s/it]\u001B[0m\n",
      "\u001B[35mevaluating Epoch:  96%|#033[32m█████████▋#033[0m| 27/28 [01:33<00:03,  3.46s/it]\u001B[0m\n",
      "\u001B[35mevaluating Epoch:  96%|#033[32m█████████▋#033[0m| 27/28 [01:33<00:03,  3.46s/it]\u001B[0m\n",
      "\u001B[34mevaluating Epoch: 100%|#033[32m██████████#033[0m| 28/28 [01:37<00:00,  3.46s/it]\u001B[0m\n",
      "\u001B[34mevaluating Epoch: 100%|#033[32m██████████#033[0m| 28/28 [01:37<00:00,  3.47s/it]\u001B[0m\n",
      "\u001B[34mevaluating Epoch: 100%|#033[32m██████████#033[0m| 28/28 [01:37<00:00,  3.46s/it]\u001B[0m\n",
      "\u001B[34mevaluating Epoch: 100%|#033[32m██████████#033[0m| 28/28 [01:37<00:00,  3.46s/it]\u001B[0m\n",
      "\u001B[34mevaluating Epoch: 100%|#033[32m██████████#033[0m| 28/28 [01:37<00:00,  3.46s/it]\u001B[0m\n",
      "\u001B[34mevaluating Epoch: 100%|#033[32m██████████#033[0m| 28/28 [01:37<00:00,  3.47s/it]\u001B[0m\n",
      "\u001B[34m#015evaluating Epoch: 100%|#033[32m██████████#033[0m| 28/28 [01:37<00:00,  3.47s/it]\u001B[0m\n",
      "\u001B[34mevaluating Epoch: 100%|#033[32m██████████#033[0m| 28/28 [01:37<00:00,  3.47s/it]\u001B[0m\n",
      "\u001B[34meval_ppl=tensor(3.5386, device='cuda:0') eval_epoch_loss=tensor(1.2637, device='cuda:0')\u001B[0m\n",
      "\u001B[34mwe are about to save the PEFT modules\u001B[0m\n",
      "\u001B[35mevaluating Epoch: 100%|#033[32m██████████#033[0m| 28/28 [01:37<00:00,  3.46s/it]\u001B[0m\n",
      "\u001B[35mevaluating Epoch: 100%|#033[32m██████████#033[0m| 28/28 [01:37<00:00,  3.46s/it]\u001B[0m\n",
      "\u001B[35mevaluating Epoch: 100%|#033[32m██████████#033[0m| 28/28 [01:37<00:00,  3.47s/it]\u001B[0m\n",
      "\u001B[35m#015evaluating Epoch: 100%|#033[32m██████████#033[0m| 28/28 [01:37<00:00,  3.47s/it]\u001B[0m\n",
      "\u001B[35mevaluating Epoch: 100%|#033[32m██████████#033[0m| 28/28 [01:37<00:00,  3.46s/it]\u001B[0m\n",
      "\u001B[35mevaluating Epoch: 100%|#033[32m██████████#033[0m| 28/28 [01:37<00:00,  3.47s/it]\u001B[0m\n",
      "\u001B[35mevaluating Epoch: 100%|#033[32m██████████#033[0m| 28/28 [01:37<00:00,  3.46s/it]\u001B[0m\n",
      "\u001B[35mevaluating Epoch: 100%|#033[32m██████████#033[0m| 28/28 [01:37<00:00,  3.47s/it]\u001B[0m\n",
      "\u001B[35meval_ppl=tensor(3.5386, device='cuda:0') eval_epoch_loss=tensor(1.2637, device='cuda:0')\u001B[0m\n",
      "\u001B[35mwe are about to save the PEFT modules\u001B[0m\n",
      "\u001B[34mPEFT modules are saved in saved_peft_model directory\u001B[0m\n",
      "\u001B[34mbest eval loss on epoch 3 is 1.2637242078781128\u001B[0m\n",
      "\u001B[34mEpoch 4: train_perplexity=3.5289, train_epoch_loss=1.2610, epcoh time 266.01821038s\u001B[0m\n",
      "\u001B[34mTraining Epoch4:   0%|#033[34m          #033[0m| 0/27 [00:00<?, ?it/s]\u001B[0m\n",
      "\u001B[34mTraining Epoch4:   0%|#033[34m          #033[0m| 0/27 [00:00<?, ?it/s]\u001B[0m\n",
      "\u001B[34mTraining Epoch4:   0%|#033[34m          #033[0m| 0/27 [00:00<?, ?it/s]\u001B[0m\n",
      "\u001B[34mTraining Epoch4:   0%|#033[34m          #033[0m| 0/27 [00:00<?, ?it/s]\u001B[0m\n",
      "\u001B[35mPEFT modules are saved in saved_peft_model directory\u001B[0m\n",
      "\u001B[35mbest eval loss on epoch 3 is 1.2637242078781128\u001B[0m\n",
      "\u001B[35mEpoch 4: train_perplexity=3.5289, train_epoch_loss=1.2610, epcoh time 265.8717684090002s\u001B[0m\n",
      "\u001B[35mTraining Epoch4:   0%|#033[34m          #033[0m| 0/27 [00:00<?, ?it/s]\u001B[0m\n",
      "\u001B[35mTraining Epoch4:   0%|#033[34m          #033[0m| 0/27 [00:00<?, ?it/s]\u001B[0m\n",
      "\u001B[35mTraining Epoch4:   0%|#033[34m          #033[0m| 0/27 [00:00<?, ?it/s]\u001B[0m\n",
      "\u001B[35mTraining Epoch4:   0%|#033[34m          #033[0m| 0/27 [00:00<?, ?it/s]\u001B[0m\n",
      "\u001B[34mTraining Epoch4:   4%|#033[34m▎         #033[0m| 1/27 [00:09<04:13,  9.76s/it]\u001B[0m\n",
      "\u001B[34mstep 0 is completed and loss is 1.3601009845733643\u001B[0m\n",
      "\u001B[34mTraining Epoch4:   4%|#033[34m▎         #033[0m| 1/27 [00:09<04:13,  9.76s/it]\u001B[0m\n",
      "\u001B[34mTraining Epoch4:   4%|#033[34m▎         #033[0m| 1/27 [00:09<04:13,  9.76s/it]\u001B[0m\n",
      "\u001B[34mTraining Epoch4:   4%|#033[34m▎         #033[0m| 1/27 [00:09<04:13,  9.76s/it]\u001B[0m\n",
      "\u001B[35mTraining Epoch4:   4%|#033[34m▎         #033[0m| 1/27 [00:09<04:13,  9.75s/it]\u001B[0m\n",
      "\u001B[35mTraining Epoch4:   4%|#033[34m▎         #033[0m| 1/27 [00:09<04:13,  9.75s/it]\u001B[0m\n",
      "\u001B[35mTraining Epoch4:   4%|#033[34m▎         #033[0m| 1/27 [00:09<04:13,  9.74s/it]\u001B[0m\n",
      "\u001B[35mstep 0 is completed and loss is 1.3601009845733643\u001B[0m\n",
      "\u001B[35mTraining Epoch4:   4%|#033[34m▎         #033[0m| 1/27 [00:09<04:13,  9.75s/it]\u001B[0m\n",
      "\u001B[34mTraining Epoch4:   7%|#033[34m▋         #033[0m| 2/27 [00:19<04:04,  9.80s/it]\u001B[0m\n",
      "\u001B[34mstep 1 is completed and loss is 1.0070364475250244\u001B[0m\n",
      "\u001B[34mTraining Epoch4:   7%|#033[34m▋         #033[0m| 2/27 [00:19<04:04,  9.80s/it]\u001B[0m\n",
      "\u001B[34mTraining Epoch4:   7%|#033[34m▋         #033[0m| 2/27 [00:19<04:04,  9.80s/it]\u001B[0m\n",
      "\u001B[34mTraining Epoch4:   7%|#033[34m▋         #033[0m| 2/27 [00:19<04:04,  9.80s/it]\u001B[0m\n",
      "\u001B[35mstep 1 is completed and loss is 1.0070364475250244\u001B[0m\n",
      "\u001B[35mTraining Epoch4:   7%|#033[34m▋         #033[0m| 2/27 [00:19<04:04,  9.78s/it]\u001B[0m\n",
      "\u001B[35mTraining Epoch4:   7%|#033[34m▋         #033[0m| 2/27 [00:19<04:04,  9.78s/it]\u001B[0m\n",
      "\u001B[35mTraining Epoch4:   7%|#033[34m▋         #033[0m| 2/27 [00:19<04:04,  9.78s/it]\u001B[0m\n",
      "\u001B[35mTraining Epoch4:   7%|#033[34m▋         #033[0m| 2/27 [00:19<04:04,  9.78s/it]\u001B[0m\n",
      "\u001B[35mTraining Epoch4:  11%|#033[34m█         #033[0m| 3/27 [00:29<03:54,  9.79s/it]\u001B[0m\n",
      "\u001B[35mTraining Epoch4:  11%|#033[34m█         #033[0m| 3/27 [00:29<03:54,  9.79s/it]\u001B[0m\n",
      "\u001B[35mstep 2 is completed and loss is 1.4239414930343628\u001B[0m\n",
      "\u001B[35mTraining Epoch4:  11%|#033[34m█         #033[0m| 3/27 [00:29<03:54,  9.79s/it]\u001B[0m\n",
      "\u001B[35mTraining Epoch4:  11%|#033[34m█         #033[0m| 3/27 [00:29<03:54,  9.79s/it]\u001B[0m\n",
      "\u001B[34mTraining Epoch4:  11%|#033[34m█         #033[0m| 3/27 [00:29<03:55,  9.80s/it]\u001B[0m\n",
      "\u001B[34mstep 2 is completed and loss is 1.4239414930343628\u001B[0m\n",
      "\u001B[34mTraining Epoch4:  11%|#033[34m█         #033[0m| 3/27 [00:29<03:55,  9.80s/it]\u001B[0m\n",
      "\u001B[34mTraining Epoch4:  11%|#033[34m█         #033[0m| 3/27 [00:29<03:55,  9.80s/it]\u001B[0m\n",
      "\u001B[34mTraining Epoch4:  11%|#033[34m█         #033[0m| 3/27 [00:29<03:55,  9.80s/it]\u001B[0m\n",
      "\u001B[34mTraining Epoch4:  15%|#033[34m█▍        #033[0m| 4/27 [00:39<03:45,  9.81s/it]\u001B[0m\n",
      "\u001B[34mTraining Epoch4:  15%|#033[34m█▍        #033[0m| 4/27 [00:39<03:45,  9.81s/it]\u001B[0m\n",
      "\u001B[34mstep 3 is completed and loss is 1.1535241603851318\u001B[0m\n",
      "\u001B[34mTraining Epoch4:  15%|#033[34m█▍        #033[0m| 4/27 [00:39<03:45,  9.81s/it]\u001B[0m\n",
      "\u001B[34mTraining Epoch4:  15%|#033[34m█▍        #033[0m| 4/27 [00:39<03:45,  9.81s/it]\u001B[0m\n",
      "\u001B[35mTraining Epoch4:  15%|#033[34m█▍        #033[0m| 4/27 [00:39<03:45,  9.80s/it]\u001B[0m\n",
      "\u001B[35mTraining Epoch4:  15%|#033[34m█▍        #033[0m| 4/27 [00:39<03:45,  9.80s/it]\u001B[0m\n",
      "\u001B[35mTraining Epoch4:  15%|#033[34m█▍        #033[0m| 4/27 [00:39<03:45,  9.80s/it]\u001B[0m\n",
      "\u001B[35mstep 3 is completed and loss is 1.1535241603851318\u001B[0m\n",
      "\u001B[35mTraining Epoch4:  15%|#033[34m█▍        #033[0m| 4/27 [00:39<03:45,  9.80s/it]\u001B[0m\n",
      "\u001B[34mstep 4 is completed and loss is 1.1296963691711426\u001B[0m\n",
      "\u001B[34mTraining Epoch4:  19%|#033[34m█▊        #033[0m| 5/27 [00:49<03:35,  9.81s/it]\u001B[0m\n",
      "\u001B[34mTraining Epoch4:  19%|#033[34m█▊        #033[0m| 5/27 [00:49<03:35,  9.81s/it]\u001B[0m\n",
      "\u001B[34mTraining Epoch4:  19%|#033[34m█▊        #033[0m| 5/27 [00:49<03:35,  9.81s/it]\u001B[0m\n",
      "\u001B[34mTraining Epoch4:  19%|#033[34m█▊        #033[0m| 5/27 [00:49<03:35,  9.81s/it]\u001B[0m\n",
      "\u001B[35mTraining Epoch4:  19%|#033[34m█▊        #033[0m| 5/27 [00:48<03:35,  9.80s/it]#015Training Epoch4:  19%|#033[34m█▊        #033[0m| 5/27 [00:48<03:35,  9.80s/it]\u001B[0m\n",
      "\u001B[35mTraining Epoch4:  19%|#033[34m█▊        #033[0m| 5/27 [00:48<03:35,  9.80s/it]\u001B[0m\n",
      "\u001B[35mstep 4 is completed and loss is 1.1296963691711426\u001B[0m\n",
      "\u001B[35mTraining Epoch4:  19%|#033[34m█▊        #033[0m| 5/27 [00:48<03:35,  9.80s/it]\u001B[0m\n",
      "\u001B[34mTraining Epoch4:  22%|#033[34m██▏       #033[0m| 6/27 [00:58<03:25,  9.81s/it]\u001B[0m\n",
      "\u001B[34mTraining Epoch4:  22%|#033[34m██▏       #033[0m| 6/27 [00:58<03:25,  9.81s/it]\u001B[0m\n",
      "\u001B[34mstep 5 is completed and loss is 1.2311228513717651\u001B[0m\n",
      "\u001B[34mTraining Epoch4:  22%|#033[34m██▏       #033[0m| 6/27 [00:58<03:25,  9.81s/it]\u001B[0m\n",
      "\u001B[34mTraining Epoch4:  22%|#033[34m██▏       #033[0m| 6/27 [00:58<03:25,  9.81s/it]\u001B[0m\n",
      "\u001B[35mTraining Epoch4:  22%|#033[34m██▏       #033[0m| 6/27 [00:58<03:25,  9.81s/it]\u001B[0m\n",
      "\u001B[35mstep 5 is completed and loss is 1.2311228513717651\u001B[0m\n",
      "\u001B[35mTraining Epoch4:  22%|#033[34m██▏       #033[0m| 6/27 [00:58<03:25,  9.81s/it]\u001B[0m\n",
      "\u001B[35mTraining Epoch4:  22%|#033[34m██▏       #033[0m| 6/27 [00:58<03:25,  9.81s/it]\u001B[0m\n",
      "\u001B[35mTraining Epoch4:  22%|#033[34m██▏       #033[0m| 6/27 [00:58<03:25,  9.81s/it]\u001B[0m\n",
      "\u001B[34mTraining Epoch4:  26%|#033[34m██▌       #033[0m| 7/27 [01:08<03:16,  9.81s/it]\u001B[0m\n",
      "\u001B[34mTraining Epoch4:  26%|#033[34m██▌       #033[0m| 7/27 [01:08<03:16,  9.81s/it]\u001B[0m\n",
      "\u001B[34mTraining Epoch4:  26%|#033[34m██▌       #033[0m| 7/27 [01:08<03:16,  9.81s/it]\u001B[0m\n",
      "\u001B[34mstep 6 is completed and loss is 1.4780508279800415\u001B[0m\n",
      "\u001B[34mTraining Epoch4:  26%|#033[34m██▌       #033[0m| 7/27 [01:08<03:16,  9.81s/it]\u001B[0m\n",
      "\u001B[35mTraining Epoch4:  26%|#033[34m██▌       #033[0m| 7/27 [01:08<03:16,  9.81s/it]\u001B[0m\n",
      "\u001B[35mTraining Epoch4:  26%|#033[34m██▌       #033[0m| 7/27 [01:08<03:16,  9.81s/it]\u001B[0m\n",
      "\u001B[35mstep 6 is completed and loss is 1.4780508279800415\u001B[0m\n",
      "\u001B[35mTraining Epoch4:  26%|#033[34m██▌       #033[0m| 7/27 [01:08<03:16,  9.81s/it]\u001B[0m\n",
      "\u001B[35mTraining Epoch4:  26%|#033[34m██▌       #033[0m| 7/27 [01:08<03:16,  9.81s/it]\u001B[0m\n",
      "\u001B[35mTraining Epoch4:  30%|#033[34m██▉       #033[0m| 8/27 [01:18<03:06,  9.81s/it]\u001B[0m\n",
      "\u001B[35mTraining Epoch4:  30%|#033[34m██▉       #033[0m| 8/27 [01:18<03:06,  9.81s/it]\u001B[0m\n",
      "\u001B[35mTraining Epoch4:  30%|#033[34m██▉       #033[0m| 8/27 [01:18<03:06,  9.81s/it]\u001B[0m\n",
      "\u001B[35mstep 7 is completed and loss is 1.3257924318313599\u001B[0m\n",
      "\u001B[35mTraining Epoch4:  30%|#033[34m██▉       #033[0m| 8/27 [01:18<03:06,  9.81s/it]\u001B[0m\n",
      "\u001B[34mstep 7 is completed and loss is 1.3257924318313599\u001B[0m\n",
      "\u001B[34mTraining Epoch4:  30%|#033[34m██▉       #033[0m| 8/27 [01:18<03:06,  9.81s/it]\u001B[0m\n",
      "\u001B[34mTraining Epoch4:  30%|#033[34m██▉       #033[0m| 8/27 [01:18<03:06,  9.81s/it]\u001B[0m\n",
      "\u001B[34mTraining Epoch4:  30%|#033[34m██▉       #033[0m| 8/27 [01:18<03:06,  9.81s/it]\u001B[0m\n",
      "\u001B[34mTraining Epoch4:  30%|#033[34m██▉       #033[0m| 8/27 [01:18<03:06,  9.81s/it]\u001B[0m\n",
      "\u001B[35mTraining Epoch4:  33%|#033[34m███▎      #033[0m| 9/27 [01:28<02:56,  9.81s/it]\u001B[0m\n",
      "\u001B[35mTraining Epoch4:  33%|#033[34m███▎      #033[0m| 9/27 [01:28<02:56,  9.81s/it]\u001B[0m\n",
      "\u001B[35mstep 8 is completed and loss is 1.5205802917480469\u001B[0m\n",
      "\u001B[35mTraining Epoch4:  33%|#033[34m███▎      #033[0m| 9/27 [01:28<02:56,  9.81s/it]\u001B[0m\n",
      "\u001B[35mTraining Epoch4:  33%|#033[34m███▎      #033[0m| 9/27 [01:28<02:56,  9.81s/it]\u001B[0m\n",
      "\u001B[34mTraining Epoch4:  33%|#033[34m███▎      #033[0m| 9/27 [01:28<02:56,  9.81s/it]\u001B[0m\n",
      "\u001B[34mTraining Epoch4:  33%|#033[34m███▎      #033[0m| 9/27 [01:28<02:56,  9.81s/it]\u001B[0m\n",
      "\u001B[34mTraining Epoch4:  33%|#033[34m███▎      #033[0m| 9/27 [01:28<02:56,  9.81s/it]\u001B[0m\n",
      "\u001B[34mstep 8 is completed and loss is 1.5205802917480469\u001B[0m\n",
      "\u001B[34mTraining Epoch4:  33%|#033[34m███▎      #033[0m| 9/27 [01:28<02:56,  9.81s/it]\u001B[0m\n",
      "\u001B[34mTraining Epoch4:  37%|#033[34m███▋      #033[0m| 10/27 [01:38<02:46,  9.82s/it]\u001B[0m\n",
      "\u001B[34mTraining Epoch4:  37%|#033[34m███▋      #033[0m| 10/27 [01:38<02:46,  9.82s/it]\u001B[0m\n",
      "\u001B[34mstep 9 is completed and loss is 1.0995334386825562\u001B[0m\n",
      "\u001B[34mTraining Epoch4:  37%|#033[34m███▋      #033[0m| 10/27 [01:38<02:46,  9.82s/it]\u001B[0m\n",
      "\u001B[34mTraining Epoch4:  37%|#033[34m███▋      #033[0m| 10/27 [01:38<02:46,  9.82s/it]\u001B[0m\n",
      "\u001B[35mstep 9 is completed and loss is 1.0995334386825562\u001B[0m\n",
      "\u001B[35mTraining Epoch4:  37%|#033[34m███▋      #033[0m| 10/27 [01:38<02:46,  9.81s/it]\u001B[0m\n",
      "\u001B[35mTraining Epoch4:  37%|#033[34m███▋      #033[0m| 10/27 [01:38<02:46,  9.81s/it]\u001B[0m\n",
      "\u001B[35mTraining Epoch4:  37%|#033[34m███▋      #033[0m| 10/27 [01:38<02:46,  9.81s/it]\u001B[0m\n",
      "\u001B[35mTraining Epoch4:  37%|#033[34m███▋      #033[0m| 10/27 [01:38<02:46,  9.81s/it]\u001B[0m\n",
      "\u001B[34mTraining Epoch4:  41%|#033[34m████      #033[0m| 11/27 [01:47<02:37,  9.82s/it]\u001B[0m\n",
      "\u001B[34mTraining Epoch4:  41%|#033[34m████      #033[0m| 11/27 [01:47<02:37,  9.82s/it]\u001B[0m\n",
      "\u001B[34mTraining Epoch4:  41%|#033[34m████      #033[0m| 11/27 [01:47<02:37,  9.82s/it]\u001B[0m\n",
      "\u001B[34mstep 10 is completed and loss is 1.1544073820114136\u001B[0m\n",
      "\u001B[34mTraining Epoch4:  41%|#033[34m████      #033[0m| 11/27 [01:47<02:37,  9.82s/it]\u001B[0m\n",
      "\u001B[35mstep 10 is completed and loss is 1.1544073820114136\u001B[0m\n",
      "\u001B[35mTraining Epoch4:  41%|#033[34m████      #033[0m| 11/27 [01:47<02:36,  9.81s/it]#015Training Epoch4:  41%|#033[34m████      #033[0m| 11/27 [01:47<02:36,  9.81s/it]\u001B[0m\n",
      "\u001B[35mTraining Epoch4:  41%|#033[34m████      #033[0m| 11/27 [01:47<02:36,  9.81s/it]\u001B[0m\n",
      "\u001B[35mTraining Epoch4:  41%|#033[34m████      #033[0m| 11/27 [01:47<02:36,  9.81s/it]\u001B[0m\n",
      "\u001B[34mstep 11 is completed and loss is 1.156472086906433\u001B[0m\n",
      "\u001B[34mTraining Epoch4:  44%|#033[34m████▍     #033[0m| 12/27 [01:57<02:27,  9.83s/it]\u001B[0m\n",
      "\u001B[34mTraining Epoch4:  44%|#033[34m████▍     #033[0m| 12/27 [01:57<02:27,  9.83s/it]\u001B[0m\n",
      "\u001B[34mTraining Epoch4:  44%|#033[34m████▍     #033[0m| 12/27 [01:57<02:27,  9.83s/it]\u001B[0m\n",
      "\u001B[34mTraining Epoch4:  44%|#033[34m████▍     #033[0m| 12/27 [01:57<02:27,  9.83s/it]\u001B[0m\n",
      "\u001B[35mstep 11 is completed and loss is 1.156472086906433\u001B[0m\n",
      "\u001B[35mTraining Epoch4:  44%|#033[34m████▍     #033[0m| 12/27 [01:57<02:27,  9.81s/it]\u001B[0m\n",
      "\u001B[35mTraining Epoch4:  44%|#033[34m████▍     #033[0m| 12/27 [01:57<02:27,  9.81s/it]\u001B[0m\n",
      "\u001B[35mTraining Epoch4:  44%|#033[34m████▍     #033[0m| 12/27 [01:57<02:27,  9.81s/it]\u001B[0m\n",
      "\u001B[35mTraining Epoch4:  44%|#033[34m████▍     #033[0m| 12/27 [01:57<02:27,  9.81s/it]\u001B[0m\n",
      "\u001B[34mstep 12 is completed and loss is 1.256338119506836\u001B[0m\n",
      "\u001B[34mTraining Epoch4:  48%|#033[34m████▊     #033[0m| 13/27 [02:07<02:18,  9.86s/it]\u001B[0m\n",
      "\u001B[34mTraining Epoch4:  48%|#033[34m████▊     #033[0m| 13/27 [02:07<02:18,  9.86s/it]\u001B[0m\n",
      "\u001B[34mTraining Epoch4:  48%|#033[34m████▊     #033[0m| 13/27 [02:07<02:18,  9.86s/it]\u001B[0m\n",
      "\u001B[34mTraining Epoch4:  48%|#033[34m████▊     #033[0m| 13/27 [02:07<02:18,  9.86s/it]\u001B[0m\n",
      "\u001B[35mTraining Epoch4:  48%|#033[34m████▊     #033[0m| 13/27 [02:07<02:18,  9.87s/it]\u001B[0m\n",
      "\u001B[35mTraining Epoch4:  48%|#033[34m████▊     #033[0m| 13/27 [02:07<02:18,  9.87s/it]\u001B[0m\n",
      "\u001B[35mstep 12 is completed and loss is 1.256338119506836\u001B[0m\n",
      "\u001B[35mTraining Epoch4:  48%|#033[34m████▊     #033[0m| 13/27 [02:07<02:18,  9.87s/it]#015Training Epoch4:  48%|#033[34m████▊     #033[0m| 13/27 [02:07<02:18,  9.87s/it]\u001B[0m\n",
      "\u001B[35mTraining Epoch4:  52%|#033[34m█████▏    #033[0m| 14/27 [02:17<02:08,  9.85s/it]\u001B[0m\n",
      "\u001B[35mTraining Epoch4:  52%|#033[34m█████▏    #033[0m| 14/27 [02:17<02:08,  9.85s/it]\u001B[0m\n",
      "\u001B[35mstep 13 is completed and loss is 0.9347164034843445\u001B[0m\n",
      "\u001B[35mTraining Epoch4:  52%|#033[34m█████▏    #033[0m| 14/27 [02:17<02:08,  9.85s/it]\u001B[0m\n",
      "\u001B[35mTraining Epoch4:  52%|#033[34m█████▏    #033[0m| 14/27 [02:17<02:08,  9.85s/it]\u001B[0m\n",
      "\u001B[34mTraining Epoch4:  52%|#033[34m█████▏    #033[0m| 14/27 [02:17<02:08,  9.88s/it]\u001B[0m\n",
      "\u001B[34mTraining Epoch4:  52%|#033[34m█████▏    #033[0m| 14/27 [02:17<02:08,  9.88s/it]\u001B[0m\n",
      "\u001B[34mstep 13 is completed and loss is 0.9347164034843445\u001B[0m\n",
      "\u001B[34mTraining Epoch4:  52%|#033[34m█████▏    #033[0m| 14/27 [02:17<02:08,  9.88s/it]\u001B[0m\n",
      "\u001B[34mTraining Epoch4:  52%|#033[34m█████▏    #033[0m| 14/27 [02:17<02:08,  9.88s/it]\u001B[0m\n",
      "\u001B[35mstep 14 is completed and loss is 1.315598964691162\u001B[0m\n",
      "\u001B[35mTraining Epoch4:  56%|#033[34m█████▌    #033[0m| 15/27 [02:27<01:58,  9.84s/it]\u001B[0m\n",
      "\u001B[35mTraining Epoch4:  56%|#033[34m█████▌    #033[0m| 15/27 [02:27<01:58,  9.84s/it]\u001B[0m\n",
      "\u001B[35mTraining Epoch4:  56%|#033[34m█████▌    #033[0m| 15/27 [02:27<01:58,  9.84s/it]#015Training Epoch4:  56%|#033[34m█████▌    #033[0m| 15/27 [02:27<01:58,  9.84s/it]\u001B[0m\n",
      "\u001B[34mTraining Epoch4:  56%|#033[34m█████▌    #033[0m| 15/27 [02:27<01:58,  9.86s/it]\u001B[0m\n",
      "\u001B[34mstep 14 is completed and loss is 1.315598964691162\u001B[0m\n",
      "\u001B[34mTraining Epoch4:  56%|#033[34m█████▌    #033[0m| 15/27 [02:27<01:58,  9.86s/it]\u001B[0m\n",
      "\u001B[34mTraining Epoch4:  56%|#033[34m█████▌    #033[0m| 15/27 [02:27<01:58,  9.86s/it]\u001B[0m\n",
      "\u001B[34mTraining Epoch4:  56%|#033[34m█████▌    #033[0m| 15/27 [02:27<01:58,  9.86s/it]\u001B[0m\n",
      "\u001B[35mTraining Epoch4:  59%|#033[34m█████▉    #033[0m| 16/27 [02:37<01:48,  9.83s/it]\u001B[0m\n",
      "\u001B[35mTraining Epoch4:  59%|#033[34m█████▉    #033[0m| 16/27 [02:37<01:48,  9.83s/it]\u001B[0m\n",
      "\u001B[35mTraining Epoch4:  59%|#033[34m█████▉    #033[0m| 16/27 [02:37<01:48,  9.83s/it]\u001B[0m\n",
      "\u001B[35mstep 15 is completed and loss is 1.4137895107269287\u001B[0m\n",
      "\u001B[35mTraining Epoch4:  59%|#033[34m█████▉    #033[0m| 16/27 [02:37<01:48,  9.83s/it]\u001B[0m\n",
      "\u001B[34mTraining Epoch4:  59%|#033[34m█████▉    #033[0m| 16/27 [02:37<01:48,  9.85s/it]#015Training Epoch4:  59%|#033[34m█████▉    #033[0m| 16/27 [02:37<01:48,  9.85s/it]\u001B[0m\n",
      "\u001B[34mstep 15 is completed and loss is 1.4137895107269287\u001B[0m\n",
      "\u001B[34mTraining Epoch4:  59%|#033[34m█████▉    #033[0m| 16/27 [02:37<01:48,  9.85s/it]\u001B[0m\n",
      "\u001B[34mTraining Epoch4:  59%|#033[34m█████▉    #033[0m| 16/27 [02:37<01:48,  9.85s/it]\u001B[0m\n",
      "\u001B[34mTraining Epoch4:  63%|#033[34m██████▎   #033[0m| 17/27 [02:47<01:38,  9.84s/it]\u001B[0m\n",
      "\u001B[34mTraining Epoch4:  63%|#033[34m██████▎   #033[0m| 17/27 [02:47<01:38,  9.84s/it]\u001B[0m\n",
      "\u001B[34mstep 16 is completed and loss is 1.4333951473236084\u001B[0m\n",
      "\u001B[34mTraining Epoch4:  63%|#033[34m██████▎   #033[0m| 17/27 [02:47<01:38,  9.84s/it]#015Training Epoch4:  63%|#033[34m██████▎   #033[0m| 17/27 [02:47<01:38,  9.84s/it]\u001B[0m\n",
      "\u001B[35mTraining Epoch4:  63%|#033[34m██████▎   #033[0m| 17/27 [02:46<01:38,  9.82s/it]\u001B[0m\n",
      "\u001B[35mstep 16 is completed and loss is 1.4333951473236084\u001B[0m\n",
      "\u001B[35mTraining Epoch4:  63%|#033[34m██████▎   #033[0m| 17/27 [02:46<01:38,  9.82s/it]\u001B[0m\n",
      "\u001B[35mTraining Epoch4:  63%|#033[34m██████▎   #033[0m| 17/27 [02:46<01:38,  9.82s/it]\u001B[0m\n",
      "\u001B[35mTraining Epoch4:  63%|#033[34m██████▎   #033[0m| 17/27 [02:46<01:38,  9.82s/it]\u001B[0m\n",
      "\u001B[34mTraining Epoch4:  67%|#033[34m██████▋   #033[0m| 18/27 [02:56<01:28,  9.83s/it]\u001B[0m\n",
      "\u001B[34mstep 17 is completed and loss is 1.3164677619934082\u001B[0m\n",
      "\u001B[34mTraining Epoch4:  67%|#033[34m██████▋   #033[0m| 18/27 [02:56<01:28,  9.83s/it]\u001B[0m\n",
      "\u001B[34mTraining Epoch4:  67%|#033[34m██████▋   #033[0m| 18/27 [02:56<01:28,  9.83s/it]\u001B[0m\n",
      "\u001B[34mTraining Epoch4:  67%|#033[34m██████▋   #033[0m| 18/27 [02:56<01:28,  9.83s/it]\u001B[0m\n",
      "\u001B[35mTraining Epoch4:  67%|#033[34m██████▋   #033[0m| 18/27 [02:56<01:28,  9.81s/it]\u001B[0m\n",
      "\u001B[35mstep 17 is completed and loss is 1.3164677619934082\u001B[0m\n",
      "\u001B[35mTraining Epoch4:  67%|#033[34m██████▋   #033[0m| 18/27 [02:56<01:28,  9.81s/it]\u001B[0m\n",
      "\u001B[35mTraining Epoch4:  67%|#033[34m██████▋   #033[0m| 18/27 [02:56<01:28,  9.81s/it]\u001B[0m\n",
      "\u001B[35mTraining Epoch4:  67%|#033[34m██████▋   #033[0m| 18/27 [02:56<01:28,  9.81s/it]\u001B[0m\n",
      "\u001B[35mTraining Epoch4:  70%|#033[34m███████   #033[0m| 19/27 [03:06<01:18,  9.81s/it]\u001B[0m\n",
      "\u001B[35mTraining Epoch4:  70%|#033[34m███████   #033[0m| 19/27 [03:06<01:18,  9.81s/it]\u001B[0m\n",
      "\u001B[35mTraining Epoch4:  70%|#033[34m███████   #033[0m| 19/27 [03:06<01:18,  9.81s/it]\u001B[0m\n",
      "\u001B[35mstep 18 is completed and loss is 1.2095344066619873\u001B[0m\n",
      "\u001B[35mTraining Epoch4:  70%|#033[34m███████   #033[0m| 19/27 [03:06<01:18,  9.81s/it]\u001B[0m\n",
      "\u001B[34mTraining Epoch4:  70%|#033[34m███████   #033[0m| 19/27 [03:06<01:18,  9.83s/it]\u001B[0m\n",
      "\u001B[34mTraining Epoch4:  70%|#033[34m███████   #033[0m| 19/27 [03:06<01:18,  9.83s/it]#015Training Epoch4:  70%|#033[34m███████   #033[0m| 19/27 [03:06<01:18,  9.83s/it]\u001B[0m\n",
      "\u001B[34mstep 18 is completed and loss is 1.2095344066619873\u001B[0m\n",
      "\u001B[34mTraining Epoch4:  70%|#033[34m███████   #033[0m| 19/27 [03:06<01:18,  9.83s/it]\u001B[0m\n",
      "\u001B[35mTraining Epoch4:  74%|#033[34m███████▍  #033[0m| 20/27 [03:16<01:08,  9.81s/it]\u001B[0m\n",
      "\u001B[35mTraining Epoch4:  74%|#033[34m███████▍  #033[0m| 20/27 [03:16<01:08,  9.81s/it]\u001B[0m\n",
      "\u001B[35mstep 19 is completed and loss is 1.1227494478225708\u001B[0m\n",
      "\u001B[35mTraining Epoch4:  74%|#033[34m███████▍  #033[0m| 20/27 [03:16<01:08,  9.81s/it]\u001B[0m\n",
      "\u001B[35mTraining Epoch4:  74%|#033[34m███████▍  #033[0m| 20/27 [03:16<01:08,  9.81s/it]\u001B[0m\n",
      "\u001B[34mTraining Epoch4:  74%|#033[34m███████▍  #033[0m| 20/27 [03:16<01:08,  9.82s/it]\u001B[0m\n",
      "\u001B[34mTraining Epoch4:  74%|#033[34m███████▍  #033[0m| 20/27 [03:16<01:08,  9.82s/it]\u001B[0m\n",
      "\u001B[34mstep 19 is completed and loss is 1.1227494478225708\u001B[0m\n",
      "\u001B[34mTraining Epoch4:  74%|#033[34m███████▍  #033[0m| 20/27 [03:16<01:08,  9.82s/it]\u001B[0m\n",
      "\u001B[34mTraining Epoch4:  74%|#033[34m███████▍  #033[0m| 20/27 [03:16<01:08,  9.82s/it]\u001B[0m\n",
      "\u001B[35mTraining Epoch4:  78%|#033[34m███████▊  #033[0m| 21/27 [03:26<00:58,  9.81s/it]\u001B[0m\n",
      "\u001B[35mTraining Epoch4:  78%|#033[34m███████▊  #033[0m| 21/27 [03:26<00:58,  9.81s/it]\u001B[0m\n",
      "\u001B[35mTraining Epoch4:  78%|#033[34m███████▊  #033[0m| 21/27 [03:26<00:58,  9.81s/it]\u001B[0m\n",
      "\u001B[35mstep 20 is completed and loss is 1.1509796380996704\u001B[0m\n",
      "\u001B[35mTraining Epoch4:  78%|#033[34m███████▊  #033[0m| 21/27 [03:26<00:58,  9.81s/it]\u001B[0m\n",
      "\u001B[34mTraining Epoch4:  78%|#033[34m███████▊  #033[0m| 21/27 [03:26<00:58,  9.82s/it]\u001B[0m\n",
      "\u001B[34mstep 20 is completed and loss is 1.1509796380996704\u001B[0m\n",
      "\u001B[34mTraining Epoch4:  78%|#033[34m███████▊  #033[0m| 21/27 [03:26<00:58,  9.82s/it]\u001B[0m\n",
      "\u001B[34mTraining Epoch4:  78%|#033[34m███████▊  #033[0m| 21/27 [03:26<00:58,  9.82s/it]\u001B[0m\n",
      "\u001B[34mTraining Epoch4:  78%|#033[34m███████▊  #033[0m| 21/27 [03:26<00:58,  9.82s/it]\u001B[0m\n",
      "\u001B[34mstep 21 is completed and loss is 1.198325276374817\u001B[0m\n",
      "\u001B[34mTraining Epoch4:  81%|#033[34m████████▏ #033[0m| 22/27 [03:36<00:49,  9.82s/it]\u001B[0m\n",
      "\u001B[34mTraining Epoch4:  81%|#033[34m████████▏ #033[0m| 22/27 [03:36<00:49,  9.82s/it]\u001B[0m\n",
      "\u001B[34mTraining Epoch4:  81%|#033[34m████████▏ #033[0m| 22/27 [03:36<00:49,  9.82s/it]\u001B[0m\n",
      "\u001B[34mTraining Epoch4:  81%|#033[34m████████▏ #033[0m| 22/27 [03:36<00:49,  9.82s/it]\u001B[0m\n",
      "\u001B[35mstep 21 is completed and loss is 1.198325276374817\u001B[0m\n",
      "\u001B[35mTraining Epoch4:  81%|#033[34m████████▏ #033[0m| 22/27 [03:35<00:49,  9.81s/it]\u001B[0m\n",
      "\u001B[35mTraining Epoch4:  81%|#033[34m████████▏ #033[0m| 22/27 [03:35<00:49,  9.81s/it]\u001B[0m\n",
      "\u001B[35mTraining Epoch4:  81%|#033[34m████████▏ #033[0m| 22/27 [03:35<00:49,  9.81s/it]\u001B[0m\n",
      "\u001B[35mTraining Epoch4:  81%|#033[34m████████▏ #033[0m| 22/27 [03:35<00:49,  9.81s/it]\u001B[0m\n",
      "\u001B[34mstep 22 is completed and loss is 1.3574827909469604\u001B[0m\n",
      "\u001B[34mTraining Epoch4:  85%|#033[34m████████▌ #033[0m| 23/27 [03:45<00:39,  9.82s/it]#015Training Epoch4:  85%|#033[34m████████▌ #033[0m| 23/27 [03:45<00:39,  9.82s/it]\u001B[0m\n",
      "\u001B[34mTraining Epoch4:  85%|#033[34m████████▌ #033[0m| 23/27 [03:45<00:39,  9.82s/it]\u001B[0m\n",
      "\u001B[34mTraining Epoch4:  85%|#033[34m████████▌ #033[0m| 23/27 [03:45<00:39,  9.82s/it]\u001B[0m\n",
      "\u001B[35mTraining Epoch4:  85%|#033[34m████████▌ #033[0m| 23/27 [03:45<00:39,  9.80s/it]\u001B[0m\n",
      "\u001B[35mTraining Epoch4:  85%|#033[34m████████▌ #033[0m| 23/27 [03:45<00:39,  9.80s/it]\u001B[0m\n",
      "\u001B[35mstep 22 is completed and loss is 1.3574827909469604\u001B[0m\n",
      "\u001B[35mTraining Epoch4:  85%|#033[34m████████▌ #033[0m| 23/27 [03:45<00:39,  9.80s/it]\u001B[0m\n",
      "\u001B[35mTraining Epoch4:  85%|#033[34m████████▌ #033[0m| 23/27 [03:45<00:39,  9.80s/it]\u001B[0m\n",
      "\u001B[35mTraining Epoch4:  89%|#033[34m████████▉ #033[0m| 24/27 [03:55<00:29,  9.80s/it]\u001B[0m\n",
      "\u001B[35mstep 23 is completed and loss is 1.2003955841064453\u001B[0m\n",
      "\u001B[35mTraining Epoch4:  89%|#033[34m████████▉ #033[0m| 24/27 [03:55<00:29,  9.80s/it]\u001B[0m\n",
      "\u001B[35mTraining Epoch4:  89%|#033[34m████████▉ #033[0m| 24/27 [03:55<00:29,  9.81s/it]\u001B[0m\n",
      "\u001B[35mTraining Epoch4:  89%|#033[34m████████▉ #033[0m| 24/27 [03:55<00:29,  9.81s/it]\u001B[0m\n",
      "\u001B[34mTraining Epoch4:  89%|#033[34m████████▉ #033[0m| 24/27 [03:55<00:29,  9.85s/it]\u001B[0m\n",
      "\u001B[34mTraining Epoch4:  89%|#033[34m████████▉ #033[0m| 24/27 [03:55<00:29,  9.85s/it]\u001B[0m\n",
      "\u001B[34mstep 23 is completed and loss is 1.2003955841064453\u001B[0m\n",
      "\u001B[34mTraining Epoch4:  89%|#033[34m████████▉ #033[0m| 24/27 [03:55<00:29,  9.85s/it]\u001B[0m\n",
      "\u001B[34mTraining Epoch4:  89%|#033[34m████████▉ #033[0m| 24/27 [03:55<00:29,  9.85s/it]\u001B[0m\n",
      "\u001B[35mstep 24 is completed and loss is 1.4388583898544312\u001B[0m\n",
      "\u001B[35mTraining Epoch4:  93%|#033[34m█████████▎#033[0m| 25/27 [04:05<00:19,  9.80s/it]\u001B[0m\n",
      "\u001B[35mTraining Epoch4:  93%|#033[34m█████████▎#033[0m| 25/27 [04:05<00:19,  9.80s/it]\u001B[0m\n",
      "\u001B[35mTraining Epoch4:  93%|#033[34m█████████▎#033[0m| 25/27 [04:05<00:19,  9.80s/it]#015Training Epoch4:  93%|#033[34m█████████▎#033[0m| 25/27 [04:05<00:19,  9.80s/it]\u001B[0m\n",
      "\u001B[34mTraining Epoch4:  93%|#033[34m█████████▎#033[0m| 25/27 [04:05<00:19,  9.85s/it]\u001B[0m\n",
      "\u001B[34mTraining Epoch4:  93%|#033[34m█████████▎#033[0m| 25/27 [04:05<00:19,  9.85s/it]\u001B[0m\n",
      "\u001B[34mstep 24 is completed and loss is 1.4388583898544312\u001B[0m\n",
      "\u001B[34mTraining Epoch4:  93%|#033[34m█████████▎#033[0m| 25/27 [04:05<00:19,  9.85s/it]\u001B[0m\n",
      "\u001B[34mTraining Epoch4:  93%|#033[34m█████████▎#033[0m| 25/27 [04:05<00:19,  9.85s/it]\u001B[0m\n",
      "\u001B[35mTraining Epoch4:  96%|#033[34m█████████▋#033[0m| 26/27 [04:15<00:09,  9.84s/it]#015Training Epoch4:  96%|#033[34m█████████▋#033[0m| 26/27 [04:15<00:09,  9.84s/it]\u001B[0m\n",
      "\u001B[35mstep 25 is completed and loss is 1.0688066482543945\u001B[0m\n",
      "\u001B[35mTraining Epoch4:  96%|#033[34m█████████▋#033[0m| 26/27 [04:15<00:09,  9.84s/it]\u001B[0m\n",
      "\u001B[35mTraining Epoch4:  96%|#033[34m█████████▋#033[0m| 26/27 [04:15<00:09,  9.84s/it]\u001B[0m\n",
      "\u001B[34mTraining Epoch4:  96%|#033[34m█████████▋#033[0m| 26/27 [04:15<00:09,  9.87s/it]\u001B[0m\n",
      "\u001B[34mTraining Epoch4:  96%|#033[34m█████████▋#033[0m| 26/27 [04:15<00:09,  9.87s/it]\u001B[0m\n",
      "\u001B[34mstep 25 is completed and loss is 1.0688066482543945\u001B[0m\n",
      "\u001B[34mTraining Epoch4:  96%|#033[34m█████████▋#033[0m| 26/27 [04:15<00:09,  9.87s/it]\u001B[0m\n",
      "\u001B[34mTraining Epoch4:  96%|#033[34m█████████▋#033[0m| 26/27 [04:15<00:09,  9.87s/it]\u001B[0m\n",
      "\u001B[35mTraining Epoch4: 100%|#033[34m██████████#033[0m| 27/27 [04:25<00:00,  9.84s/it]\u001B[0m\n",
      "\u001B[35mTraining Epoch4: 100%|#033[34m██████████#033[0m| 27/27 [04:25<00:00,  9.82s/it]\u001B[0m\n",
      "\u001B[35mstep 26 is completed and loss is 1.019423484802246\u001B[0m\n",
      "\u001B[35mTraining Epoch4: 100%|#033[34m██████████#033[0m| 27/27 [04:25<00:00,  9.84s/it]\u001B[0m\n",
      "\u001B[35mTraining Epoch4: 100%|#033[34m██████████#033[0m| 27/27 [04:25<00:00,  9.84s/it]\u001B[0m\n",
      "\u001B[35mTraining Epoch4: 100%|#033[34m██████████#033[0m| 27/27 [04:25<00:00,  9.82s/it]\u001B[0m\n",
      "\u001B[35mTraining Epoch4: 100%|#033[34m██████████#033[0m| 27/27 [04:25<00:00,  9.82s/it]\u001B[0m\n",
      "\u001B[35mTraining Epoch4: 100%|#033[34m██████████#033[0m| 27/27 [04:25<00:00,  9.84s/it]\u001B[0m\n",
      "\u001B[35mTraining Epoch4: 100%|#033[34m██████████#033[0m| 27/27 [04:25<00:00,  9.82s/it]\u001B[0m\n",
      "\u001B[35mMax CUDA memory allocated was 8 GB\u001B[0m\n",
      "\u001B[35mMax CUDA memory reserved was 9 GB\u001B[0m\n",
      "\u001B[35mPeak active CUDA memory was 8 GB\u001B[0m\n",
      "\u001B[35mCuda Malloc retires : 0\u001B[0m\n",
      "\u001B[35mCPU Total Peak Memory consumed during the train (max): 2 GB\u001B[0m\n",
      "\u001B[35mevaluating Epoch:   0%|#033[32m          #033[0m| 0/28 [00:00<?, ?it/s]\u001B[0m\n",
      "\u001B[35mevaluating Epoch:   0%|#033[32m          #033[0m| 0/28 [00:00<?, ?it/s]#015evaluating Epoch:   0%|#033[32m          #033[0m| 0/28 [00:00<?, ?it/s]\u001B[0m\n",
      "\u001B[35mevaluating Epoch:   0%|#033[32m          #033[0m| 0/28 [00:00<?, ?it/s]\u001B[0m\n",
      "\u001B[34mTraining Epoch4: 100%|#033[34m██████████#033[0m| 27/27 [04:25<00:00,  9.85s/it]\u001B[0m\n",
      "\u001B[34mTraining Epoch4: 100%|#033[34m██████████#033[0m| 27/27 [04:25<00:00,  9.83s/it]\u001B[0m\n",
      "\u001B[34mTraining Epoch4: 100%|#033[34m██████████#033[0m| 27/27 [04:25<00:00,  9.85s/it]\u001B[0m\n",
      "\u001B[34mTraining Epoch4: 100%|#033[34m██████████#033[0m| 27/27 [04:25<00:00,  9.83s/it]\u001B[0m\n",
      "\u001B[34mstep 26 is completed and loss is 1.019423484802246\u001B[0m\n",
      "\u001B[34mTraining Epoch4: 100%|#033[34m██████████#033[0m| 27/27 [04:25<00:00,  9.85s/it]\u001B[0m\n",
      "\u001B[34mTraining Epoch4: 100%|#033[34m██████████#033[0m| 27/27 [04:25<00:00,  9.83s/it]\u001B[0m\n",
      "\u001B[34mTraining Epoch4: 100%|#033[34m██████████#033[0m| 27/27 [04:25<00:00,  9.85s/it]\u001B[0m\n",
      "\u001B[34mTraining Epoch4: 100%|#033[34m██████████#033[0m| 27/27 [04:25<00:00,  9.83s/it]\u001B[0m\n",
      "\u001B[34mMax CUDA memory allocated was 8 GB\u001B[0m\n",
      "\u001B[34mMax CUDA memory reserved was 9 GB\u001B[0m\n",
      "\u001B[34mPeak active CUDA memory was 8 GB\u001B[0m\n",
      "\u001B[34mCuda Malloc retires : 0\u001B[0m\n",
      "\u001B[34mCPU Total Peak Memory consumed during the train (max): 2 GB\u001B[0m\n",
      "\u001B[34mevaluating Epoch:   0%|#033[32m          #033[0m| 0/28 [00:00<?, ?it/s]\u001B[0m\n",
      "\u001B[34mevaluating Epoch:   0%|#033[32m          #033[0m| 0/28 [00:00<?, ?it/s]\u001B[0m\n",
      "\u001B[34mevaluating Epoch:   0%|#033[32m          #033[0m| 0/28 [00:00<?, ?it/s]\u001B[0m\n",
      "\u001B[34mevaluating Epoch:   0%|#033[32m          #033[0m| 0/28 [00:00<?, ?it/s]\u001B[0m\n",
      "\u001B[35mevaluating Epoch:   4%|#033[32m▎         #033[0m| 1/28 [00:03<01:36,  3.58s/it]\u001B[0m\n",
      "\u001B[35mevaluating Epoch:   4%|#033[32m▎         #033[0m| 1/28 [00:03<01:36,  3.58s/it]\u001B[0m\n",
      "\u001B[35mevaluating Epoch:   4%|#033[32m▎         #033[0m| 1/28 [00:03<01:36,  3.58s/it]\u001B[0m\n",
      "\u001B[35mevaluating Epoch:   4%|#033[32m▎         #033[0m| 1/28 [00:03<01:36,  3.58s/it]\u001B[0m\n",
      "\u001B[34mevaluating Epoch:   4%|#033[32m▎         #033[0m| 1/28 [00:03<01:36,  3.59s/it]\u001B[0m\n",
      "\u001B[34mevaluating Epoch:   4%|#033[32m▎         #033[0m| 1/28 [00:03<01:36,  3.59s/it]\u001B[0m\n",
      "\u001B[34mevaluating Epoch:   4%|#033[32m▎         #033[0m| 1/28 [00:03<01:37,  3.60s/it]\u001B[0m\n",
      "\u001B[34mevaluating Epoch:   4%|#033[32m▎         #033[0m| 1/28 [00:03<01:37,  3.59s/it]\u001B[0m\n",
      "\u001B[35mevaluating Epoch:   7%|#033[32m▋         #033[0m| 2/28 [00:07<01:31,  3.51s/it]\u001B[0m\n",
      "\u001B[35mevaluating Epoch:   7%|#033[32m▋         #033[0m| 2/28 [00:07<01:31,  3.51s/it]\u001B[0m\n",
      "\u001B[35mevaluating Epoch:   7%|#033[32m▋         #033[0m| 2/28 [00:07<01:31,  3.51s/it]\u001B[0m\n",
      "\u001B[35mevaluating Epoch:   7%|#033[32m▋         #033[0m| 2/28 [00:07<01:31,  3.51s/it]\u001B[0m\n",
      "\u001B[34mevaluating Epoch:   7%|#033[32m▋         #033[0m| 2/28 [00:07<01:31,  3.52s/it]\u001B[0m\n",
      "\u001B[34mevaluating Epoch:   7%|#033[32m▋         #033[0m| 2/28 [00:07<01:31,  3.52s/it]#015evaluating Epoch:   7%|#033[32m▋         #033[0m| 2/28 [00:07<01:31,  3.52s/it]\u001B[0m\n",
      "\u001B[34mevaluating Epoch:   7%|#033[32m▋         #033[0m| 2/28 [00:07<01:31,  3.52s/it]\u001B[0m\n",
      "\u001B[34mevaluating Epoch:  11%|#033[32m█         #033[0m| 3/28 [00:10<01:27,  3.49s/it]\u001B[0m\n",
      "\u001B[34mevaluating Epoch:  11%|#033[32m█         #033[0m| 3/28 [00:10<01:27,  3.50s/it]#015evaluating Epoch:  11%|#033[32m█         #033[0m| 3/28 [00:10<01:27,  3.49s/it]\u001B[0m\n",
      "\u001B[34mevaluating Epoch:  11%|#033[32m█         #033[0m| 3/28 [00:10<01:27,  3.50s/it]\u001B[0m\n",
      "\u001B[35mevaluating Epoch:  11%|#033[32m█         #033[0m| 3/28 [00:10<01:27,  3.48s/it]#015evaluating Epoch:  11%|#033[32m█         #033[0m| 3/28 [00:10<01:27,  3.48s/it]\u001B[0m\n",
      "\u001B[35mevaluating Epoch:  11%|#033[32m█         #033[0m| 3/28 [00:10<01:27,  3.48s/it]\u001B[0m\n",
      "\u001B[35mevaluating Epoch:  11%|#033[32m█         #033[0m| 3/28 [00:10<01:27,  3.49s/it]\u001B[0m\n",
      "\u001B[35mevaluating Epoch:  14%|#033[32m█▍        #033[0m| 4/28 [00:13<01:23,  3.47s/it]\u001B[0m\n",
      "\u001B[35mevaluating Epoch:  14%|#033[32m█▍        #033[0m| 4/28 [00:13<01:23,  3.47s/it]\u001B[0m\n",
      "\u001B[35mevaluating Epoch:  14%|#033[32m█▍        #033[0m| 4/28 [00:13<01:23,  3.47s/it]\u001B[0m\n",
      "\u001B[35mevaluating Epoch:  14%|#033[32m█▍        #033[0m| 4/28 [00:13<01:23,  3.47s/it]\u001B[0m\n",
      "\u001B[34mevaluating Epoch:  14%|#033[32m█▍        #033[0m| 4/28 [00:13<01:23,  3.48s/it]\u001B[0m\n",
      "\u001B[34mevaluating Epoch:  14%|#033[32m█▍        #033[0m| 4/28 [00:13<01:23,  3.48s/it]#015evaluating Epoch:  14%|#033[32m█▍        #033[0m| 4/28 [00:13<01:23,  3.48s/it]\u001B[0m\n",
      "\u001B[34mevaluating Epoch:  14%|#033[32m█▍        #033[0m| 4/28 [00:13<01:23,  3.48s/it]\u001B[0m\n",
      "\u001B[34mevaluating Epoch:  18%|#033[32m█▊        #033[0m| 5/28 [00:17<01:20,  3.48s/it]\u001B[0m\n",
      "\u001B[34mevaluating Epoch:  18%|#033[32m█▊        #033[0m| 5/28 [00:17<01:20,  3.48s/it]\u001B[0m\n",
      "\u001B[34mevaluating Epoch:  18%|#033[32m█▊        #033[0m| 5/28 [00:17<01:20,  3.48s/it]\u001B[0m\n",
      "\u001B[34mevaluating Epoch:  18%|#033[32m█▊        #033[0m| 5/28 [00:17<01:20,  3.48s/it]\u001B[0m\n",
      "\u001B[35mevaluating Epoch:  18%|#033[32m█▊        #033[0m| 5/28 [00:17<01:19,  3.47s/it]\u001B[0m\n",
      "\u001B[35mevaluating Epoch:  18%|#033[32m█▊        #033[0m| 5/28 [00:17<01:19,  3.47s/it]\u001B[0m\n",
      "\u001B[35mevaluating Epoch:  18%|#033[32m█▊        #033[0m| 5/28 [00:17<01:19,  3.47s/it]#015evaluating Epoch:  18%|#033[32m█▊        #033[0m| 5/28 [00:17<01:19,  3.47s/it]\u001B[0m\n",
      "\u001B[35mevaluating Epoch:  21%|#033[32m██▏       #033[0m| 6/28 [00:20<01:16,  3.46s/it]\u001B[0m\n",
      "\u001B[35mevaluating Epoch:  21%|#033[32m██▏       #033[0m| 6/28 [00:20<01:16,  3.46s/it]\u001B[0m\n",
      "\u001B[35mevaluating Epoch:  21%|#033[32m██▏       #033[0m| 6/28 [00:20<01:16,  3.46s/it]\u001B[0m\n",
      "\u001B[35mevaluating Epoch:  21%|#033[32m██▏       #033[0m| 6/28 [00:20<01:16,  3.46s/it]\u001B[0m\n",
      "\u001B[34mevaluating Epoch:  21%|#033[32m██▏       #033[0m| 6/28 [00:20<01:16,  3.48s/it]\u001B[0m\n",
      "\u001B[34mevaluating Epoch:  21%|#033[32m██▏       #033[0m| 6/28 [00:20<01:16,  3.48s/it]\u001B[0m\n",
      "\u001B[34mevaluating Epoch:  21%|#033[32m██▏       #033[0m| 6/28 [00:20<01:16,  3.48s/it]\u001B[0m\n",
      "\u001B[34mevaluating Epoch:  21%|#033[32m██▏       #033[0m| 6/28 [00:20<01:16,  3.48s/it]\u001B[0m\n",
      "\u001B[34mevaluating Epoch:  25%|#033[32m██▌       #033[0m| 7/28 [00:24<01:12,  3.47s/it]\u001B[0m\n",
      "\u001B[34mevaluating Epoch:  25%|#033[32m██▌       #033[0m| 7/28 [00:24<01:12,  3.47s/it]\u001B[0m\n",
      "\u001B[34mevaluating Epoch:  25%|#033[32m██▌       #033[0m| 7/28 [00:24<01:12,  3.47s/it]\u001B[0m\n",
      "\u001B[34mevaluating Epoch:  25%|#033[32m██▌       #033[0m| 7/28 [00:24<01:12,  3.47s/it]\u001B[0m\n",
      "\u001B[35mevaluating Epoch:  25%|#033[32m██▌       #033[0m| 7/28 [00:24<01:12,  3.46s/it]\u001B[0m\n",
      "\u001B[35mevaluating Epoch:  25%|#033[32m██▌       #033[0m| 7/28 [00:24<01:12,  3.46s/it]\u001B[0m\n",
      "\u001B[35mevaluating Epoch:  25%|#033[32m██▌       #033[0m| 7/28 [00:24<01:12,  3.46s/it]\u001B[0m\n",
      "\u001B[35mevaluating Epoch:  25%|#033[32m██▌       #033[0m| 7/28 [00:24<01:12,  3.46s/it]\u001B[0m\n",
      "\u001B[35mevaluating Epoch:  29%|#033[32m██▊       #033[0m| 8/28 [00:27<01:09,  3.46s/it]#015evaluating Epoch:  29%|#033[32m██▊       #033[0m| 8/28 [00:27<01:09,  3.46s/it]\u001B[0m\n",
      "\u001B[35mevaluating Epoch:  29%|#033[32m██▊       #033[0m| 8/28 [00:27<01:09,  3.46s/it]\u001B[0m\n",
      "\u001B[35mevaluating Epoch:  29%|#033[32m██▊       #033[0m| 8/28 [00:27<01:09,  3.46s/it]\u001B[0m\n",
      "\u001B[34mevaluating Epoch:  29%|#033[32m██▊       #033[0m| 8/28 [00:27<01:09,  3.47s/it]\u001B[0m\n",
      "\u001B[34mevaluating Epoch:  29%|#033[32m██▊       #033[0m| 8/28 [00:27<01:09,  3.47s/it]\u001B[0m\n",
      "\u001B[34mevaluating Epoch:  29%|#033[32m██▊       #033[0m| 8/28 [00:27<01:09,  3.47s/it]\u001B[0m\n",
      "\u001B[34mevaluating Epoch:  29%|#033[32m██▊       #033[0m| 8/28 [00:27<01:09,  3.47s/it]\u001B[0m\n",
      "\u001B[35mevaluating Epoch:  32%|#033[32m███▏      #033[0m| 9/28 [00:31<01:05,  3.46s/it]\u001B[0m\n",
      "\u001B[35mevaluating Epoch:  32%|#033[32m███▏      #033[0m| 9/28 [00:31<01:05,  3.46s/it]\u001B[0m\n",
      "\u001B[35mevaluating Epoch:  32%|#033[32m███▏      #033[0m| 9/28 [00:31<01:05,  3.46s/it]\u001B[0m\n",
      "\u001B[35mevaluating Epoch:  32%|#033[32m███▏      #033[0m| 9/28 [00:31<01:05,  3.46s/it]\u001B[0m\n",
      "\u001B[34mevaluating Epoch:  32%|#033[32m███▏      #033[0m| 9/28 [00:31<01:05,  3.47s/it]#015evaluating Epoch:  32%|#033[32m███▏      #033[0m| 9/28 [00:31<01:05,  3.47s/it]#015evaluating Epoch:  32%|#033[32m███▏      #033[0m| 9/28 [00:31<01:05,  3.47s/it]\u001B[0m\n",
      "\u001B[34mevaluating Epoch:  32%|#033[32m███▏      #033[0m| 9/28 [00:31<01:05,  3.47s/it]\u001B[0m\n",
      "\u001B[35mevaluating Epoch:  36%|#033[32m███▌      #033[0m| 10/28 [00:34<01:02,  3.46s/it]\u001B[0m\n",
      "\u001B[35mevaluating Epoch:  36%|#033[32m███▌      #033[0m| 10/28 [00:34<01:02,  3.46s/it]\u001B[0m\n",
      "\u001B[35mevaluating Epoch:  36%|#033[32m███▌      #033[0m| 10/28 [00:34<01:02,  3.46s/it]\u001B[0m\n",
      "\u001B[35mevaluating Epoch:  36%|#033[32m███▌      #033[0m| 10/28 [00:34<01:02,  3.46s/it]\u001B[0m\n",
      "\u001B[34mevaluating Epoch:  36%|#033[32m███▌      #033[0m| 10/28 [00:34<01:02,  3.47s/it]\u001B[0m\n",
      "\u001B[34mevaluating Epoch:  36%|#033[32m███▌      #033[0m| 10/28 [00:34<01:02,  3.47s/it]\u001B[0m\n",
      "\u001B[34mevaluating Epoch:  36%|#033[32m███▌      #033[0m| 10/28 [00:34<01:02,  3.47s/it]\u001B[0m\n",
      "\u001B[34mevaluating Epoch:  36%|#033[32m███▌      #033[0m| 10/28 [00:34<01:02,  3.47s/it]\u001B[0m\n",
      "\u001B[35mevaluating Epoch:  39%|#033[32m███▉      #033[0m| 11/28 [00:38<00:58,  3.46s/it]\u001B[0m\n",
      "\u001B[35mevaluating Epoch:  39%|#033[32m███▉      #033[0m| 11/28 [00:38<00:58,  3.46s/it]\u001B[0m\n",
      "\u001B[35mevaluating Epoch:  39%|#033[32m███▉      #033[0m| 11/28 [00:38<00:58,  3.46s/it]\u001B[0m\n",
      "\u001B[35mevaluating Epoch:  39%|#033[32m███▉      #033[0m| 11/28 [00:38<00:58,  3.46s/it]\u001B[0m\n",
      "\u001B[34mevaluating Epoch:  39%|#033[32m███▉      #033[0m| 11/28 [00:38<00:59,  3.47s/it]#015evaluating Epoch:  39%|#033[32m███▉      #033[0m| 11/28 [00:38<00:59,  3.47s/it]#015evaluating Epoch:  39%|#033[32m███▉      #033[0m| 11/28 [00:38<00:59,  3.47s/it]\u001B[0m\n",
      "\u001B[34mevaluating Epoch:  39%|#033[32m███▉      #033[0m| 11/28 [00:38<00:59,  3.47s/it]\u001B[0m\n",
      "\u001B[35mevaluating Epoch:  43%|#033[32m████▎     #033[0m| 12/28 [00:41<00:55,  3.46s/it]#015evaluating Epoch:  43%|#033[32m████▎     #033[0m| 12/28 [00:41<00:55,  3.46s/it]\u001B[0m\n",
      "\u001B[35mevaluating Epoch:  43%|#033[32m████▎     #033[0m| 12/28 [00:41<00:55,  3.46s/it]\u001B[0m\n",
      "\u001B[35mevaluating Epoch:  43%|#033[32m████▎     #033[0m| 12/28 [00:41<00:55,  3.46s/it]\u001B[0m\n",
      "\u001B[34mevaluating Epoch:  43%|#033[32m████▎     #033[0m| 12/28 [00:41<00:55,  3.47s/it]\u001B[0m\n",
      "\u001B[34mevaluating Epoch:  43%|#033[32m████▎     #033[0m| 12/28 [00:41<00:55,  3.47s/it]#015evaluating Epoch:  43%|#033[32m████▎     #033[0m| 12/28 [00:41<00:55,  3.47s/it]\u001B[0m\n",
      "\u001B[34mevaluating Epoch:  43%|#033[32m████▎     #033[0m| 12/28 [00:41<00:55,  3.47s/it]\u001B[0m\n",
      "\u001B[35mevaluating Epoch:  46%|#033[32m████▋     #033[0m| 13/28 [00:45<00:51,  3.46s/it]\u001B[0m\n",
      "\u001B[35mevaluating Epoch:  46%|#033[32m████▋     #033[0m| 13/28 [00:45<00:51,  3.46s/it]\u001B[0m\n",
      "\u001B[35mevaluating Epoch:  46%|#033[32m████▋     #033[0m| 13/28 [00:45<00:51,  3.46s/it]\u001B[0m\n",
      "\u001B[35mevaluating Epoch:  46%|#033[32m████▋     #033[0m| 13/28 [00:45<00:51,  3.46s/it]\u001B[0m\n",
      "\u001B[34mevaluating Epoch:  46%|#033[32m████▋     #033[0m| 13/28 [00:45<00:52,  3.47s/it]\u001B[0m\n",
      "\u001B[34mevaluating Epoch:  46%|#033[32m████▋     #033[0m| 13/28 [00:45<00:52,  3.47s/it]\u001B[0m\n",
      "\u001B[34mevaluating Epoch:  46%|#033[32m████▋     #033[0m| 13/28 [00:45<00:52,  3.47s/it]\u001B[0m\n",
      "\u001B[34mevaluating Epoch:  46%|#033[32m████▋     #033[0m| 13/28 [00:45<00:52,  3.47s/it]\u001B[0m\n",
      "\u001B[35mevaluating Epoch:  50%|#033[32m█████     #033[0m| 14/28 [00:48<00:48,  3.46s/it]#015evaluating Epoch:  50%|#033[32m█████     #033[0m| 14/28 [00:48<00:48,  3.46s/it]\u001B[0m\n",
      "\u001B[35mevaluating Epoch:  50%|#033[32m█████     #033[0m| 14/28 [00:48<00:48,  3.46s/it]#015evaluating Epoch:  50%|#033[32m█████     #033[0m| 14/28 [00:48<00:48,  3.46s/it]\u001B[0m\n",
      "\u001B[34mevaluating Epoch:  50%|#033[32m█████     #033[0m| 14/28 [00:48<00:48,  3.47s/it]#015evaluating Epoch:  50%|#033[32m█████     #033[0m| 14/28 [00:48<00:48,  3.47s/it]\u001B[0m\n",
      "\u001B[34mevaluating Epoch:  50%|#033[32m█████     #033[0m| 14/28 [00:48<00:48,  3.47s/it]\u001B[0m\n",
      "\u001B[34mevaluating Epoch:  50%|#033[32m█████     #033[0m| 14/28 [00:48<00:48,  3.47s/it]\u001B[0m\n",
      "\u001B[35mevaluating Epoch:  54%|#033[32m█████▎    #033[0m| 15/28 [00:51<00:44,  3.46s/it]\u001B[0m\n",
      "\u001B[35mevaluating Epoch:  54%|#033[32m█████▎    #033[0m| 15/28 [00:51<00:44,  3.46s/it]\u001B[0m\n",
      "\u001B[35mevaluating Epoch:  54%|#033[32m█████▎    #033[0m| 15/28 [00:51<00:44,  3.46s/it]\u001B[0m\n",
      "\u001B[35mevaluating Epoch:  54%|#033[32m█████▎    #033[0m| 15/28 [00:51<00:44,  3.46s/it]\u001B[0m\n",
      "\u001B[34mevaluating Epoch:  54%|#033[32m█████▎    #033[0m| 15/28 [00:52<00:45,  3.47s/it]#015evaluating Epoch:  54%|#033[32m█████▎    #033[0m| 15/28 [00:52<00:45,  3.47s/it]\u001B[0m\n",
      "\u001B[34mevaluating Epoch:  54%|#033[32m█████▎    #033[0m| 15/28 [00:52<00:45,  3.47s/it]\u001B[0m\n",
      "\u001B[34mevaluating Epoch:  54%|#033[32m█████▎    #033[0m| 15/28 [00:52<00:45,  3.47s/it]\u001B[0m\n",
      "\u001B[35mevaluating Epoch:  57%|#033[32m█████▋    #033[0m| 16/28 [00:55<00:41,  3.46s/it]\u001B[0m\n",
      "\u001B[35mevaluating Epoch:  57%|#033[32m█████▋    #033[0m| 16/28 [00:55<00:41,  3.46s/it]\u001B[0m\n",
      "\u001B[35mevaluating Epoch:  57%|#033[32m█████▋    #033[0m| 16/28 [00:55<00:41,  3.46s/it]\u001B[0m\n",
      "\u001B[35mevaluating Epoch:  57%|#033[32m█████▋    #033[0m| 16/28 [00:55<00:41,  3.46s/it]\u001B[0m\n",
      "\u001B[34mevaluating Epoch:  57%|#033[32m█████▋    #033[0m| 16/28 [00:55<00:41,  3.47s/it]#015evaluating Epoch:  57%|#033[32m█████▋    #033[0m| 16/28 [00:55<00:41,  3.47s/it]\u001B[0m\n",
      "\u001B[34mevaluating Epoch:  57%|#033[32m█████▋    #033[0m| 16/28 [00:55<00:41,  3.47s/it]\u001B[0m\n",
      "\u001B[34mevaluating Epoch:  57%|#033[32m█████▋    #033[0m| 16/28 [00:55<00:41,  3.47s/it]\u001B[0m\n",
      "\u001B[35mevaluating Epoch:  61%|#033[32m██████    #033[0m| 17/28 [00:58<00:38,  3.46s/it]\u001B[0m\n",
      "\u001B[35mevaluating Epoch:  61%|#033[32m██████    #033[0m| 17/28 [00:58<00:38,  3.46s/it]\u001B[0m\n",
      "\u001B[35mevaluating Epoch:  61%|#033[32m██████    #033[0m| 17/28 [00:58<00:38,  3.46s/it]\u001B[0m\n",
      "\u001B[35mevaluating Epoch:  61%|#033[32m██████    #033[0m| 17/28 [00:58<00:38,  3.46s/it]\u001B[0m\n",
      "\u001B[34mevaluating Epoch:  61%|#033[32m██████    #033[0m| 17/28 [00:59<00:38,  3.47s/it]\u001B[0m\n",
      "\u001B[34mevaluating Epoch:  61%|#033[32m██████    #033[0m| 17/28 [00:59<00:38,  3.47s/it]\u001B[0m\n",
      "\u001B[34mevaluating Epoch:  61%|#033[32m██████    #033[0m| 17/28 [00:59<00:38,  3.47s/it]\u001B[0m\n",
      "\u001B[34mevaluating Epoch:  61%|#033[32m██████    #033[0m| 17/28 [00:59<00:38,  3.47s/it]\u001B[0m\n",
      "\u001B[35mevaluating Epoch:  64%|#033[32m██████▍   #033[0m| 18/28 [01:02<00:34,  3.46s/it]#015evaluating Epoch:  64%|#033[32m██████▍   #033[0m| 18/28 [01:02<00:34,  3.46s/it]\u001B[0m\n",
      "\u001B[35mevaluating Epoch:  64%|#033[32m██████▍   #033[0m| 18/28 [01:02<00:34,  3.46s/it]#015evaluating Epoch:  64%|#033[32m██████▍   #033[0m| 18/28 [01:02<00:34,  3.46s/it]\u001B[0m\n",
      "\u001B[34mevaluating Epoch:  64%|#033[32m██████▍   #033[0m| 18/28 [01:02<00:34,  3.47s/it]\u001B[0m\n",
      "\u001B[34mevaluating Epoch:  64%|#033[32m██████▍   #033[0m| 18/28 [01:02<00:34,  3.47s/it]\u001B[0m\n",
      "\u001B[34mevaluating Epoch:  64%|#033[32m██████▍   #033[0m| 18/28 [01:02<00:34,  3.47s/it]\u001B[0m\n",
      "\u001B[34mevaluating Epoch:  64%|#033[32m██████▍   #033[0m| 18/28 [01:02<00:34,  3.47s/it]\u001B[0m\n",
      "\u001B[35mevaluating Epoch:  68%|#033[32m██████▊   #033[0m| 19/28 [01:05<00:31,  3.46s/it]\u001B[0m\n",
      "\u001B[35mevaluating Epoch:  68%|#033[32m██████▊   #033[0m| 19/28 [01:05<00:31,  3.46s/it]\u001B[0m\n",
      "\u001B[35mevaluating Epoch:  68%|#033[32m██████▊   #033[0m| 19/28 [01:05<00:31,  3.46s/it]\u001B[0m\n",
      "\u001B[35mevaluating Epoch:  68%|#033[32m██████▊   #033[0m| 19/28 [01:05<00:31,  3.46s/it]\u001B[0m\n",
      "\u001B[34mevaluating Epoch:  68%|#033[32m██████▊   #033[0m| 19/28 [01:06<00:31,  3.47s/it]\u001B[0m\n",
      "\u001B[34mevaluating Epoch:  68%|#033[32m██████▊   #033[0m| 19/28 [01:06<00:31,  3.47s/it]#015evaluating Epoch:  68%|#033[32m██████▊   #033[0m| 19/28 [01:06<00:31,  3.47s/it]\u001B[0m\n",
      "\u001B[34mevaluating Epoch:  68%|#033[32m██████▊   #033[0m| 19/28 [01:06<00:31,  3.47s/it]\u001B[0m\n",
      "\u001B[34mevaluating Epoch:  71%|#033[32m███████▏  #033[0m| 20/28 [01:09<00:27,  3.47s/it]\u001B[0m\n",
      "\u001B[34mevaluating Epoch:  71%|#033[32m███████▏  #033[0m| 20/28 [01:09<00:27,  3.47s/it]\u001B[0m\n",
      "\u001B[34mevaluating Epoch:  71%|#033[32m███████▏  #033[0m| 20/28 [01:09<00:27,  3.47s/it]#015evaluating Epoch:  71%|#033[32m███████▏  #033[0m| 20/28 [01:09<00:27,  3.47s/it]\u001B[0m\n",
      "\u001B[35mevaluating Epoch:  71%|#033[32m███████▏  #033[0m| 20/28 [01:09<00:27,  3.46s/it]#015evaluating Epoch:  71%|#033[32m███████▏  #033[0m| 20/28 [01:09<00:27,  3.46s/it]\u001B[0m\n",
      "\u001B[35mevaluating Epoch:  71%|#033[32m███████▏  #033[0m| 20/28 [01:09<00:27,  3.46s/it]#015evaluating Epoch:  71%|#033[32m███████▏  #033[0m| 20/28 [01:09<00:27,  3.46s/it]\u001B[0m\n",
      "\u001B[35mevaluating Epoch:  75%|#033[32m███████▌  #033[0m| 21/28 [01:12<00:24,  3.46s/it]\u001B[0m\n",
      "\u001B[35mevaluating Epoch:  75%|#033[32m███████▌  #033[0m| 21/28 [01:12<00:24,  3.46s/it]\u001B[0m\n",
      "\u001B[35mevaluating Epoch:  75%|#033[32m███████▌  #033[0m| 21/28 [01:12<00:24,  3.46s/it]\u001B[0m\n",
      "\u001B[35mevaluating Epoch:  75%|#033[32m███████▌  #033[0m| 21/28 [01:12<00:24,  3.46s/it]\u001B[0m\n",
      "\u001B[34mevaluating Epoch:  75%|#033[32m███████▌  #033[0m| 21/28 [01:12<00:24,  3.47s/it]\u001B[0m\n",
      "\u001B[34mevaluating Epoch:  75%|#033[32m███████▌  #033[0m| 21/28 [01:12<00:24,  3.47s/it]#015evaluating Epoch:  75%|#033[32m███████▌  #033[0m| 21/28 [01:12<00:24,  3.47s/it]\u001B[0m\n",
      "\u001B[34mevaluating Epoch:  75%|#033[32m███████▌  #033[0m| 21/28 [01:12<00:24,  3.47s/it]\u001B[0m\n",
      "\u001B[35mevaluating Epoch:  79%|#033[32m███████▊  #033[0m| 22/28 [01:16<00:20,  3.46s/it]\u001B[0m\n",
      "\u001B[35mevaluating Epoch:  79%|#033[32m███████▊  #033[0m| 22/28 [01:16<00:20,  3.46s/it]\u001B[0m\n",
      "\u001B[35mevaluating Epoch:  79%|#033[32m███████▊  #033[0m| 22/28 [01:16<00:20,  3.46s/it]#015evaluating Epoch:  79%|#033[32m███████▊  #033[0m| 22/28 [01:16<00:20,  3.46s/it]\u001B[0m\n",
      "\u001B[34mevaluating Epoch:  79%|#033[32m███████▊  #033[0m| 22/28 [01:16<00:20,  3.47s/it]\u001B[0m\n",
      "\u001B[34mevaluating Epoch:  79%|#033[32m███████▊  #033[0m| 22/28 [01:16<00:20,  3.47s/it]\u001B[0m\n",
      "\u001B[34mevaluating Epoch:  79%|#033[32m███████▊  #033[0m| 22/28 [01:16<00:20,  3.47s/it]\u001B[0m\n",
      "\u001B[34mevaluating Epoch:  79%|#033[32m███████▊  #033[0m| 22/28 [01:16<00:20,  3.47s/it]\u001B[0m\n",
      "\u001B[35mevaluating Epoch:  82%|#033[32m████████▏ #033[0m| 23/28 [01:19<00:17,  3.46s/it]\u001B[0m\n",
      "\u001B[35mevaluating Epoch:  82%|#033[32m████████▏ #033[0m| 23/28 [01:19<00:17,  3.46s/it]\u001B[0m\n",
      "\u001B[35mevaluating Epoch:  82%|#033[32m████████▏ #033[0m| 23/28 [01:19<00:17,  3.46s/it]\u001B[0m\n",
      "\u001B[35mevaluating Epoch:  82%|#033[32m████████▏ #033[0m| 23/28 [01:19<00:17,  3.46s/it]\u001B[0m\n",
      "\u001B[34mevaluating Epoch:  82%|#033[32m████████▏ #033[0m| 23/28 [01:19<00:17,  3.47s/it]\u001B[0m\n",
      "\u001B[34mevaluating Epoch:  82%|#033[32m████████▏ #033[0m| 23/28 [01:19<00:17,  3.47s/it]\u001B[0m\n",
      "\u001B[34mevaluating Epoch:  82%|#033[32m████████▏ #033[0m| 23/28 [01:19<00:17,  3.47s/it]\u001B[0m\n",
      "\u001B[34mevaluating Epoch:  82%|#033[32m████████▏ #033[0m| 23/28 [01:19<00:17,  3.47s/it]\u001B[0m\n",
      "\u001B[35mevaluating Epoch:  86%|#033[32m████████▌ #033[0m| 24/28 [01:23<00:13,  3.46s/it]\u001B[0m\n",
      "\u001B[35mevaluating Epoch:  86%|#033[32m████████▌ #033[0m| 24/28 [01:23<00:13,  3.46s/it]\u001B[0m\n",
      "\u001B[35mevaluating Epoch:  86%|#033[32m████████▌ #033[0m| 24/28 [01:23<00:13,  3.46s/it]#015evaluating Epoch:  86%|#033[32m████████▌ #033[0m| 24/28 [01:23<00:13,  3.46s/it]\u001B[0m\n",
      "\u001B[34mevaluating Epoch:  86%|#033[32m████████▌ #033[0m| 24/28 [01:23<00:13,  3.47s/it]\u001B[0m\n",
      "\u001B[34mevaluating Epoch:  86%|#033[32m████████▌ #033[0m| 24/28 [01:23<00:13,  3.47s/it]\u001B[0m\n",
      "\u001B[34mevaluating Epoch:  86%|#033[32m████████▌ #033[0m| 24/28 [01:23<00:13,  3.47s/it]\u001B[0m\n",
      "\u001B[34mevaluating Epoch:  86%|#033[32m████████▌ #033[0m| 24/28 [01:23<00:13,  3.47s/it]\u001B[0m\n",
      "\u001B[35mevaluating Epoch:  89%|#033[32m████████▉ #033[0m| 25/28 [01:26<00:10,  3.46s/it]\u001B[0m\n",
      "\u001B[35mevaluating Epoch:  89%|#033[32m████████▉ #033[0m| 25/28 [01:26<00:10,  3.46s/it]\u001B[0m\n",
      "\u001B[35mevaluating Epoch:  89%|#033[32m████████▉ #033[0m| 25/28 [01:26<00:10,  3.46s/it]\u001B[0m\n",
      "\u001B[35mevaluating Epoch:  89%|#033[32m████████▉ #033[0m| 25/28 [01:26<00:10,  3.46s/it]\u001B[0m\n",
      "\u001B[34mevaluating Epoch:  89%|#033[32m████████▉ #033[0m| 25/28 [01:26<00:10,  3.47s/it]\u001B[0m\n",
      "\u001B[34mevaluating Epoch:  89%|#033[32m████████▉ #033[0m| 25/28 [01:26<00:10,  3.47s/it]\u001B[0m\n",
      "\u001B[34mevaluating Epoch:  89%|#033[32m████████▉ #033[0m| 25/28 [01:26<00:10,  3.47s/it]#015evaluating Epoch:  89%|#033[32m████████▉ #033[0m| 25/28 [01:26<00:10,  3.47s/it]\u001B[0m\n",
      "\u001B[35mevaluating Epoch:  93%|#033[32m█████████▎#033[0m| 26/28 [01:30<00:06,  3.46s/it]\u001B[0m\n",
      "\u001B[35mevaluating Epoch:  93%|#033[32m█████████▎#033[0m| 26/28 [01:30<00:06,  3.46s/it]\u001B[0m\n",
      "\u001B[35mevaluating Epoch:  93%|#033[32m█████████▎#033[0m| 26/28 [01:30<00:06,  3.46s/it]\u001B[0m\n",
      "\u001B[35mevaluating Epoch:  93%|#033[32m█████████▎#033[0m| 26/28 [01:30<00:06,  3.46s/it]\u001B[0m\n",
      "\u001B[34mevaluating Epoch:  93%|#033[32m█████████▎#033[0m| 26/28 [01:30<00:06,  3.47s/it]\u001B[0m\n",
      "\u001B[34mevaluating Epoch:  93%|#033[32m█████████▎#033[0m| 26/28 [01:30<00:06,  3.47s/it]\u001B[0m\n",
      "\u001B[34mevaluating Epoch:  93%|#033[32m█████████▎#033[0m| 26/28 [01:30<00:06,  3.47s/it]\u001B[0m\n",
      "\u001B[34mevaluating Epoch:  93%|#033[32m█████████▎#033[0m| 26/28 [01:30<00:06,  3.47s/it]\u001B[0m\n",
      "\u001B[35mevaluating Epoch:  96%|#033[32m█████████▋#033[0m| 27/28 [01:33<00:03,  3.46s/it]\u001B[0m\n",
      "\u001B[35mevaluating Epoch:  96%|#033[32m█████████▋#033[0m| 27/28 [01:33<00:03,  3.46s/it]#015evaluating Epoch:  96%|#033[32m█████████▋#033[0m| 27/28 [01:33<00:03,  3.46s/it]\u001B[0m\n",
      "\u001B[35mevaluating Epoch:  96%|#033[32m█████████▋#033[0m| 27/28 [01:33<00:03,  3.46s/it]\u001B[0m\n",
      "\u001B[34mevaluating Epoch:  96%|#033[32m█████████▋#033[0m| 27/28 [01:33<00:03,  3.47s/it]\u001B[0m\n",
      "\u001B[34mevaluating Epoch:  96%|#033[32m█████████▋#033[0m| 27/28 [01:33<00:03,  3.47s/it]\u001B[0m\n",
      "\u001B[34mevaluating Epoch:  96%|#033[32m█████████▋#033[0m| 27/28 [01:33<00:03,  3.47s/it]\u001B[0m\n",
      "\u001B[34mevaluating Epoch:  96%|#033[32m█████████▋#033[0m| 27/28 [01:33<00:03,  3.47s/it]\u001B[0m\n",
      "\u001B[35mevaluating Epoch: 100%|#033[32m██████████#033[0m| 28/28 [01:37<00:00,  3.46s/it]\u001B[0m\n",
      "\u001B[35mevaluating Epoch: 100%|#033[32m██████████#033[0m| 28/28 [01:37<00:00,  3.46s/it]\u001B[0m\n",
      "\u001B[35mevaluating Epoch: 100%|#033[32m██████████#033[0m| 28/28 [01:37<00:00,  3.46s/it]\u001B[0m\n",
      "\u001B[35mevaluating Epoch: 100%|#033[32m██████████#033[0m| 28/28 [01:37<00:00,  3.46s/it]\u001B[0m\n",
      "\u001B[35mevaluating Epoch: 100%|#033[32m██████████#033[0m| 28/28 [01:37<00:00,  3.46s/it]\u001B[0m\n",
      "\u001B[35mevaluating Epoch: 100%|#033[32m██████████#033[0m| 28/28 [01:37<00:00,  3.46s/it]\u001B[0m\n",
      "\u001B[35mevaluating Epoch: 100%|#033[32m██████████#033[0m| 28/28 [01:37<00:00,  3.46s/it]\u001B[0m\n",
      "\u001B[35mevaluating Epoch: 100%|#033[32m██████████#033[0m| 28/28 [01:37<00:00,  3.46s/it]\u001B[0m\n",
      "\u001B[35meval_ppl=tensor(3.5329, device='cuda:0') eval_epoch_loss=tensor(1.2621, device='cuda:0')\u001B[0m\n",
      "\u001B[35mwe are about to save the PEFT modules\u001B[0m\n",
      "\u001B[35mPEFT modules are saved in saved_peft_model directory\u001B[0m\n",
      "\u001B[35mbest eval loss on epoch 4 is 1.2621121406555176\u001B[0m\n",
      "\u001B[35mEpoch 5: train_perplexity=3.4810, train_epoch_loss=1.2473, epcoh time 265.53094474299996s\u001B[0m\n",
      "\u001B[35mINFO:root:Key: avg_train_prep, Value: 3.6892006397247314\u001B[0m\n",
      "\u001B[35mINFO:root:Key: avg_train_loss, Value: 1.3033479452133179\u001B[0m\n",
      "\u001B[35mINFO:root:Key: avg_eval_prep, Value: 3.592533588409424\u001B[0m\n",
      "\u001B[35mINFO:root:Key: avg_eval_loss, Value: 1.2786312103271484\u001B[0m\n",
      "\u001B[35mINFO:root:Key: avg_epoch_time, Value: 266.70217377520004\u001B[0m\n",
      "\u001B[35mINFO:root:Key: avg_checkpoint_time, Value: 3.5263832425999224\u001B[0m\n",
      "\u001B[34mevaluating Epoch: 100%|#033[32m██████████#033[0m| 28/28 [01:37<00:00,  3.47s/it]#015evaluating Epoch: 100%|#033[32m██████████#033[0m| 28/28 [01:37<00:00,  3.47s/it]\u001B[0m\n",
      "\u001B[34mevaluating Epoch: 100%|#033[32m██████████#033[0m| 28/28 [01:37<00:00,  3.47s/it]\u001B[0m\n",
      "\u001B[34mevaluating Epoch: 100%|#033[32m██████████#033[0m| 28/28 [01:37<00:00,  3.47s/it]\u001B[0m\n",
      "\u001B[34m#015evaluating Epoch: 100%|#033[32m██████████#033[0m| 28/28 [01:37<00:00,  3.47s/it]\u001B[0m\n",
      "\u001B[34mevaluating Epoch: 100%|#033[32m██████████#033[0m| 28/28 [01:37<00:00,  3.47s/it]\u001B[0m\n",
      "\u001B[34mevaluating Epoch: 100%|#033[32m██████████#033[0m| 28/28 [01:37<00:00,  3.47s/it]\u001B[0m\n",
      "\u001B[34mevaluating Epoch: 100%|#033[32m██████████#033[0m| 28/28 [01:37<00:00,  3.47s/it]\u001B[0m\n",
      "\u001B[34meval_ppl=tensor(3.5329, device='cuda:0') eval_epoch_loss=tensor(1.2621, device='cuda:0')\u001B[0m\n",
      "\u001B[34mwe are about to save the PEFT modules\u001B[0m\n",
      "\u001B[34mPEFT modules are saved in saved_peft_model directory\u001B[0m\n",
      "\u001B[34mbest eval loss on epoch 4 is 1.2621121406555176\u001B[0m\n",
      "\u001B[34mEpoch 5: train_perplexity=3.4810, train_epoch_loss=1.2473, epcoh time 265.9898576360001s\u001B[0m\n",
      "\u001B[34mINFO:root:Key: avg_train_prep, Value: 3.6892006397247314\u001B[0m\n",
      "\u001B[34mINFO:root:Key: avg_train_loss, Value: 1.3033479452133179\u001B[0m\n",
      "\u001B[34mINFO:root:Key: avg_eval_prep, Value: 3.592533588409424\u001B[0m\n",
      "\u001B[34mINFO:root:Key: avg_eval_loss, Value: 1.2786312103271484\u001B[0m\n",
      "\u001B[34mINFO:root:Key: avg_epoch_time, Value: 266.0087538062\u001B[0m\n",
      "\u001B[34mINFO:root:Key: avg_checkpoint_time, Value: 3.4896333988001063\u001B[0m\n",
      "\u001B[35mINFO:root:Combining pre-trained base model with the PEFT adapter module.\u001B[0m\n",
      "\u001B[34mINFO:root:Combining pre-trained base model with the PEFT adapter module.\u001B[0m\n",
      "\u001B[35mLoading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]\u001B[0m\n",
      "\u001B[35mLoading checkpoint shards:  50%|█████     | 1/2 [00:00<00:00,  1.61it/s]\u001B[0m\n",
      "\u001B[35mLoading checkpoint shards: 100%|██████████| 2/2 [00:00<00:00,  2.37it/s]\u001B[0m\n",
      "\u001B[35mLoading checkpoint shards: 100%|██████████| 2/2 [00:00<00:00,  2.21it/s]\u001B[0m\n",
      "\u001B[34mLoading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]\u001B[0m\n",
      "\u001B[34mLoading checkpoint shards:  50%|█████     | 1/2 [00:00<00:00,  1.58it/s]\u001B[0m\n",
      "\u001B[34mLoading checkpoint shards: 100%|██████████| 2/2 [00:00<00:00,  2.33it/s]\u001B[0m\n",
      "\u001B[34mLoading checkpoint shards: 100%|██████████| 2/2 [00:00<00:00,  2.18it/s]\u001B[0m\n",
      "\u001B[35mINFO:root:Saving the combined model in safetensors format.\u001B[0m\n",
      "\u001B[34mINFO:root:Saving the combined model in safetensors format.\u001B[0m\n",
      "\u001B[35mINFO:root:Saving complete.\u001B[0m\n",
      "\u001B[35mINFO:root:Copying tokenizer to the output directory.\u001B[0m\n",
      "\u001B[35mINFO:root:Putting inference code with the fine-tuned model directory.\u001B[0m\n",
      "\u001B[35m2024-01-01 21:53:15,453 sagemaker-training-toolkit INFO     Waiting for the process to finish and give a return code.\u001B[0m\n",
      "\u001B[35m2024-01-01 21:53:15,453 sagemaker-training-toolkit INFO     Done waiting for a return code. Received 0 from exiting process.\u001B[0m\n",
      "\u001B[35m2024-01-01 21:53:15,453 sagemaker-training-toolkit INFO     Reporting training SUCCESS\u001B[0m\n",
      "\u001B[34mINFO:root:Saving complete.\u001B[0m\n",
      "\u001B[34mINFO:root:Copying tokenizer to the output directory.\u001B[0m\n",
      "\u001B[34mINFO:root:Putting inference code with the fine-tuned model directory.\u001B[0m\n",
      "\u001B[34m2024-01-01 21:53:18,735 sagemaker-training-toolkit INFO     Waiting for the process to finish and give a return code.\u001B[0m\n",
      "\u001B[34m2024-01-01 21:53:18,735 sagemaker-training-toolkit INFO     Done waiting for a return code. Received 0 from exiting process.\u001B[0m\n",
      "\u001B[34m2024-01-01 21:53:18,735 sagemaker-training-toolkit INFO     Reporting training SUCCESS\u001B[0m\n",
      "\n",
      "2024-01-01 21:53:25 Uploading - Uploading generated training model\n",
      "2024-01-01 21:59:11 Completed - Training job completed\n",
      "Training seconds: 5574\n",
      "Billable seconds: 5574\n"
     ]
    }
   ],
   "source": [
    "from sagemaker.jumpstart.estimator import JumpStartEstimator\n",
    "\n",
    "estimator = JumpStartEstimator(\n",
    "    model_id=model_id,\n",
    "    model_version=model_version,\n",
    "    instance_type=\"ml.g5.12xlarge\",\n",
    "    instance_count=2,\n",
    "    environment={\"accept_eula\": \"true\"}\n",
    ")\n",
    "\n",
    "# 기본적으로 명령어 기반 조정은 비활성화 되어있습니다. 따라서 명령어 기반 조정 데이터 세트를 사용하려면\n",
    "estimator.set_hyperparameters(instruction_tuned=\"True\", \n",
    "                              epoch=\"5\", \n",
    "                              max_input_length=\"1024\")\n",
    "estimator.fit({\"training\": train_data_location})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e9decbf-08c6-4cb4-8644-4a96afb5bebf",
   "metadata": {
    "tags": []
   },
   "source": [
    "### 미세조정된 모델 배포하기\n",
    "\n",
    "---\n",
    "\n",
    "다음으로 미세 조정된 모델을 배포합니다. 이후 미세 조정된 모델과 사전 훈련된 모델의 성능을 비교합니다.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "016e591b-63f8-4e0f-941c-4b4e0b9dc6fc",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No instance type selected for inference hosting endpoint. Defaulting to ml.g5.2xlarge.\n",
      "INFO:sagemaker.jumpstart:No instance type selected for inference hosting endpoint. Defaulting to ml.g5.2xlarge.\n",
      "INFO:sagemaker:Creating model with name: meta-textgeneration-llama-2-7b-2024-01-01-22-07-44-444\n",
      "INFO:sagemaker:Creating endpoint-config with name meta-textgeneration-llama-2-7b-2024-01-01-22-07-44-442\n",
      "INFO:sagemaker:Creating endpoint with name meta-textgeneration-llama-2-7b-2024-01-01-22-07-44-442\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------!"
     ]
    }
   ],
   "source": [
    "finetuned_predictor = estimator.deploy()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb57904a-9631-45fe-bc3f-ae2fbb992960",
   "metadata": {
    "tags": []
   },
   "source": [
    "### 사전 훈련된 모델과 미세 조정된 모델 평가하기\n",
    "\n",
    "---\n",
    "\n",
    "다음으로 테스트 데이터를 사용하여 미세 조정된 모델의 성능을 평가하고 이를 사전 훈련된 모델과 비교합니다.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "87085bf6-dc7e-46f3-8563-d2e4aafd0820",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Inputs</th>\n",
       "      <th>Ground Truth</th>\n",
       "      <th>Response from non-finetuned model</th>\n",
       "      <th>Response from fine-tuned model</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Below is an instruction that describes a task, paired with an input that provides further context. Write a response that appropriately completes the request.\\n\\n### Instruction:\\nPlease describe  what is oil and give me a list of it’s applications.\\n\\n### Input:\\nAn oil is any nonpolar chemical substance that is composed primarily of hydrocarbons and is hydrophobic (does not mix with water) &amp; lipophilic (mixes with other oils). Oils are usually flammable and surface active. Most oils are unsaturated lipids that are liquid at room temperature.\\n\\nThe general definition of oil includes classes of chemical compounds that may be otherwise unrelated in structure, properties, and uses. Oils may be animal, vegetable, or petrochemical in origin, and may be volatile or non-volatile. They are used for food (e.g., olive oil), fuel (e.g., heating oil), medical purposes (e.g., mineral oil), lubrication (e.g. motor oil), and the manufacture of many types of paints, plastics, and other materials. Specially prepared oils are used in some religious ceremonies and rituals as purifying agents.\\n\\n\\n\\n### Response:\\n</td>\n",
       "      <td>An oil is a chemical substance that is composed primarily of hydrocarbons and may be animal, vegetable or petrochemical in origin.\\nOil is used in a wide range of applications and is essential to everyday human life. These are:\\nCooking - edible vegetable and animal oils are used for various purposes in cooking and food preparation\\nCosmetics - most facial cleansers, lotions and hair care products contain molecules that come from mineral and vegetable oils\\nFuel - crude oil is refined and converted to diesel, gasoline or jet fuel to power cars, trucks and planes\\nHeating - petrochemical oil is used for heating\\nPainting - oil is used as a supporting medium for paints\\nLubrication - oils are used in various engineering purposes as they do not easily adhere to other substance which makes them useful as lubricants\\nReligion - oil has been used throughout history as a religious medium. It is often considered a spiritually purifying agent and is used to anointing purposes\\nHealth - oils holds lots of fats and medical properties, for example fish oil holds the omega-3 fatty acid which helps with inflammation and reduces fat in the bloodstream</td>\n",
       "      <td>An oil is any nonpolar chemical substance that is composed primarily of hydrocarbons and is hydrophobic (does not mix with water) &amp; lipophilic (mixes with other oils). Oils are usually flammable and surface active. Most oils are unsaturated lipids that are liquid at room temperature.\\n\\nThe general definition of oil includes classes of chemical compounds that may be otherwise unrelated in structure, properties, and uses. O</td>\n",
       "      <td>Oil is made by a chemical reaction between coal and petroleum (oil).It is non-polar in nature and it is hydrophobic in nature. It is lipid of hydrocarbon compounds. It's liquid and light-colored in nature and is combustible.\\nOils are used for food, fuel for cars and machines like generators, oils are used by medical.\\nPaints and plastics are used in the</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Below is an instruction that describes a task, paired with an input that provides further context. Write a response that appropriately completes the request.\\n\\n### Instruction:\\nWhat is the Jones-Connally Act?\\n\\n### Input:\\nThe Jones–Connally Act was a New Deal Initiative passed by Congress in April 1934 as an extension to the Agricultural Adjustment Act. Largely in response to the great drought of 1933–1934, cattle ranchers acted against their former opposition to the commodification of cattle and appealed to the government for assistance in ridding of themselves of the millions of cattle they could no longer afford to feed or to keep alive without a loss on return.\\n\\n\\n\\n### Response:\\n</td>\n",
       "      <td>The Jones–Connally Act was passed by the US Congress in April 1934.  It was an extension to the Agricultural Adjustment Act.  It was part of the New Deal and was in response to the drought of 1933-1934.  It made cattle a basic commodity giving the government authority over the distribution and processing of the cattle for public relief purposes.</td>\n",
       "      <td>\\nThe 1934 Act (Public Law 49-1254, 83 Stat-189) was an extension to the Agricultural Adjustment Act signed into law on April 27, 1934 by then sitting President Franklin D. Roosevelt.\\n\\n### Credits:\\n- Input provided by an automated speech-to-text system with editing by hand to ensure clarity, accuracy, and context</td>\n",
       "      <td>The Jones-Connally Act was a New Deal Initiative passed by Congress in April 1934 as an extension to the Agricultural Adjustment Act. Largely in response to the great drought of 1933-1934, cattle ranchers acted against their former opposition to the commodification of cattle and appealed to the government for assistance in ridding of themselves of the millions of cattle they could no longer afford to</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Below is an instruction that describes a task, paired with an input that provides further context. Write a response that appropriately completes the request.\\n\\n### Instruction:\\nWhat the five love languages?\\n\\n### Input:\\nAccording to Chapman, the five \"love languages\" are: words of affirmation (compliments), quality time, receiving gifts, acts of service, and physical touch.\\n\\nExamples are given from his counseling practice, as well as questions to help determine one's own love languages. According to Chapman's theory, each person has one primary and one secondary love language.\\n\\nChapman suggests that to discover another person's love language, one must observe the way they express love to others, and analyze what they complain about most often and what they request from their significant other most often. He theorizes that people tend to naturally give love in the way that they prefer to receive love, and better communication between couples can be accomplished when one can demonstrate caring to the other person in the love language the recipient understands.\\n\\nAn example would be: if a husband's love language is acts of service, he may be confused when he does the laundry and his wife does not perceive that as an act of love, viewing it as simply performing household duties, because the love language she comprehends is words of affirmation (verbal affirmation that he loves her). She may try to use what she values, words of affirmation, to express her love to him, which he would not value as much as she does. If she understands his love language and mows the lawn for him, he perceives it in his love language as an act of expressing her love for him; likewise, if he tells her he loves her, she values that as an act of love.\\n\\n\\n\\n### Response:\\n</td>\n",
       "      <td>The five love languages include words of affirmation, quality time, receiving gifts, acts of service, and physical touch.</td>\n",
       "      <td>The 5 Love Languages\\n\\n</td>\n",
       "      <td>There are five types of love languages to express love that are,\\n1 Words of affirmation\\n2 quality time\\n3 Receiving gifts\\n4 Acts of service\\n5 Physical Touch</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Below is an instruction that describes a task, paired with an input that provides further context. Write a response that appropriately completes the request.\\n\\n### Instruction:\\nWithout quoting directly from the text, give me a summary of the Voyager 1 space mission\\n\\n### Input:\\nVoyager 1 is a space probe launched by NASA on September 5, 1977, as part of the Voyager program to study the outer Solar System and interstellar space beyond the Sun's heliosphere. Launched 16 days after its twin Voyager 2, Voyager 1 has been operating for 45 years, 7 months and 1 day as of April 6, 2023 UTC . It communicates through NASA's Deep Space Network to receive routine commands and to transmit data to Earth. Real-time distance and velocity data is provided by NASA and JPL. At a distance of 159.20 AU (23.816 billion km; 14.799 billion mi) from Earth as of March 27, 2023, it is the most distant human-made object from Earth.\\n\\nThe probe made flybys of Jupiter, Saturn, and Saturn's largest moon, Titan. NASA had a choice of either doing a Pluto or Titan flyby; exploration of the moon took priority because it was known to have a substantial atmosphere. Voyager 1 studied the weather, magnetic fields, and rings of the two gas giants and was the first probe to provide detailed images of their moons.\\n\\nAs part of the Voyager program and like its sister craft Voyager 2, the spacecraft's extended mission is to locate and study the regions and boundaries of the outer heliosphere and to begin exploring the interstellar medium. Voyager 1 crossed the heliopause and entered interstellar space on August 25, 2012, making it the first spacecraft to do so. Two years later, Voyager 1 began experiencing a third \"tsunami wave\" of coronal mass ejections from the Sun that continued to at least December 15, 2014, further confirming that the probe is indeed in interstellar space.\\n\\nIn a further testament to the robustness of Voyager 1, the Voyager team tested the spacecraft's trajectory correction maneuver (TCM) thrusters in late 2017 (the first time these thrusters had been fired since 1980), a project enabling the mission to be extended by two to three years. Voyager 1's extended mission is expected to continue until about 2025, when its radioisotope thermoelectric generators (RTGs) will no longer supply enough electric power to operate its scientific instruments.\\n\\n\\n\\n### Response:\\n</td>\n",
       "      <td>The Voyager 1 space mission began on September 5, 1977 when the probe was launched with mission parameters to explore out solar system, planets, and outer solar system beyond the sun.  The mission is currently in it's 45th year and has provided significant learning about the atmosphere of planets like Jupiter and Saturn, while continuing to scientific data on regions of space never before encountered.</td>\n",
       "      <td>\\nIt has been on an unforgettable journey. It was launched on September 5, 1977 and explored Jupiter and Saturn. It was the first spacecraft to reach interstellar space\\n</td>\n",
       "      <td>Voyager 1 is a space probe launched by NASA on September 5, 1977, as part of the Voyager program to study the outer Solar System and interstellar space beyond the Sun's heliosphere. It launched 16 days after its twin, Voyager 2. It has been operating for 43 years, 7 months and 1 day as of April 6, 2023 UTC.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Below is an instruction that describes a task, paired with an input that provides further context. Write a response that appropriately completes the request.\\n\\n### Instruction:\\nGive me a summary of the Naked Brothers Band.\\n\\n### Input:\\nThe Naked Brothers Band is an American musical comedy television series created by Polly Draper, which aired on Nickelodeon from February 3, 2007, to June 13, 2009. It depicts the daily lives of Draper's sons, who lead a faux world-renowned children's rock band in New York City. As a mockumentary, the storyline is an embellished satire of their real lives, and the fictional presence of a camera is often acknowledged. The show stars Nat Wolff and Alex Wolff, the lead singer-songwriter and drummer, respectively. Nat's fictional female interest (Allie DiMeco) and real-life friends Thomas Batuello, David Levi, and Cooper Pillot, as well as Qaasim Middleton—who has no prior acquaintance with the family—are featured as the other band members, with Draper's jazz musician husband Michael Wolff as his sons' widowed accordion-playing dad and her niece Jesse Draper portraying the group's babysitter.\\n\\n\\n\\n### Response:\\n</td>\n",
       "      <td>The Naked Brother Bands is a TV show about the lives of Draper's sons. The storyline is a satirical version of their real lives and was aired on Nickelodeon from 2007 to 2009.</td>\n",
       "      <td>Nat Wolff and Alex Wolff are from New York City. They used to play music together when they were young. They became famous when they were kids. They are now adults and they have a band called Naked Brothers Band.\\n\\n\\n\\n### Instruction:\\nHow many different letters appear in the word SAT?\\n\\n### Input:\\nSAT\\n\\n### Input:\\n\\n### Instruction:\\nGive a review of the</td>\n",
       "      <td>The Naked Brothers Band is an American musical comedy television series created by Polly Draper, which aired on Nickelodeon from February 3, 2007, to June 13, 2009. It depicts the daily lives of Draper's sons, who lead a faux world-renowned children's rock band in New York City. As a mockumentary, the storyline is an embellished satire of</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from IPython.display import display, HTML\n",
    "\n",
    "test_dataset = train_and_test_dataset[\"test\"]\n",
    "\n",
    "inputs, ground_truth_responses, responses_before_finetuning, responses_after_finetuning = (\n",
    "    [],\n",
    "    [],\n",
    "    [],\n",
    "    [],\n",
    ")\n",
    "\n",
    "def predict_and_print(datapoint):\n",
    "    # 명령어 기반 미세 조정을 위해 입력과 출력 사이에 특별한 키를 삽입합니다.\n",
    "    input_output_demarkation_key = \"\\n\\n### Response:\\n\"\n",
    "\n",
    "    prompt = f'Below is an instruction that describes a task, paired with an input that provides further context. Write a response that appropriately completes the request.\\n\\n### Instruction:\\n{datapoint[\"instruction\"]}\\n\\n### Input:\\n{datapoint[\"context\"]}\\n\\n',\n",
    "    \n",
    "    payload = {\n",
    "        \"inputs\": prompt[0] + input_output_demarkation_key,\n",
    "        \"parameters\": {\"max_new_tokens\": 100},\n",
    "    }\n",
    "    inputs.append(payload[\"inputs\"])\n",
    "    ground_truth_responses.append(datapoint[\"response\"])\n",
    "\n",
    "    pretrained_response = pretrained_predictor.predict(\n",
    "        payload, custom_attributes=\"accept_eula=true\"\n",
    "    )\n",
    "    responses_before_finetuning.append(pretrained_response[0][\"generation\"])\n",
    "\n",
    "    finetuned_response = finetuned_predictor.predict(payload, custom_attributes=\"accept_eula=true\")\n",
    "    responses_after_finetuning.append(finetuned_response[0][\"generation\"])\n",
    "\n",
    "\n",
    "try:\n",
    "    for i, datapoint in enumerate(test_dataset.select(range(5))):\n",
    "        predict_and_print(datapoint)\n",
    "\n",
    "    df = pd.DataFrame(\n",
    "        {\n",
    "            \"Inputs\": inputs,\n",
    "            \"Ground Truth\": ground_truth_responses,\n",
    "            \"Response from non-finetuned model\": responses_before_finetuning,\n",
    "            \"Response from fine-tuned model\": responses_after_finetuning,\n",
    "        }\n",
    "    )\n",
    "    display(HTML(df.to_html()))\n",
    "except Exception as e:\n",
    "    print(e)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6b0a0f5-ef34-40db-8ab7-c24a5d14b525",
   "metadata": {},
   "source": "### 리소스 정리하기"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d73ab2da-d00f-46db-90eb-81812898653b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # 리소스 삭제\n",
    "# pretrained_predictor.delete_model()\n",
    "# pretrained_predictor.delete_endpoint()\n",
    "# finetuned_predictor.delete_model()\n",
    "# finetuned_predictor.delete_endpoint()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "759ce98f-a35a-4c64-9fae-50894b5e9f37",
   "metadata": {
    "tags": []
   },
   "source": "# 부록"
  },
  {
   "cell_type": "markdown",
   "id": "7d1c8c86-bfe2-4828-a7aa-dbd7a5ee075f",
   "metadata": {
    "tags": []
   },
   "source": [
    "### 지원되는 추론 매개변수\n",
    "\n",
    "---\n",
    "이 모델은 다음과 같은 추론 페이로드 매개변수를 지원합니다:\n",
    "\n",
    "* **max_new_tokens:** 모델은 출력 길이(입력 컨텍스트 길이를 제외한)가 max_new_tokens에 도달할 때까지 텍스트를 생성합니다. 이 값은 반드시 양의 정수여야 합니다.\n",
    "* **temperature:** 출력의 무작위성을 조절합니다. 높은 temperature 값은 낮은 확률의 단어를 포함한 출력을, 낮은 temperature 값은 높은 확률의 단어를 포함한 출력을 생성합니다. `temperature`가 0이면 탐욕적 디코딩(greedy decoding)이 수행됩니다. 이 값은 반드시 양의 실수여야 합니다.\n",
    "* **top_p:** 텍스트 생성의 각 단계에서 누적 확률 `top_p`에 해당하는 가장 작은 집합의 단어들 중에서 샘플링합니다. 이 값은 0과 1 사이의 실수여야 합니다.\n",
    "* **return_full_text:** True로 설정하면, 입력 텍스트가 생성된 출력 텍스트의 일부가 됩니다. 이 값은 반드시 불(boolean)이어야 하며, 기본값은 False입니다.\n",
    " \n",
    "엔드포인트를 호출할 때 위에 언급된 매개변수의 하위 집합을 지정할 수 있습니다.\n",
    "\n",
    "\n",
    "### 참고 사항\n",
    "- `max_new_tokens`가 정의되지 않은 경우, 모델은 최대 4,000개의 전체 토큰까지 생성할 수 있습니다. 이 경우 엔드포인트 쿼리 시간 초과 오류가 발생할 수 있으므로, 가능한 경우 `max_new_tokens`를 설정하는 것이 좋습니다. 7B, 13B, 70B 모델에 대해서는 각각 `max_new_tokens`를 최대 1500, 1000, 500 이하로 설정하고, 총 토큰 수를 4,000개 이하로 유지하는 것을 권장합니다.\n",
    "- 이 모델은 4,000개의 컨텍스트 길이를 지원하기 위해 배치 크기를 1로 제한하고 있습니다. 더 큰 배치 크기를 사용하는 페이로드는 추론 전에 엔드포인트 오류가 발생합니다.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19f7e4f8-970f-4a1d-b6ee-86bc77b8b9a9",
   "metadata": {},
   "source": [
    "### 미세 조정을 위해 지원되는 하이퍼파라미터\n",
    "\n",
    "---\n",
    "\n",
    "- epoch: 미세 조정 알고리즘이 훈련 데이터 세트를 통과하는 횟수입니다. 1보다 큰 정수여야 합니다. 기본값: 5\n",
    "- learning_rate: 각 배치의 훈련 예제를 처리한 후 모델 가중치가 업데이트되는 속도입니다. 0보다 큰 양의 실수여야 합니다. 기본값: 1e-4\n",
    "- instruction_tuned: 모델을 명령어로 훈련할지 여부입니다. 'True' 또는 'False' 이어야 합니다. 기본값: 'False'\n",
    "- per_device_train_batch_size: 훈련을 위한 GPU 코어/CPU 당 배치 크기입니다. 양의 정수여야 합니다. 기본값: 4\n",
    "- per_device_eval_batch_size: 평가를 위한 GPU 코어/CPU당 배치 크기입니다. 양의 정수여야 합니다. 기본값: 1\n",
    "- max_train_samples: 디버깅 목적 또는 더 빠른 훈련을 위해 훈련 예제의 수를 이 값으로 줄입니다. -1은 모든 훈련 샘플을 사용하는 것을 의미합니다. 양의 정수 또는 -1이어야 합니다. 기본값: -1 \n",
    "- max_val_samples: 디버깅 목적 또는 더 빠른 훈련을 위해 검증 예제의 수를 이 값으로 줄입니다. -1은 모든 검증 샘플을 사용하는 것을 의미합니다. 양의 정수 또는 -1이어야 합니다. 기본값: -1 \n",
    "- max_input_length: 토큰화 후의 입력 시퀀스의 최대 길이입니다. 이 길이를 초과하는 시퀀스는 잘립니다. -1로 설정하면, max_input_length는 1024와 토크나이저에 의해 정의된 모델의 최대 길이 중 더 작은 값으로 설정됩니다. 양의 값으로 설정하면, max_input_length는 제공된 값과 토크나이저에 의해 정의된 모델의 최대 길이 중 더 작은 값으로 설정됩니다. 양의 정수 또는 -1이어야 합니다. 기본값: -1 \n",
    "- validation_split_ratio: 검증 채널이 없는 경우, 훈련 데이터에서 훈련-검증 분할 비율입니다. 0과 1 사이여야 합니다. 기본값: 0.2 \n",
    "- train_data_split_seed: 검증 데이터가 없는 경우, 알고리즘이 사용하는 훈련 데이터와 검증 데이터를 무작위로 분할하는 것을 고정합니다. 정수여야 합니다. 기본값: 0\n",
    "- preprocessing_num_workers: 전처리를 위해 사용할 프로세스의 수입니다. None이면 메인 프로세스를 사용하여 전처리합니다. 기본값: \"None\"\n",
    "- lora_r: Lora R입니다. 양의 정수여야 합니다. 기본값: 8\n",
    "- lora_alpha: Lora Alpha입니다. 양의 정수여야 합니다. 기본값: 32\n",
    "- lora_dropout: Lora Dropout입니다. 0과 1 사이의 양의 실수여야 합니다. 기본값: 0.05 \n",
    "- int8_quantization: True로 설정하면 훈련을 위해 모델이 8비트 정밀도로 로드됩니다. 7B/13B 모델의 기본값: False. 70B 모델의 기본값: True\n",
    "- enable_fsdp: True로 설정하면 완전 분할 데이터 병렬 처리(Fully Sharded Data Parallelism, FSDP)를 사용하여 훈련합니다. 7B/13B 모델의 기본값: True. 70B 모델의 기본값: False\n",
    "\n",
    "참고 사항 1: int8_quantization은 FSDP와 함께 사용할 수 없습니다. 또한, 모든 g5 인스턴스 유형에 대해 int8_quantization = 'False'와 enable_fsdp = 'False' 설정은 CUDA 메모리 문제로 인해 지원되지 않습니다. 따라서, int8_quantization 또는 enable_fsdp 중 하나를 반드시 'True'로 설정하는 것을 권장합니다.\n",
    "\n",
    "참고 사항 2: 모델의 크기 때문에 70B 모델은 어떤 지원 인스턴스 유형에서도 enable_fsdp = 'True' 설정으로 미세 조정할 수 없습니다.\n",
    "\n",
    "---"
   ]
  }
 ],
 "metadata": {
  "availableInstances": [
   {
    "_defaultOrder": 0,
    "_isFastLaunch": true,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 4,
    "name": "ml.t3.medium",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 1,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 8,
    "name": "ml.t3.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 2,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.t3.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 3,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.t3.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 4,
    "_isFastLaunch": true,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 8,
    "name": "ml.m5.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 5,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.m5.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 6,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.m5.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 7,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 64,
    "name": "ml.m5.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 8,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 128,
    "name": "ml.m5.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 9,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 192,
    "name": "ml.m5.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 10,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 256,
    "name": "ml.m5.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 11,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 384,
    "name": "ml.m5.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 12,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 8,
    "name": "ml.m5d.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 13,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.m5d.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 14,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.m5d.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 15,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 64,
    "name": "ml.m5d.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 16,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 128,
    "name": "ml.m5d.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 17,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 192,
    "name": "ml.m5d.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 18,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 256,
    "name": "ml.m5d.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 19,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 384,
    "name": "ml.m5d.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 20,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": true,
    "memoryGiB": 0,
    "name": "ml.geospatial.interactive",
    "supportedImageNames": [
     "sagemaker-geospatial-v1-0"
    ],
    "vcpuNum": 0
   },
   {
    "_defaultOrder": 21,
    "_isFastLaunch": true,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 4,
    "name": "ml.c5.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 22,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 8,
    "name": "ml.c5.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 23,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.c5.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 24,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.c5.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 25,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 72,
    "name": "ml.c5.9xlarge",
    "vcpuNum": 36
   },
   {
    "_defaultOrder": 26,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 96,
    "name": "ml.c5.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 27,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 144,
    "name": "ml.c5.18xlarge",
    "vcpuNum": 72
   },
   {
    "_defaultOrder": 28,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 192,
    "name": "ml.c5.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 29,
    "_isFastLaunch": true,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.g4dn.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 30,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.g4dn.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 31,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 64,
    "name": "ml.g4dn.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 32,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 128,
    "name": "ml.g4dn.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 33,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 4,
    "hideHardwareSpecs": false,
    "memoryGiB": 192,
    "name": "ml.g4dn.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 34,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 256,
    "name": "ml.g4dn.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 35,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 61,
    "name": "ml.p3.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 36,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 4,
    "hideHardwareSpecs": false,
    "memoryGiB": 244,
    "name": "ml.p3.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 37,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 8,
    "hideHardwareSpecs": false,
    "memoryGiB": 488,
    "name": "ml.p3.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 38,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 8,
    "hideHardwareSpecs": false,
    "memoryGiB": 768,
    "name": "ml.p3dn.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 39,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.r5.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 40,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.r5.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 41,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 64,
    "name": "ml.r5.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 42,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 128,
    "name": "ml.r5.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 43,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 256,
    "name": "ml.r5.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 44,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 384,
    "name": "ml.r5.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 45,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 512,
    "name": "ml.r5.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 46,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 768,
    "name": "ml.r5.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 47,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.g5.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 48,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.g5.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 49,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 64,
    "name": "ml.g5.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 50,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 128,
    "name": "ml.g5.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 51,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 256,
    "name": "ml.g5.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 52,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 4,
    "hideHardwareSpecs": false,
    "memoryGiB": 192,
    "name": "ml.g5.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 53,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 4,
    "hideHardwareSpecs": false,
    "memoryGiB": 384,
    "name": "ml.g5.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 54,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 8,
    "hideHardwareSpecs": false,
    "memoryGiB": 768,
    "name": "ml.g5.48xlarge",
    "vcpuNum": 192
   },
   {
    "_defaultOrder": 55,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 8,
    "hideHardwareSpecs": false,
    "memoryGiB": 1152,
    "name": "ml.p4d.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 56,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 8,
    "hideHardwareSpecs": false,
    "memoryGiB": 1152,
    "name": "ml.p4de.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 57,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.trn1.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 58,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 512,
    "name": "ml.trn1.32xlarge",
    "vcpuNum": 128
   },
   {
    "_defaultOrder": 59,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 512,
    "name": "ml.trn1n.32xlarge",
    "vcpuNum": 128
   }
  ],
  "instance_type": "ml.t3.medium",
  "kernelspec": {
   "display_name": "Python 3 (Data Science 3.0)",
   "language": "python",
   "name": "python3__SAGEMAKER_INTERNAL__arn:aws:sagemaker:us-east-1:081325390199:image/sagemaker-data-science-310-v1"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
