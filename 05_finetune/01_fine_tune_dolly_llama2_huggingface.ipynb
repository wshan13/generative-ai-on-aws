{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0a9f1ed1-db76-4ecb-a96b-c425c68db9c9",
   "metadata": {},
   "source": "# Llama 7B 모델 미세 조정하기"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## 준비사항 \n",
    "\n",
    "아래 실습을 원활하게 수행하기 위해 AWS P4d 또는 P4de 인스턴스가 필요합니다."
   ],
   "id": "5ed476e77308ebd3"
  },
  {
   "cell_type": "code",
   "id": "07ad2e56-bc79-45a9-8017-272272e1eaeb",
   "metadata": {
    "tags": []
   },
   "source": [
    "%pip install transformers\n",
    "%pip install peft\n",
    "%pip install accelerate\n",
    "%pip install bitsandbytes\n",
    "%pip install safetensors\n",
    "%pip install tokenizers\n",
    "%pip install datasets\n",
    "%pip install numpy\n",
    "%pip install torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cu121\n",
    "\n",
    "\n",
    "# %pip install -U transformers==4.39.0\n",
    "# %pip install -U peft==0.5.0\n",
    "# %pip install -U accelerate==0.26.0\n",
    "# #%pip install bitsandbytes #==0.40.2\n",
    "# #%pip install safetensors==0.3.1\"\n",
    "# #%pip install tokenizers==0.13.3\n",
    "# %pip install -U datasets==2.17.0\n",
    "# %pip install --no-cache https://developer.download.nvidia.com/compute/redist/jp/v60dp/pytorch/torch-2.3.0a0+ebedce2.nv24.02-cp310-cp310-linux_aarch64.whl"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "861f4453-1601-4420-9fd2-4d6f518ec327",
   "metadata": {
    "tags": []
   },
   "source": [
    "import os\n",
    "import argparse\n",
    "from transformers import (\n",
    "    AutoModelForCausalLM,\n",
    "    AutoTokenizer,\n",
    "    set_seed,\n",
    "    default_data_collator,\n",
    "#    BitsAndBytesConfig,\n",
    "    Trainer,\n",
    "    TrainingArguments,\n",
    ")\n",
    "from datasets import load_dataset\n",
    "import torch\n",
    "\n",
    "#import bitsandbytes as bnb\n",
    "#from huggingface_hub import login, HfFolder"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "3f021069-dfec-4355-80dd-f8c18af2d791",
   "metadata": {
    "tags": []
   },
   "source": ""
  },
  {
   "cell_type": "code",
   "id": "ca663aa7-7c3f-4123-b775-e6dae28b4f71",
   "metadata": {
    "tags": []
   },
   "source": [
    "import argparse\n",
    "parser = argparse.ArgumentParser()\n",
    "\n",
    "# 모델 ID와 데이터 세트 경로를 인자에 추가\n",
    "parser.add_argument(\n",
    "    \"--model_id\",\n",
    "    type=str,\n",
    "    default=\"NousResearch/Llama-2-7b-hf\", # 게이트 처리 안함\n",
    "    help=\"훈련에 사용할 모델 ID.\",\n",
    ")\n",
    "parser.add_argument(\n",
    "    \"--dataset_path\", \n",
    "    type=str, \n",
    "    default=\"lm_dataset\", \n",
    "    help=\"데이터 세트 경로.\"\n",
    ")\n",
    "# parser.add_argument(\n",
    "#     \"--hf_token\", \n",
    "#     type=str, \n",
    "#     default=HfFolder.get_token(), \n",
    "#     help=\"허깅페이스 사용자 토큰.\"\n",
    "# )\n",
    "# 에포크, 배치 크기, 학습률 및 시드를 위한 훈련 하이퍼파라미터 추가\n",
    "parser.add_argument(\n",
    "    \"--epochs\", \n",
    "    type=int, \n",
    "    default=1, \n",
    "    help=\"훈련할 에포크 수.\"\n",
    ")\n",
    "parser.add_argument(\n",
    "    \"--per_device_train_batch_size\",\n",
    "    type=int,\n",
    "    default=1,\n",
    "    help=\"훈련에 사용할 배치 크기.\",\n",
    ")\n",
    "parser.add_argument(\n",
    "    \"--lr\", \n",
    "    type=float, \n",
    "    default=5e-5, \n",
    "    help=\"훈련에 사용할 학습률.\"\n",
    ")\n",
    "parser.add_argument(\n",
    "    \"--seed\", \n",
    "    type=int, \n",
    "    default=42, \n",
    "    help=\"훈련에 사용할 시드.\"\n",
    ")\n",
    "parser.add_argument(\n",
    "    \"--gradient_checkpointing\",\n",
    "    type=bool,\n",
    "    default=True,\n",
    "    help=\"Deepspeed 구성 파일 경로.\",\n",
    ")\n",
    "parser.add_argument(\n",
    "    \"--bf16\",\n",
    "    type=bool,\n",
    "    default=True if torch.cuda.get_device_capability()[0] >= 8 else False,\n",
    "    help=\"bf16을 사용할지 여부.\",\n",
    ")\n",
    "# parser.add_argument(\n",
    "#     \"--merge_weights\",\n",
    "#     type=bool,\n",
    "#     default=True,\n",
    "#     help=\"LoRA 가중치를 기본 모델과 병합할지 여부.\",\n",
    "# )\n",
    "args, _ = parser.parse_known_args()\n",
    "\n",
    "# if args.hf_token:\n",
    "#     print(f\"허깅페이스 허브에 토큰 {args.hf_token[:10]}로 로그인 중...\")\n",
    "#     login(token=args.hf_token)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# # https://github.com/artidoro/qlora/blob/main/qlora.py 에서 복사한 코드\n",
    "# def print_trainable_parameters(model, use_4bit=False):\n",
    "#     \"\"\"\n",
    "#     모델에서 학습 가능한 매개변수의 수를 출력합니다.\n",
    "#     \"\"\"\n",
    "#     trainable_params = 0\n",
    "#     all_param = 0\n",
    "#     for _, param in model.named_parameters():\n",
    "#         num_params = param.numel()\n",
    "#         # DS Zero 3을 사용하고 가중치가 빈 값으로 초기화된 경우\n",
    "#         if num_params == 0 and hasattr(param, \"ds_numel\"):\n",
    "#             num_params = param.ds_numel\n",
    "# \n",
    "#         all_param += num_params\n",
    "#         if param.requires_grad:\n",
    "#             trainable_params += num_params\n",
    "#     # if use_4bit:\n",
    "#     #     trainable_params /= 2\n",
    "#     print(\n",
    "#         f\"전체 파라미터: {all_param:,d} || 학습 가능한 파라미터: {trainable_params:,d} || 학습 가능 비율%: {100 * trainable_params / all_param}\"\n",
    "#     )\n",
    "\n",
    "\n",
    "# # https://github.com/artidoro/qlora/blob/main/qlora.py 에서 복사한 코드\n",
    "# def find_all_linear_names(model):\n",
    "#     lora_module_names = set()\n",
    "#     # for name, module in model.named_modules():\n",
    "#     #     if isinstance(module, bnb.nn.Linear4bit):\n",
    "#     #         names = name.split(\".\")\n",
    "#     #         lora_module_names.add(names[0] if len(names) == 1 else names[-1])\n",
    "#     # \n",
    "#     # if \"lm_head\" in lora_module_names:  # 16비트에 필요\n",
    "#     #     lora_module_names.remove(\"lm_head\")\n",
    "#     # return list(lora_module_names)\n",
    "\n",
    "\n",
    "# def create_peft_model(model, gradient_checkpointing=True, bf16=True):\n",
    "#     from peft import (\n",
    "#         get_peft_model,\n",
    "#         LoraConfig,\n",
    "#         TaskType,\n",
    "# #        prepare_model_for_kbit_training,\n",
    "#     )\n",
    "#     from peft.tuners.lora import LoraLayer\n",
    "# \n",
    "#     # # 훈련을 위해 int-4 모델 준비\n",
    "#     # model = prepare_model_for_kbit_training(\n",
    "#     #     model, use_gradient_checkpointing=gradient_checkpointing\n",
    "#     # )\n",
    "#     if gradient_checkpointing:\n",
    "#         model.gradient_checkpointing_enable()\n",
    "# \n",
    "#     # # LoRA 대상 모듈 가져오기\n",
    "#     # modules = find_all_linear_names(model)    \n",
    "#     \n",
    "#     # 모델의 어텐션 블록만 대상으로 할 경우\n",
    "#     # modules = [\"q_proj\", \"v_proj\"]\n",
    "# \n",
    "#     # 모든 선형 레이어를 대상으로 하는 경우\n",
    "#     modules = ['q_proj','k_proj','v_proj','o_proj','gate_proj','down_proj','up_proj'] #,'lm_head']\n",
    "# \n",
    "#     print(f\"양자화할 모듈 {len(modules)}개를 찾았습니다: {modules}\")\n",
    "# \n",
    "#     peft_config = LoraConfig(\n",
    "#         r=64,\n",
    "#         lora_alpha=16,\n",
    "#         target_modules=modules,\n",
    "#         lora_dropout=0.1,\n",
    "#         bias=\"none\",\n",
    "#         task_type=TaskType.CAUSAL_LM,\n",
    "#     )\n",
    "# \n",
    "#     model = get_peft_model(model, peft_config)\n",
    "# \n",
    "#     # 'norm' 레이어를 float 32로 상향 변환하여 모델 전처리\n",
    "#     for name, module in model.named_modules():\n",
    "#         if isinstance(module, LoraLayer):\n",
    "#             if bf16:\n",
    "#                 module = module.to(torch.bfloat16)\n",
    "#         if \"norm\" in name:\n",
    "#             module = module.to(torch.float32)\n",
    "#         if \"lm_head\" in name or \"embed_tokens\" in name:\n",
    "#             if hasattr(module, \"weight\"):\n",
    "#                 if bf16 and module.weight.dtype == torch.float32:\n",
    "#                     module = module.to(torch.bfloat16)\n",
    "# \n",
    "#     model.print_trainable_parameters()\n",
    "#     return model"
   ],
   "id": "f47e9fa5-446d-4fb0-973d-c270a30db03e",
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "9b0c2612-8951-4f4c-b5c4-1277fca469cb",
   "metadata": {},
   "source": [
    "## 데이터 세트 불러오기 및 준비\n",
    "\n",
    "[InstructGPT 논문](https://arxiv.org/abs/2203.02155)에 설명된 여러 행동 카테고리(브레인스토밍, 분류, 폐쇄형 QA, 생성, 정보 추출, 개방형 QA, 요약) 분야에 수천 명의 Databricks 직원들이 생성한 명령어-응답 항목들을 포함한 오픈 소스 데이터 세트 [dolly](https://huggingface.co/datasets/databricks/databricks-dolly-15k)를 사용할 것입니다.\n",
    "\n",
    "```python\n",
    "{\n",
    "  \"instruction\": \"World of Warcraft란 무엇인가?\",\n",
    "  \"context\": \"\",\n",
    "  \"response\": \"World of Warcraft는 대규모 온라인 멀티 플레이어 롤플레잉 게임입니다. 2004년에 Blizzard Entertainment에서 출시했습니다.\"\n",
    "}\n",
    "```\n",
    "\n",
    "`samsum` 데이터 세트를 불러오려면 허깅페이스 Datasets 라이브러리의 `load_dataset()` 메소드를 사용합니다."
   ]
  },
  {
   "cell_type": "code",
   "id": "83bace11-a0fb-433c-8d2a-c1e9bc8864f0",
   "metadata": {
    "tags": []
   },
   "source": [
    "# 시드 설정하기\n",
    "set_seed(args.seed)\n",
    "\n",
    "from datasets import load_dataset\n",
    "from random import randrange\n",
    "\n",
    "# 허깅페이스 허브에서 데이터 세트 불러오기\n",
    "dataset = load_dataset(\"databricks/databricks-dolly-15k\", split=\"train\")\n",
    "dataset = dataset.select(range(1000))\n",
    "\n",
    "print(f\"데이터 세트 크기: {len(dataset)}\")\n",
    "print(dataset[randrange(len(dataset))])\n",
    "# 데이터 세트 크기: 15011"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "89f07a4c-8035-45ef-a498-93acd8ba0d33",
   "metadata": {},
   "source": "모델을 명령어로 조정하려면 구조화된 예제들을 명령어로 표현된 작업들의 모음으로 변환해야 합니다. 샘플을 입력 받아서 지정된 형식의 명령어 문자열을 반환하는 `formatting_function`을 정의합니다."
  },
  {
   "cell_type": "code",
   "id": "4e938006-7605-4e4c-9cda-dc80f194cc57",
   "metadata": {
    "tags": []
   },
   "source": [
    "def format_dolly(sample):\n",
    "    instruction = f\"### Instruction\\n{sample['instruction']}\"\n",
    "    context = f\"### Context\\n{sample['context']}\" if len(sample[\"context\"]) > 0 else None\n",
    "    response = f\"### Answer\\n{sample['response']}\"\n",
    "    # 모든 파트를 하나의 문자열로 결합합니다.\n",
    "    prompt = \"\\n\\n\".join([i for i in [instruction, context, response] if i is not None])\n",
    "    return prompt\n"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "포맷팅 함수를 임의의 예제로 테스트 해봅시다.",
   "id": "7e783c760c4197f6"
  },
  {
   "cell_type": "code",
   "id": "483e7629-595f-49c3-a2c1-e19f65eac49a",
   "metadata": {
    "tags": []
   },
   "source": [
    "from random import randrange\n",
    "\n",
    "print(format_dolly(dataset[randrange(len(dataset))]))"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "또한, 샘플을 포맷팅하는 것 외에도 더 효율적인 훈련을 위해 여러 샘플을 하나의 시퀀스로 묶고자 합니다.",
   "id": "2398ee9189eff3a3"
  },
  {
   "cell_type": "code",
   "id": "48c0c16f-9471-4e84-8e0f-a2b4ca476b25",
   "metadata": {
    "tags": []
   },
   "source": [
    "from transformers import AutoTokenizer\n",
    "\n",
    "#model_id = \"meta-llama/Llama-2-13b-hf\" # 분할된 가중치, 게이트 처리\n",
    "model_id = \"NousResearch/Llama-2-7b-hf\" # 게이트 처리 안함\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_id)\n",
    "tokenizer.pad_token = tokenizer.eos_token"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "샘플들을 주어진 길이의 시퀀스로 묶고, 이를 토큰화하기 위한 몇 가지 보조 함수를 정의합니다.",
   "id": "4b473ae095dd56dc"
  },
  {
   "cell_type": "code",
   "id": "ecb7e1ca-9c07-432b-ae73-c06776a4897b",
   "metadata": {
    "tags": []
   },
   "source": [
    "from random import randint\n",
    "from itertools import chain\n",
    "from functools import partial\n",
    "\n",
    "\n",
    "# 각 샘플에 프롬프트를 추가하기 위한 템플릿 데이터 세트\n",
    "def template_dataset(sample):\n",
    "    sample[\"text\"] = f\"{format_dolly(sample)}{tokenizer.eos_token}\"\n",
    "    return sample\n",
    "\n",
    "\n",
    "# 각 샘플에 프롬프트 템플릿 적용\n",
    "dataset = dataset.map(template_dataset, remove_columns=list(dataset.features))\n",
    "# 임의의 샘플 출력\n",
    "print(dataset[randint(0, len(dataset))][\"text\"])\n",
    "\n",
    "# 다음 배치에서 사용할 수 있도록 배치에서 남은 부분을 저장할 빈 리스트 생성\n",
    "remainder = {\"input_ids\": [], \"attention_mask\": [], \"token_type_ids\": []}\n",
    "\n",
    "def chunk(sample, chunk_length=2048):\n",
    "    # 이전 배치에서 남은 부분을 저장할 전역 변수 remainder 정의\n",
    "    global remainder\n",
    "    # 모든 텍스트를 연결하고 이전 배치에서 남은 부분 추가\n",
    "    concatenated_examples = {k: list(chain(*sample[k])) for k in sample.keys()}\n",
    "    concatenated_examples = {k: remainder[k] + concatenated_examples[k] for k in concatenated_examples.keys()}\n",
    "    # 배치의 총 토큰 수 가져오기\n",
    "    batch_total_length = len(concatenated_examples[list(sample.keys())[0]])\n",
    "\n",
    "    # 배치에서 최대 청크 수 가져오기\n",
    "    if batch_total_length >= chunk_length:\n",
    "        batch_chunk_length = (batch_total_length // chunk_length) * chunk_length\n",
    "\n",
    "    # 최대 길이의 청크로 분할\n",
    "    result = {\n",
    "        k: [t[i : i + chunk_length] for i in range(0, batch_chunk_length, chunk_length)]\n",
    "        for k, t in concatenated_examples.items()\n",
    "    }\n",
    "    # 다음 배치를 위해 전역 변수에 남은 부분 추가\n",
    "    remainder = {k: concatenated_examples[k][batch_chunk_length:] for k in concatenated_examples.keys()}\n",
    "    # 레이블 준비\n",
    "    result[\"labels\"] = result[\"input_ids\"].copy()\n",
    "    return result\n",
    "\n",
    "\n",
    "# 데이터 세트를 토큰화하고 청크로 나누기\n",
    "lm_dataset = dataset.map(\n",
    "    lambda sample: tokenizer(sample[\"text\"]), batched=True, remove_columns=list(dataset.features)\n",
    ").map(\n",
    "    partial(chunk, chunk_length=2048),\n",
    "    batched=True,\n",
    ")\n",
    "\n",
    "# 전체 샘플 수 출력\n",
    "print(f\"전체 샘플 수: {len(lm_dataset)}\")"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "71d95685-4573-4943-8ecb-9d42167a5808",
   "metadata": {
    "tags": []
   },
   "source": [
    "# 위의 청킹 작업은 행의 수를 줄여줌\n",
    "print(lm_dataset)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "6eafc7bb-cd67-4ddc-8930-90dc649d5626",
   "metadata": {
    "tags": []
   },
   "source": [
    "# bnb 설정을 사용하여 허깅페이스 허브에서 모델 불러오기\n",
    "# bnb_config = BitsAndBytesConfig(\n",
    "#     load_in_4bit=True,\n",
    "#     bnb_4bit_use_double_quant=True,\n",
    "#     bnb_4bit_quant_type=\"nf4\",\n",
    "#     bnb_4bit_compute_dtype=torch.bfloat16,\n",
    "# )\n",
    "\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    args.model_id,\n",
    "    use_cache=False\n",
    "    if args.gradient_checkpointing\n",
    "    else True,  # gradient checkpointing에 필요함\n",
    "    device_map=\"auto\",\n",
    "#    quantization_config=bnb_config,\n",
    ")\n",
    "\n",
    "# peft 설정 생성\n",
    "# model = create_peft_model(\n",
    "#     model, gradient_checkpointing=args.gradient_checkpointing, bf16=args.bf16\n",
    "# )"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "37c19f83-da6a-4f7b-9b0c-e37704b9bd5c",
   "metadata": {
    "tags": []
   },
   "source": [
    "# 훈련 설정 정의하기\n",
    "output_dir = \"./tmp/llama2\"\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=output_dir,\n",
    "    per_device_train_batch_size=args.per_device_train_batch_size,\n",
    "    bf16=args.bf16,  # 사용 가능한 경우 BF16 사용\n",
    "    learning_rate=args.lr,\n",
    "    num_train_epochs=args.epochs,\n",
    "    gradient_checkpointing=args.gradient_checkpointing,\n",
    "    # 로깅 전략\n",
    "    logging_dir=f\"{output_dir}/logs\",\n",
    "    logging_strategy=\"steps\",\n",
    "    logging_steps=10,\n",
    "    save_strategy=\"no\",\n",
    ")\n",
    "\n",
    "# Trainer 인스턴스 생성\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=lm_dataset,\n",
    "    data_collator=default_data_collator,\n",
    ")\n",
    "\n",
    "# 훈련 시작\n",
    "trainer.train()"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "2a831a1c-adcc-4674-8bd5-5cfc4986aa34",
   "metadata": {
    "tags": []
   },
   "source": [
    "sagemaker_save_dir=\"./llama2_dolly\"\n",
    "# if args.merge_weights:\n",
    "#     # 어댑터 가중치를 기본 모델과 병합하고 저장\n",
    "#     # int 4 모델 저장\n",
    "#     trainer.model.save_pretrained(output_dir, safe_serialization=False)\n",
    "#     # 메모리 정리\n",
    "#     del model\n",
    "#     del trainer\n",
    "#     torch.cuda.empty_cache()\n",
    "\n",
    "#     from peft import AutoPeftModelForCausalLM\n",
    "\n",
    "#     # FP16으로 PEFT 모델 불러오기\n",
    "#     model = AutoPeftModelForCausalLM.from_pretrained(\n",
    "#         output_dir,\n",
    "#         low_cpu_mem_usage=True,\n",
    "#         torch_dtype=torch.bfloat16,\n",
    "#     )  \n",
    "#     # LoRA와 기본 모델을 병합하고 저장\n",
    "#     model = model.merge_and_unload()        \n",
    "#     model.save_pretrained(\n",
    "#         sagemaker_save_dir, safe_serialization=True, max_shard_size=\"2GB\"\n",
    "#     )\n",
    "# else:\n",
    "\n",
    "trainer.model.save_pretrained(\n",
    "    sagemaker_save_dir, safe_serialization=True\n",
    ")\n",
    "\n",
    "# 추론을 쉽게 하기 위해 토크나이저 저장\n",
    "tokenizer = AutoTokenizer.from_pretrained(args.model_id)\n",
    "tokenizer.save_pretrained(sagemaker_save_dir)"
   ],
   "outputs": [],
   "execution_count": null
  }
 ],
 "metadata": {
  "availableInstances": [
   {
    "_defaultOrder": 0,
    "_isFastLaunch": true,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 4,
    "name": "ml.t3.medium",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 1,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 8,
    "name": "ml.t3.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 2,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.t3.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 3,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.t3.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 4,
    "_isFastLaunch": true,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 8,
    "name": "ml.m5.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 5,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.m5.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 6,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.m5.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 7,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 64,
    "name": "ml.m5.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 8,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 128,
    "name": "ml.m5.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 9,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 192,
    "name": "ml.m5.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 10,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 256,
    "name": "ml.m5.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 11,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 384,
    "name": "ml.m5.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 12,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 8,
    "name": "ml.m5d.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 13,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.m5d.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 14,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.m5d.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 15,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 64,
    "name": "ml.m5d.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 16,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 128,
    "name": "ml.m5d.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 17,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 192,
    "name": "ml.m5d.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 18,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 256,
    "name": "ml.m5d.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 19,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 384,
    "name": "ml.m5d.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 20,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": true,
    "memoryGiB": 0,
    "name": "ml.geospatial.interactive",
    "supportedImageNames": [
     "sagemaker-geospatial-v1-0"
    ],
    "vcpuNum": 0
   },
   {
    "_defaultOrder": 21,
    "_isFastLaunch": true,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 4,
    "name": "ml.c5.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 22,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 8,
    "name": "ml.c5.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 23,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.c5.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 24,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.c5.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 25,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 72,
    "name": "ml.c5.9xlarge",
    "vcpuNum": 36
   },
   {
    "_defaultOrder": 26,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 96,
    "name": "ml.c5.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 27,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 144,
    "name": "ml.c5.18xlarge",
    "vcpuNum": 72
   },
   {
    "_defaultOrder": 28,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 192,
    "name": "ml.c5.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 29,
    "_isFastLaunch": true,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.g4dn.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 30,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.g4dn.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 31,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 64,
    "name": "ml.g4dn.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 32,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 128,
    "name": "ml.g4dn.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 33,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 4,
    "hideHardwareSpecs": false,
    "memoryGiB": 192,
    "name": "ml.g4dn.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 34,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 256,
    "name": "ml.g4dn.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 35,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 61,
    "name": "ml.p3.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 36,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 4,
    "hideHardwareSpecs": false,
    "memoryGiB": 244,
    "name": "ml.p3.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 37,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 8,
    "hideHardwareSpecs": false,
    "memoryGiB": 488,
    "name": "ml.p3.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 38,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 8,
    "hideHardwareSpecs": false,
    "memoryGiB": 768,
    "name": "ml.p3dn.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 39,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.r5.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 40,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.r5.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 41,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 64,
    "name": "ml.r5.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 42,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 128,
    "name": "ml.r5.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 43,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 256,
    "name": "ml.r5.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 44,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 384,
    "name": "ml.r5.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 45,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 512,
    "name": "ml.r5.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 46,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 768,
    "name": "ml.r5.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 47,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.g5.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 48,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.g5.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 49,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 64,
    "name": "ml.g5.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 50,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 128,
    "name": "ml.g5.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 51,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 256,
    "name": "ml.g5.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 52,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 4,
    "hideHardwareSpecs": false,
    "memoryGiB": 192,
    "name": "ml.g5.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 53,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 4,
    "hideHardwareSpecs": false,
    "memoryGiB": 384,
    "name": "ml.g5.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 54,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 8,
    "hideHardwareSpecs": false,
    "memoryGiB": 768,
    "name": "ml.g5.48xlarge",
    "vcpuNum": 192
   },
   {
    "_defaultOrder": 55,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 8,
    "hideHardwareSpecs": false,
    "memoryGiB": 1152,
    "name": "ml.p4d.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 56,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 8,
    "hideHardwareSpecs": false,
    "memoryGiB": 1152,
    "name": "ml.p4de.24xlarge",
    "vcpuNum": 96
   }
  ],
  "instance_type": "ml.t3.medium",
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
