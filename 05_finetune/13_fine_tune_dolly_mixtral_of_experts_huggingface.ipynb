{
 "cells": [
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# Mixtral-8x7B 모델 미세 조정하기",
   "id": "212f096c9aab92a2"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## 준비사항 \n",
    "\n",
    "이번 실습은 `ml.p4de.24xlarge` 인스턴스에서 수행되었습니다.\n",
    "\n",
    "그리고 PyTorch 2.3.1, Python 3.11, CUDA Version 12.1을 사용하였습니다."
   ],
   "id": "0a9f1ed1-db76-4ecb-a96b-c425c68db9c9"
  },
  {
   "cell_type": "code",
   "id": "44308438-b684-4043-aca5-ffde8177cbdf",
   "metadata": {
    "tags": []
   },
   "source": [
    "# GPU 정보 및 CUDA 버전 확인하기\n",
    "!nvidia-smi"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "07ad2e56-bc79-45a9-8017-272272e1eaeb",
   "metadata": {
    "tags": []
   },
   "source": [
    "# nvidia-smi에 표시된 CUDA 버전과 맞춰서 torch를 설치합니다. (이 경우 12.1)\n",
    "# 여기에서 설치 명령어를 확인하세요: https://pytorch.org/get-started/locally/\n",
    "%pip install -U torch --index-url https://download.pytorch.org/whl/cu121\n",
    "%pip install -U transformers==4.41.2\n",
    "%pip install -U peft==0.11.1\n",
    "%pip install -U datasets==2.19.2\n",
    "%pip install -U bitsandbytes==0.43.1\n",
    "%pip install -U scipy==1.13.1\n",
    "%pip install -U ipywidgets==8.1.3\n",
    "%pip install -U matplotlib==3.9.0"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "47107a26-00ff-45c7-b62e-b259aab43f64",
   "metadata": {
    "tags": []
   },
   "source": [
    "!python -m bitsandbytes"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "861f4453-1601-4420-9fd2-4d6f518ec327",
   "metadata": {
    "tags": []
   },
   "source": [
    "from accelerate import FullyShardedDataParallelPlugin, Accelerator\n",
    "from torch.distributed.fsdp.fully_sharded_data_parallel import FullOptimStateDictConfig, FullStateDictConfig\n",
    "\n",
    "fsdp_plugin = FullyShardedDataParallelPlugin(\n",
    "    state_dict_config=FullStateDictConfig(offload_to_cpu=True, rank0_only=False),\n",
    "    optim_state_dict_config=FullOptimStateDictConfig(offload_to_cpu=True, rank0_only=False),\n",
    ")\n",
    "\n",
    "accelerator = Accelerator(fsdp_plugin=fsdp_plugin)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "69703202-966d-4d97-be8d-8a4696cd774f",
   "metadata": {
    "tags": []
   },
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "dataset_name = \"databricks/databricks-dolly-15k\"\n",
    "\n",
    "train_dataset = load_dataset(dataset_name, split=\"train[0:800]\")\n",
    "eval_dataset = load_dataset(dataset_name, split=\"train[800:1000]\")"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "96bae2b2-9976-4c3f-a083-ad84ff1cfa76",
   "metadata": {
    "tags": []
   },
   "source": [
    "import torch\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM, BitsAndBytesConfig\n",
    "\n",
    "base_model_id = \"mistralai/Mixtral-8x7B-Instruct-v0.1\"\n",
    "\n",
    "bnb_config = BitsAndBytesConfig(\n",
    "   load_in_4bit=True,\n",
    "   bnb_4bit_use_double_quant=True,\n",
    "   bnb_4bit_compute_dtype=torch.bfloat16\n",
    ")\n",
    "\n",
    "model = AutoModelForCausalLM.from_pretrained(base_model_id, \n",
    "                                             quantization_config=bnb_config, \n",
    "                                             device_map=\"auto\")"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "292c54b9-c2d9-4ac6-90eb-c0e73c67be2c",
   "metadata": {},
   "source": [
    "## 명령어 데이터 세트 설정\n",
    "\n",
    "다음 형식을 따라야 합니다: https://huggingface.co/mistralai/Mixtral-8x7B-Instruct-v0.1"
   ]
  },
  {
   "cell_type": "code",
   "id": "bb7aa08b-2f47-4605-8029-4fbdbea9b609",
   "metadata": {
    "tags": []
   },
   "source": [
    "# 토큰화 설정 \n",
    "tokenizer = AutoTokenizer.from_pretrained(\n",
    "    base_model_id,\n",
    "    padding_side=\"left\",\n",
    "    add_bos_token=True,\n",
    ")\n",
    "tokenizer.pad_token = tokenizer.eos_token"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "be2945b3-a158-497c-9813-c1ccb5659c4b",
   "metadata": {
    "tags": []
   },
   "source": [
    "def generate_and_tokenize_prompt(data_point):\n",
    "    full_prompt = f\"\"\"\n",
    "\n",
    "[INST] Given a question and some additional context, provide an answer. [/INST]\n",
    "\n",
    "### Question:\n",
    "{data_point['instruction']}\n",
    "\n",
    "### Context:\n",
    "{f\"Here is some context: {data_point['context']}\" if len(data_point[\"context\"]) > 0 else \"\"}\n",
    "\n",
    "### Response:\n",
    "{data_point['response']}\n",
    "\n",
    "</s>\n",
    "\"\"\"\n",
    "\n",
    "    tokenized_prompt = tokenizer(full_prompt)\n",
    "    return tokenized_prompt\n",
    "\n",
    "tokenized_train_dataset = train_dataset.map(generate_and_tokenize_prompt)\n",
    "tokenized_val_dataset = eval_dataset.map(generate_and_tokenize_prompt)\n",
    "\n",
    "untokenized_text = tokenizer.decode(tokenized_train_dataset[0]['input_ids']) \n",
    "print(untokenized_text)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "5f8c597f-67b3-4e4a-8575-9ffb481db5ae",
   "metadata": {
    "tags": []
   },
   "source": [
    "# #max_length = 400 \n",
    "\n",
    "# tokenized_train_dataset = train_dataset.map(generate_and_tokenize_prompt)\n",
    "# tokenized_val_dataset = eval_dataset.map(generate_and_tokenize_prompt)\n",
    "\n",
    "# untokenized_text = tokenizer.decode(tokenized_train_dataset[4]['input_ids']) \n",
    "# print(untokenized_text)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "8f39720f-901d-41eb-80a0-e37ef463a624",
   "metadata": {
    "tags": []
   },
   "source": [
    "from peft import prepare_model_for_kbit_training\n",
    "\n",
    "model.gradient_checkpointing_enable()\n",
    "model = prepare_model_for_kbit_training(model)\n",
    "\n",
    "def print_trainable_parameters(model):\n",
    "    \"\"\"\n",
    "    모델에서 학습 가능한 파라미터 수를 출력합니다.\n",
    "    \"\"\"\n",
    "    trainable_params = 0\n",
    "    all_param = 0\n",
    "    for _, param in model.named_parameters():\n",
    "        all_param += param.numel()\n",
    "        if param.requires_grad:\n",
    "            trainable_params += param.numel()\n",
    "    print(\n",
    "        f\"trainable params: {trainable_params} || all params: {all_param} || trainable%: {100 * trainable_params / all_param}\"\n",
    "    )"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "a77a1fac-ba03-483e-aabe-ce729ed408d0",
   "metadata": {
    "tags": []
   },
   "source": [
    "from peft import LoraConfig, get_peft_model\n",
    "\n",
    "config = LoraConfig(\n",
    "    r=8,\n",
    "    lora_alpha=16,\n",
    "    target_modules=[\n",
    "        \"q_proj\",\n",
    "        \"k_proj\",\n",
    "        \"v_proj\",\n",
    "        \"o_proj\",\n",
    "        \"w1\",\n",
    "        \"w2\",\n",
    "        \"w3\",\n",
    "        \"lm_head\",\n",
    "    ],\n",
    "    bias=\"none\",\n",
    "    lora_dropout=0.05,  # 일반적인 방법\n",
    "    task_type=\"CAUSAL_LM\",\n",
    ")\n",
    "\n",
    "model = get_peft_model(model, config)\n",
    "print_trainable_parameters(model)\n",
    "\n",
    "# 가속기를 적용합니다. 가속기를 제거하려면 아래 코드를 주석 처리하세요.\n",
    "model = accelerator.prepare_model(model)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "539487ff-5e60-4b99-b48f-dfb5c1e9beab",
   "metadata": {
    "tags": []
   },
   "source": [
    "!rm -rf ./dolly_mixtral_finetune"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "e6fe505b-cdff-4d05-bbd1-f64e37c59db2",
   "metadata": {
    "tags": []
   },
   "source": [
    "import transformers\n",
    "from datetime import datetime\n",
    "\n",
    "if torch.cuda.device_count() > 1: # GPU가 1개 이상인 경우\n",
    "    model.is_parallelizable = True\n",
    "    model.model_parallel = True\n",
    "\n",
    "output_dir = \"./dolly_mixtral_finetune\"\n",
    "\n",
    "trainer = transformers.Trainer(\n",
    "    model=model,\n",
    "    train_dataset=tokenized_train_dataset,\n",
    "    eval_dataset=tokenized_val_dataset,\n",
    "    args=transformers.TrainingArguments(\n",
    "        output_dir=output_dir,\n",
    "        warmup_steps=5,\n",
    "        per_device_train_batch_size=1,\n",
    "        gradient_checkpointing=True,\n",
    "        gradient_accumulation_steps=4,\n",
    "        max_steps=10,\n",
    "        learning_rate=2.5e-5, \n",
    "        logging_steps=5,\n",
    "        fp16=True, \n",
    "        optim=\"paged_adamw_8bit\",\n",
    "        logging_dir=\"./logs\",        # 로그를 저장할 디렉토리\n",
    "        save_strategy=\"steps\",       # 로그 스텝마다 모델 체크포인트 저장\n",
    "        save_steps=10,               # 10 스텝마다 체크포인트 저장\n",
    "        evaluation_strategy=\"steps\", # 로그 스텝마다 모델 평가\n",
    "        eval_steps=10,               # 10 스텝마다 평가하고 체크포인트 저장\n",
    "        do_eval=True,                # 훈련 종료 시 평가 수행\n",
    "    ),\n",
    "    data_collator=transformers.DataCollatorForLanguageModeling(tokenizer, mlm=False),\n",
    ")\n",
    "\n",
    "model.config.use_cache = False  # 경고 비활성화. 추론 시에는 다시 활성화하세요!\n",
    "trainer.train()"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "8311a434-351d-4089-a3bf-020c5b5b7ee1",
   "metadata": {
    "tags": []
   },
   "source": [
    "eval_tokenizer = AutoTokenizer.from_pretrained(\n",
    "    base_model_id,\n",
    "    padding_side=\"left\"\n",
    ")\n",
    "eval_tokenizer.pad_token = eval_tokenizer.eos_token"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "7fe33586-45dc-4d7c-abf6-405669231d82",
   "metadata": {
    "tags": []
   },
   "source": [
    "prompt = \"\"\"\n",
    "<s>\n",
    "[INST] Given a question and some additional context, provide an answer. [/INST]\n",
    "\n",
    "### Question:\n",
    "Who wrote \"Generative AI on AWS\" by O'Reilly Media?\n",
    "\n",
    "### Context: \n",
    "Chris Fregly and Antje Barth wrote \"Data Science on AWS\" by O'Reilly Media.\n",
    "Chris Fregly, Antje Barth, and Shelbee Eigenbrode wrote \"Generative AI on AWS\" by O'Reilly Media.\n",
    "\n",
    "### Response:</s>\n",
    "\"\"\"\n",
    "\n",
    "model_input = eval_tokenizer(prompt, return_tensors=\"pt\").to(\"cuda\")\n",
    "\n",
    "with torch.no_grad():\n",
    "    print(eval_tokenizer.decode(model.generate(**model_input, max_new_tokens=32)[0], skip_special_tokens=True))"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "47b2da4e-ea8f-4b33-bfa0-dcc5743be8c6",
   "metadata": {},
   "source": "# PEFT 모델 로드 및 추론 수행"
  },
  {
   "cell_type": "code",
   "id": "28851715-112e-4005-ba8f-7edf73b69b20",
   "metadata": {
    "tags": []
   },
   "source": [
    "# import torch\n",
    "# from transformers import AutoTokenizer, AutoModelForCausalLM, BitsAndBytesConfig\n",
    "\n",
    "# base_model_id = \"mistralai/Mixtral-8x7B-Instruct-v0.1\"\n",
    "# bnb_config = BitsAndBytesConfig(\n",
    "#     load_in_4bit=True,\n",
    "#     bnb_4bit_use_double_quant=True,\n",
    "#     bnb_4bit_compute_dtype=torch.bfloat16\n",
    "# )\n",
    "\n",
    "# base_model = AutoModelForCausalLM.from_pretrained(\n",
    "#     base_model_id, \n",
    "#     quantization_config=bnb_config,\n",
    "#     device_map=\"auto\"\n",
    "# )"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "93f7be54-05b6-4413-bd76-545be540d9e9",
   "metadata": {
    "tags": []
   },
   "source": [
    "# from peft import PeftModel\n",
    "\n",
    "# peft_model = PeftModel.from_pretrained(base_model, f\"{output_dir}/checkpoint-10\")\n",
    "# #peft_model.eval()"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "93da703d-519c-47c0-95b5-fed562fb0be9",
   "metadata": {
    "tags": []
   },
   "source": [
    "# tokenizer = AutoTokenizer.from_pretrained(\n",
    "#     base_model_id,\n",
    "# #    add_bos_token=True\n",
    "# )\n",
    "# tokenizer.pad_token = tokenizer.eos_token\n",
    "\n",
    "# eval_tokenizer = AutoTokenizer.from_pretrained(\n",
    "#     base_model_id,\n",
    "#     add_bos_token=True,\n",
    "# )"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "245f812d-cf71-49d6-a71b-1803f7073aea",
   "metadata": {
    "tags": []
   },
   "source": [
    "# prompt = \"\"\"\n",
    "\n",
    "# [INST] Given a question and some additional context, provide an answer. [/INST]\n",
    "\n",
    "# ### Question:\n",
    "# Who wrote \"Generative AI on AWS\" by O'Reilly Media?\n",
    "\n",
    "# ### Context: \n",
    "# Chris Fregly and Antje Barth wrote \"Data Science on AWS\" by O'Reilly Media.\n",
    "# Chris Fregly, Antje Barth, and Shelbee Eigenbrode wrote \"Generative AI on AWS\" by O'Reilly Media.\n",
    "\n",
    "# ### Response:\n",
    "\n",
    "# \"\"\"\n",
    "\n",
    "# model_input = tokenizer(prompt, return_tensors=\"pt\").to(\"cuda\")\n",
    "\n",
    "# with torch.no_grad():\n",
    "#     print(tokenizer.decode(peft_model.generate(**model_input, max_new_tokens=100)[0], skip_special_tokens=True))"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "f25686dd-9c0f-4e56-ab85-2a8058269352",
   "metadata": {},
   "source": [],
   "outputs": [],
   "execution_count": null
  }
 ],
 "metadata": {
  "availableInstances": [
   {
    "_defaultOrder": 0,
    "_isFastLaunch": true,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 4,
    "name": "ml.t3.medium",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 1,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 8,
    "name": "ml.t3.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 2,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.t3.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 3,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.t3.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 4,
    "_isFastLaunch": true,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 8,
    "name": "ml.m5.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 5,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.m5.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 6,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.m5.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 7,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 64,
    "name": "ml.m5.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 8,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 128,
    "name": "ml.m5.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 9,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 192,
    "name": "ml.m5.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 10,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 256,
    "name": "ml.m5.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 11,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 384,
    "name": "ml.m5.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 12,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 8,
    "name": "ml.m5d.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 13,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.m5d.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 14,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.m5d.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 15,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 64,
    "name": "ml.m5d.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 16,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 128,
    "name": "ml.m5d.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 17,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 192,
    "name": "ml.m5d.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 18,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 256,
    "name": "ml.m5d.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 19,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 384,
    "name": "ml.m5d.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 20,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": true,
    "memoryGiB": 0,
    "name": "ml.geospatial.interactive",
    "supportedImageNames": [
     "sagemaker-geospatial-v1-0"
    ],
    "vcpuNum": 0
   },
   {
    "_defaultOrder": 21,
    "_isFastLaunch": true,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 4,
    "name": "ml.c5.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 22,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 8,
    "name": "ml.c5.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 23,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.c5.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 24,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.c5.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 25,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 72,
    "name": "ml.c5.9xlarge",
    "vcpuNum": 36
   },
   {
    "_defaultOrder": 26,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 96,
    "name": "ml.c5.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 27,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 144,
    "name": "ml.c5.18xlarge",
    "vcpuNum": 72
   },
   {
    "_defaultOrder": 28,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 192,
    "name": "ml.c5.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 29,
    "_isFastLaunch": true,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.g4dn.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 30,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.g4dn.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 31,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 64,
    "name": "ml.g4dn.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 32,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 128,
    "name": "ml.g4dn.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 33,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 4,
    "hideHardwareSpecs": false,
    "memoryGiB": 192,
    "name": "ml.g4dn.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 34,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 256,
    "name": "ml.g4dn.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 35,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 61,
    "name": "ml.p3.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 36,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 4,
    "hideHardwareSpecs": false,
    "memoryGiB": 244,
    "name": "ml.p3.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 37,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 8,
    "hideHardwareSpecs": false,
    "memoryGiB": 488,
    "name": "ml.p3.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 38,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 8,
    "hideHardwareSpecs": false,
    "memoryGiB": 768,
    "name": "ml.p3dn.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 39,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.r5.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 40,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.r5.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 41,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 64,
    "name": "ml.r5.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 42,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 128,
    "name": "ml.r5.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 43,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 256,
    "name": "ml.r5.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 44,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 384,
    "name": "ml.r5.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 45,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 512,
    "name": "ml.r5.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 46,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 768,
    "name": "ml.r5.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 47,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.g5.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 48,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.g5.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 49,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 64,
    "name": "ml.g5.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 50,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 128,
    "name": "ml.g5.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 51,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 256,
    "name": "ml.g5.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 52,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 4,
    "hideHardwareSpecs": false,
    "memoryGiB": 192,
    "name": "ml.g5.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 53,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 4,
    "hideHardwareSpecs": false,
    "memoryGiB": 384,
    "name": "ml.g5.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 54,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 8,
    "hideHardwareSpecs": false,
    "memoryGiB": 768,
    "name": "ml.g5.48xlarge",
    "vcpuNum": 192
   },
   {
    "_defaultOrder": 55,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 8,
    "hideHardwareSpecs": false,
    "memoryGiB": 1152,
    "name": "ml.p4d.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 56,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 8,
    "hideHardwareSpecs": false,
    "memoryGiB": 1152,
    "name": "ml.p4de.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 57,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.trn1.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 58,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 512,
    "name": "ml.trn1.32xlarge",
    "vcpuNum": 128
   },
   {
    "_defaultOrder": 59,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 512,
    "name": "ml.trn1n.32xlarge",
    "vcpuNum": 128
   }
  ],
  "instance_type": "ml.p4de.24xlarge",
  "kernelspec": {
   "display_name": "Python 3 (PyTorch 2.0.0 Python 3.10 GPU Optimized)",
   "language": "python",
   "name": "python3__SAGEMAKER_INTERNAL__arn:aws:sagemaker:us-east-1:081325390199:image/pytorch-2.0.0-gpu-py310"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
